{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47f7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests      # Библиотека работы с HTTP-запросами по API\n",
    "import json          # Для обработки полученных результатов\n",
    "import time          # Для задержки между запросами\n",
    "import os            # Для работы с файлами\n",
    "import pandas as pd  # Для формирования датафрейма с результатами\n",
    "import re            # Для работы с регулярными выражениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d16b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка получения описания вакансии 78534080. Повтор запроса через 5 секунд.\n"
     ]
    }
   ],
   "source": [
    "def get_description(vacancy_id):\n",
    "    url = f'https://api.hh.ru/vacancies/{vacancy_id}'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    description = \"\"\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.ok:\n",
    "                data = response.json()\n",
    "                description = data['description']\n",
    "            break\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Ошибка получения описания вакансии {vacancy_id}. Повтор запроса через 5 секунд.\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "    return description\n",
    "\n",
    "# Чтение файла vacancies.json и создание словаря vacancies_dict\n",
    "with open('2/vacancies.json', 'r', encoding='utf-8') as f:\n",
    "    vacancies = json.load(f)\n",
    "vacancies_dict = {vacancy[\"id\"]: \"\" for vacancy in vacancies}\n",
    "\n",
    "# Обход словаря vacancies_dict и заполнение значениями ключа \"description\"\n",
    "for vacancy_id in vacancies_dict:\n",
    "    vacancies_dict[vacancy_id] = get_description(vacancy_id)\n",
    "\n",
    "# Сохранение результата в файл result.json\n",
    "with open('2/result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vacancies_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9209042c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'79110745': '<p>Ищу <strong>Data Engineer</strong> в SportTech компанию <strong>Sportradar.</strong></p> <p>Sportradar - крупнейший провайдер спортивных данных в мире, работают с FIFA, UEFA, NBA, NHL, ITF и многими другими. Торгуются на Nasdaq.</p> <p>Нанятый человек присоединится к annotation/labeling команде в computer vision продуктах, состоящей из 3 (будет 4) SW Engineers, 1 MLOps и 1 Product Manager’а. Annotation является частью более крупной команды Automated content, состоящей из 70 человек (аналитика, дополненная реальность).</p> <p><strong>Задачи</strong><br />Взаимодействие с data scientist’ами и аналитиками, работа над поиском новых способов генерации и улучшения качества данных.<br />Data modeling, создание и поддержка датасетов и инструментов для их генерации.<br />Развитие ETL и data ingestion пайплайнов.</p> <p><strong>Стек:</strong> Python, Pandas, Postgres, SQLAlchemy, Airflow, AWS (RDS, S3, Step Functions, EC2).</p> <p><strong>Требования</strong><br />Разговорный английский<br />Опыт работы Data Engineer от 2х лет, предпочтительно с подготовкой датасетов для ML<br />Представление о статистическом анализе данных, методах ML<br />Python, SQL, AWS (RDS, S3, Step Functions, EC2)<br />Плюсом будет: Airflow, Spark, Pandas, AWS SQS, SNS, EventBridge, Lambda.</p> <p><strong>Этапы интервью</strong><br />30 минут звонок с рекрутером, я проверю англ, отвечу на вопросы, уточню пожелания.<br />После 2 инженера проведут техническое интервью (1-2 собеседования).</p> <p><strong>Условия</strong><br />Первые 6 месяцев работа удаленная, не из РФ/РБ, по b2b контракту.<br />ЗП до 6k EUR gross + после 6 мес, RSU и годовой бонус (об этом ниже).<br />После - релокация с официальным трудоустройством на Кипр или в Словению, возможно в Сербию. Если вы уже легализовались в Европе, то добавляются Испания, Австрия, Германия, Польша, Словакия. Условия в этих странах варьируются, например где-то есть фитнес в офисе, где-то нет. Где-то будет сложнее устроить человека с РФ паспортом, где-то проще.<br />К ЗП добавится страховка, годовой бонус до 10% (обычно он в районе 6-8%), RSU в размере ~30% от годового дохода, а вот пересмотра фиксированной части ЗП в сторону увеличения, скорее всего, не будет.<br />Часы работы гибкие, нужно пересекаться в часы, о которых договоритесь с командой.<br />Посещение офиса от 3х дней в неделю. Если поселитесь очень далеко, то можно попробовать договориться на меньше.</p>',\n",
       " '79120615': '<p>В крупную исследовательскую компанию, специализирующуюся на анализе рыночных данных продаж товаров потребительского спроса, требуется <strong>Data engineer</strong>.</p> <p><strong>Обязанности:</strong></p> <ul> <li>Системный анализ, инженерная аналитика</li> <li>разработке архитектуры данных и структур баз данных</li> <li>Работа с базами данных, формирование логики наполнения и хранения данных.</li> <li>Cоздание, настройка, администрирование потоков данных</li> <li>Разработка базовой архитектуры</li> <li>Работа в BigData</li> <li>Разработка и оптимизация SQL запросов.</li> <li>Подготовка витрин данных.</li> </ul> <ul> <li>Разработка и совершенствование аналитического продукта, позволяющего клиенту отслеживать продажи и увеличивать эффективность инвестиций в аптечном канале сбыта.</li> <li>Обработка массива данных и подготовка информации для расчетов, автоматизация расчетов, работа с базами данных.</li> <li>Подготовка данных (Выборка, очистка, генерация признаков, интеграция, формирование).</li> <li>Интерпретация данных, постановка задач для построения дашбордов.</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Высшее техническое образование, предпочтительные профили: экономика/финансы/математика/физика.</li> <li>Знание специализированных программных средств для работы с базами данных.</li> <li>Умение формировать логичные аналитические материалы и экспертные заключения.</li> <li>Умение анализировать, обобщать статистические данные (в том числе используя VBA MS Office)</li> <li>Знание классических алгоритмов и структур данных.</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>комфортный офис в БЦ Магистраль Плаза</li> <li>возможность гибридного графика работы</li> <li>конкурентная ЗП</li> <li>профессиональный рост</li> </ul> <p> </p> <p> </p>',\n",
       " '78934984': '<p><strong>НЕ НУЖНО ОТКЛИКАТЬСЯ НА ВАКАНСИЮ! ЧИТАЙТЕ ИНФОРМАЦИЮ ВНИМАТЕЛЬНО НИЖЕ!</strong></p> <p>Компания <strong>Sapiens solutions</strong> открывает двери для начинающих талантливых айтишников и айтишниц, которые хотят прокачиваться в направлениях хранилищ данных, бизнес-аналитики, систем планирования и прогнозирования.</p> <p>Для таких энтузиастов мы предлагаем стартовую площадку в этой сфере — короткую насыщенную учебную программу по работе с MPP СУБД Greenplum. Вы получите комплексные знания по программе в режиме онлайн, которая составлена нашими опытными инженерами и архитекторами, <strong>совершенно бесплатно!</strong></p> <p><strong><em>Это не сама стажировка, это только обучение! Стажировка начинается спустя месяц, после успешного прохождения обучения!</em></strong></p> <p><strong>Что такое СУБД Greenlum?</strong></p> <p>Это аналитическая колоночная массивно‑параллельная СУБД, разработанная для хранения и обработки больших объёмов информации. Она позволяет работать с терабайтами и даже петабайтами данных.</p> <p><strong>Как поступить на программу обучения:</strong></p> <p>Вы отправляете заявку через сайт (5 минут). Сайт стажерской программы можно найти на нашем официальном сайте.</p> <p>↓</p> <p>За следующие 1–2 дня вы проходите короткий вступительный тест и выполняете задания на знание SQL. Также заполняете анкету &quot;Расскажи о себе&quot;.</p> <p>↓</p> <p>В течение 2-3 дней мы даём Вам обратную связь (Прошли ли вы на курсы или нет).</p> <p>↓</p> <p>Для успешно прошедших тестирование мы проводим дистанционный welcome-тренинг и рассказываем подробности о нашей работе и о будущем обучении.</p> <p>↓</p> <p>Вы учитесь на нашей платформе и выполняете учебные задания (6 недель). Весь процесс обучения вас будет сопровождать куратор из числа опытных инженеров данных Sapiens solutions, многие из них тоже когда-то пришли к нам стажерами.</p> <p>↓</p> <p>По результатам обучения – обязательная защита итогового проекта (Проверка и оценка полученных знаний). Представляете проект и разбираете его нюансы с нашими тимлидами и архитекторами (2 часа). Получаете сертификат о прохождение учебной программы.</p> <p>↓</p> <p>Congrats! После успешной защиты и пройденного собеседования, вы можете перейти на следующий уровень как полноценный начинающий инженер данных в составе команды Sapiens Solutions.</p> <p><strong>СТАРТ ОБУЧЕНИЯ - 10 МАЯ!!!!</strong></p> <p><strong>Подходит ли вам программа:</strong></p> <ul> <li>Имеете высшее образование или заканчиваете учиться <strong>(в том числе студенты последних курсов!!!!!).</strong></li> <li>Опыт в ИТ приветствуется.</li> <li>По завершении учёбы вы готовы работать по <strong>40 часов в неделю</strong> (с понедельника по пятницу, с 10:00-19:00) в Sapiens solutions. Частичная занятость и удаленная работа не позволяет стажеру погрузиться в технологический стек полностью.</li> <li><strong>Знаете основы SQL и не боитесь программирование.</strong> <strong>(При приеме это будет обязательно учитываться. Нам бы хотелось что-то большее, чем знание Select* и Inner Join)</strong></li> <li>Готовы перебраться в столицу.</li> <li>Желание обучаться и развивать знания и навыки в области ИТ, в целом, и в области аналитических решений, в частности; нацеленность на результат.</li> </ul> <p><strong>После успешной защиты итогового проекта мы предлагаем:</strong></p> <ul> <li>Оформление в соответствии с ТК РФ;</li> <li>Работа в большом современном офисе на м.Бауманская/Красносельская</li> <li>Заработная плата - 40000 руб. на руки</li> <li>Работа на интересных проектах в крупных компаниях, где вы сможете развивать, как профессиональные, так и личностные навыки;</li> <li>Перспективы карьерного роста внутри компании, через полгода Вы уже можете претендовать на уровень «младшего» инженера и далее;</li> <li>ДМС, включая стоматологию, начиная с позиции «Младший консультант»;</li> <li>Молодой и дружный коллектив профессионалов, комфортная рабочая атмосфера.</li> <li>Корпоративные, спортивные, образовательные мероприятия для наших сотрудников.</li> </ul> <p> </p> <p> </p> <p> </p>',\n",
       " '78954091': '<p><strong><em>Приветствуем тебя, будущий участник нашей прогрессивной команды! </em>Мы аккредитованная IT-компания &quot;Платформа больших данных&quot; – разрабатываем IT-</strong></p> <p><strong>продукты для бизнеса на основе big data. Наша компания объединила данные и компетенции крупнейших игроков рынка, ВТБ, Ростелекома и множества других партнеров. </strong>Мы обладаем одной из самых передовых технологических экспертиз и в течение ближайших лет мы планируем стать лидером рынка больших данных в России.</p> <p>Сейчас мы находимся в поисках <strong>разработчика/Дата-инженера</strong></p> <p><strong>Стек технологий: Hadoop, Spark; Python; SQL.</strong></p> <p><strong>Наши преимущества:</strong></p> <p>- Работа в одной из высокотехнологичных аккредитованных российских IT-компаний</p> <p>- Участник Сколково</p> <p>- Работу с отличной командой настоящих профессионалов (в компании более 140 человек)</p> <p>- Полис ДМС со стоматологией</p> <p>- Гибкое начало дня</p> <p>- Можно работать полностью удаленно в РФ либо гибридно (по вашему усмотрению)</p> <p>- Официальное оформление с 1-ого рабочего дня</p> <p>- Мероприятия для поддержания хорошего настроения (корпоративы, презентации новых IT-продуктов, сюрпризы)</p> <p>- Современное оборудование для работы</p> <p><strong>Требования:</strong></p> <ul> <li><strong>Опыт работы дата-инженером, разработчиком от 2 лет</strong></li> <li><strong>Опыт работы c Hadoop, Hive, Spark;</strong></li> <li>Хорошее знание <strong>Python;</strong></li> <li>Отличное знание <strong>SQL;</strong></li> <li>Понимание и интерес к области больших данных;</li> <li>Понимание и интерес к data science решениям и ML;</li> <li>Желателен опыт работы с NiFi и Airflow;</li> </ul> <p><strong>Твоими задачами станут:</strong></p> <p>∙ Разработка архитектуры решений по загрузке данных в кластер;</p> <p>∙ Интеграция систем обмена данными с различными источниками;</p> <p>∙ Оптимизация вычислений и работа с узкими местами платформы;</p> <p>∙ Реализация витрин данных на Spark;</p> <p>∙ Разработка и оптимизация ETL пайплайнов;</p> <p>∙ Refactoring, code review</p> <p><strong>Будем рады видеть тебя в нашей дружной команде профессионалов!</strong></p> <p> </p>',\n",
       " '79190614': '<p>IT community at P&amp;G is looking for interns to join our small but mighty Data Engineering team in Moscow! We offer you an exciting experience – a 6-month paid internship at P&amp;G, where you will be responsible for building solutions leveraging various Azure components and tools. This is your chance to work on interesting, global projects with some highly skilled and fun people.</p> <p>Your manager and team members will support and coach you, while you will be the leader of your project. We will provide you with all the necessary training and knowledge to perform well during your internship.</p> <p> </p> <p><strong>Our stack</strong></p> <ul> <li> <p>Azure Cloud: Azure Data Factory, SQL Database, etc.</p> </li> <li> <p>Databricks (Python/PySpark/SQL)</p> </li> <li> <p>Data visualization with Power BI</p> </li> <li> <p>Azure DevOps</p> </li> </ul> <p> </p> <p><strong>What can you expect?</strong></p> <p> </p> <p>You can expect meaningful work from Day 1. In this role, you will use cutting-edge technology, an industry-leading data solution architecture, and a governance model to solve real P&amp;G business problems. During your internship, you will have the opportunity to:</p> <ul> <li> <p>Learn from the global P&amp;G IT community to advance your skills and reapply the best practices;</p> </li> <li> <p>Develop business analytics products as part of a SCRUM team. You will need to bring technical solutions to obtain, process, store data, and provide business insights. You will learn to work with technologies like MS Azure, Python/PySpark, Power BI, etc;</p> </li> <li> <p>Become a professional in your specific project area and be able to understand a business need and transform it into an end-to-end solution;</p> </li> <li> <p>Create automated dataflows and be accountable for the whole ETL cycle;</p> </li> <li> <p>Get access to the comprehensive training portfolio, including MS Certifications, Coursera, and other courses.</p> </li> </ul> <p> </p> <p><strong>Compensation &amp; benefits</strong></p> <ul> <li> <p>Monthly salary 85 000 RUB gross</p> </li> <li> <p>Monthly lunch &amp; flexibility allowance 5 000 RUB gross</p> </li> <li> <p>Hybrid WFH/WFO schedule</p> </li> <li> <p>Access to P&amp;G’s global comprehensive training portfolio</p> </li> <li> <p>Relocation support in case you move to Moscow from another region</p> </li> </ul> <p> </p> <p><strong>Job Qualifications</strong></p> <p><strong>Mandatory</strong></p> <ul> <li> <p>3+ year students and recent graduates with any higher education degree (up to 2 years after graduation)</p> </li> <li> <p>Good command of English (Upper-Intermediate or higher level)</p> </li> <li> <p>Python basic knowledge</p> </li> <li> <p>Availability for at least a 6-month internship</p> </li> <li> <p>Readiness to work 40 hours per week</p> </li> <li> <p>Eagerness to learn, experiment, fail, and try again</p> </li> </ul> <p> </p> <p><strong>Nice to have</strong></p> <ul> <li> <p>Cloud understanding</p> </li> <li> <p>Experience with using database technology (SQL)</p> </li> <li> <p>Experience with data visualization tools</p> </li> </ul> <p> </p> <p><strong>Just so you know</strong></p> <ul> <li> <p>All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status, or any other legally protected factor;</p> </li> <li> <p>No immigration sponsorship is available for this position.</p> </li> </ul> <p> </p> <p><em>P&amp;G is a leading global consumer goods company whose winning brands are built around the model of innovation. Whatever your passion is, we want to ignite your potential to become your very best self. We hold true to our purpose, values, and principles as we seek to make a difference in the world around us. You will engage in meaningful work that will touch the lives of others and have a real impact. Everything at P&amp;G starts with understanding – understanding our consumers and our employees as we innovate to improve lives now and for generations to come.</em></p>',\n",
       " '79084256': '<strong>Что нужно будет делать:</strong> <ul> <li>Проектировать, разрабатывать и поддерживать пайплайны для сбора и обработки данных;</li> <li>Обеспечивать SLA и качество данных;</li> <li>Готовить данные для моделей машинного обучения и участвовать в их продукционализации совместно с data science командой; • Работать в команде и развивать отдел DE.</li> </ul> <strong>Будет классно, если у тебя: </strong> <ul> <li>Хорошее знание технологий из стека: Python, SQL, Spark, Airflow;</li> <li>Опыт работы на проектах с большими данными, понимание принципов распределенной обработки данных;</li> <li>Опыт продуктовой разработки в технологических компаниях;</li> <li>Опыт постановки задач;</li> <li>Опыт наставничества;</li> <li>Отличные коммуникативные навыки.</li> </ul> <strong>Ты покоришь наши сердца и разум, если у тебя:</strong> <ul> <li>Опыт работы с облаками, особенно, с Яндекс.Облаком;</li> <li>Опыт разработки высоконагруженных бэкенд сервисов на Java, Scala или Python;</li> <li>Опыт работы с моделями машинного обучения в продакшене;</li> <li>Опыт работы с базами данных для аналитики, особенно, с ClickHouse.</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Работу в аккредитованной IT компании с сильнейшей командой в разных масштабных проектах;</li> <li>Гибридный график работы 5/2, с 10:00 - 19:00;</li> <li>ДМС со стоматологией;</li> <li>В современном офисе в стиле Лофт с капсулой медитации, спортзалом, большой современной библиотекой и кабинетом для записи подкастов и треков;</li> <li>Комфортную кухню с холодильником, кофемашиной, тостером, микроволновкой и Magic Bullet;</li> <li>Холодильник с напитками (соки, энергетики, вода и т.д.) и едой (сыры, колбасы, сырки и м.ч.);</li> <li>Каждую пятницу совместные обеды с разными кухнями мира за счет компании.</li> </ul>',\n",
       " '78613909': '<p><strong>ГК «Автомакон»</strong> – системный интегратор IT-решений. 13 лет занимаемся комплексной автоматизацией бизнес-процессов. В команде 400+ квалифицированных специалистов, которые работают из 60+ городов России и мира.<br /><br /><strong>Наши направления деятельности:</strong><br /><br />1. Сервисные услуги.</p> <p>2. Проекты торговой сети ВкусВилл.</p> <p>3.Заказная разработка ПО.</p> <p>4. Веб- и мобильная разработка.</p> <p>5. Внедрение и сопровождение ПП 1С.</p> <p>6. Компьютерное зрение и системы видеоанализа.</p> <p>7. Нейросети и Big Data.</p> <p>8. Кассовый софт и оборудование.<br /><br />Мы в поиске специалистов, которые хотят заниматься масштабными и интересными проектами в сплоченной команде экспертов.<br /><br />На данный момент мы ищем<strong> middle/senior Data Engineer</strong> в <strong>ДатаЛаб.</strong></p> <p><br />Направление «ДатаЛаб» специализируется на Big Data, машинном и глубоком обучении и искусственном интеллекте с 2021 года. В составе направления более 20 специалистов. Гордимся тем, что создали для «ВкусВилл» предиктивную модель по прогнозированию сроков доставки, а также систему, которая рекомендует покупателям товары в мобильном приложении, что улучшает лояльность клиентов.</p> <p><strong>Что предстоит делать:</strong></p> <ul> <li> <p>Извлечение, преобразование, загрузка данных и их обработка (ETL/ELT);</p> </li> <li> <p>Формирование отчетов, визуализация данных;</p> </li> <li> <p>Создание и развитие процессов управления данными и их качеством;</p> </li> <li> <p>Работа с высоконагруженными базами;</p> </li> <li> <p>Разработка хранимых процедур, функций.</p> </li> </ul> <p><strong>Мы ждем от вас:</strong></p> <ul> <li> <p>Уверенный опыт программирования на Python (не менее 2-х лет);</p> </li> <li> <p>Уверенные знания SQL и опыт работы с базами данных;</p> </li> <li> <p>Опыт работы с Apache Airflow;</p> </li> <li> <p>Опыт написания процессов загрузки данных (ETL);</p> </li> <li> <p>Оптимизация SQL запросов;</p> </li> <li> <p>Понимание принципов работы данными;</p> </li> <li> <p>Опыт работы с СlickHouse/GreenPlum;</p> </li> <li> <p>Понимания концепций построения хранилищ данных.</p> </li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li> <p>Опыт работы со Spark;</p> </li> <li> <p>Опыт работы с Postgres;</p> </li> <li> <p>Опыт работы с MS SQL;</p> </li> <li> <p>Опыт работы с NoSQL базами данных.</p> </li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li> <p>работу в аккредитованной ИТ-компании;</p> </li> <li> <p>партнерскую программу «Зелёный свет»: за рекомендации знакомых специалистов можно получить до 80 000 руб.</p> </li> <li> <p>возможность удаленной работы, а также в офисе в Москве (м. Румянцево);</p> </li> <li> <p>официальное оформление с первого дня работы;</p> </li> <li> <p>поддержка куратора во время адаптации;</p> </li> <li> <p>внутреннее обучение soft &amp; hard skills, создание индивидуального плана развития;</p> </li> <li> <p>система бенефитов: компенсация медицинских расходов, возможность беспроцентного займа, компенсация обучения, спортивных занятий и затрат на обучение детей программированию;</p> </li> <li> <p>богатый опыт сплоченной и профессиональной команды.</p> </li> </ul> <p><strong>Наша команда помогает клиентам строить технологическое будущее, присоединяйся к нам – выходи на новый карьерный уровень!</strong></p>',\n",
       " '79315237': '<p>Наша команда занимается разработкой витрин данных и созданием внутренних инструментов для автоматизации процесса разработки и вывода, исследованием новых источников данных, помогает командам соседних подразделений построить интеграцию с нашими процессами. Мы ищем специалиста, желающего развиваться в DE и готового делиться своими знаниями с коллегами.</p> <p><strong>Обязанности</strong></p> <ul> <li>исследование источников данных (внешних данных, реплик АС Банка, других витрин)</li> <li>разработка и изменение витрин данных на Hadoop/GreenPlum исходя из требований аналитиков данных</li> <li>создание алгоритмов загрузки данных в витрины с учётом историчности, уникальности, логики обновления таблиц</li> <li>написание функций для первичной обработки, преобразования и агрегации данных на Spark (Scala, Java, Python)</li> </ul> <p><strong>Требования</strong></p> <ul> <li>опыт работы в роли Data Engineer</li> <li>понимание работы Hive, Spark на Hadoop</li> <li>базовые знания языков программирования Python</li> <li>знание SQL на уровне аналитических запросов</li> <li>понимание объектно-ориентированного подхода к разработке</li> <li>умение обращаться с git, bash</li> </ul> <p><strong>Условия</strong></p> <ul> <li>команда высокомотивированных и увлеченных профессионалов, с которой вы получите максимальное удовольствие от работы</li> <li>мощное железо, дополнительные мониторы и всё, что нужно для продуктивной работы</li> <li>возможность работы с новыми технологиями</li> <li>обучение за счет компании</li> <li>красивый и комфортный офис</li> <li>ДМС с первого дня</li> </ul>',\n",
       " '78968207': '<p><strong>Мы - аккредитованная Минцифры IT-компания DNS Технологии, находимся в поиске инженера данных в отдел «</strong><strong>DataOps</strong><strong>» направления «Инфраструктура»!</strong></p> <p><strong>На сегодняшний день команда работает над построением корпоративного аналитического хранилища данных на основе </strong><strong>Greenplum</strong><strong>, а также над установкой и поддержкой наших аналитических инструментов.</strong></p> <p><strong>Сейчас, для выполнения этих и других задач, мы готовы принять в команду еще одного коллегу!</strong></p> <p><em>Основной используемый стек технологий: Apache Airflow, Apache NiFi,Apache Kafka, Debezium, Minio, Greenplum, PostgreSQL, MSSQL, MySQL, ClickHouse, </em><em>Linux</em><em>, </em><em>Docker</em><em>, </em><em>Werf</em><em>, </em><em>K</em><em>8</em><em>S</em><em>, Git, </em><em>CI</em><em>/</em><em>CD</em><em>, Python</em></p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Наполнением КХД Greenplum из различных источников</li> <li>Доработкой и поддержкой ETL / ELT процесса</li> <li>Разворотом типовых аналитических инструментов средствами CI/CD</li> <li>Разработкой проектов, связанных с поддержкой внутренней инфраструктуры отдела</li> </ul> <p><strong>Ты нам подходишь, если:</strong></p> <ul> <li>Имеешь знания по SQL</li> <li>Имеешь знания по Python</li> <li>Умеешь и хочешь работать в команде</li> <li>Коммуникабельный, исполнительный и ответственный</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>Знание одного или нескольких инструментов из основного технологического стека <em>(описан выше)</em></li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Развитую систему наставничества</li> <li>Прозрачную систему грейдов для роста и развития внутри компании</li> <li>Командный подход к разработке, где приветствуется атмосфера взаимопомощи и общения</li> <li>Разнообразные интересные задачи</li> <li>Обширную базу знаний</li> <li>Лояльное отношение к дресс-коду</li> <li>Возможность найти коллег по интересам для участия в различных внерабочих активностях</li> </ul> <p><em>А еще:</em></p> <ul> <li>Трудоустройство по ТК РФ в официально зарегистрированной ИТ-компании и полностью “белую” заработную плату</li> <li>Ежегодную премию по результатам работы Компании</li> <li>Возможность приобретать товары Компании по специальной цене</li> <li>Компенсацию внешнего обучения по профилю 70%</li> <li>Скидки на обучение английскому языку и скидки от партнёров (фитнес-клуб, мобильная связь)</li> <li>Участие в корпоративных мероприятиях</li> <li>Офис, оснащенный всей необходимой техникой для работы, а также местом для отдыха и кухней. Как дополнительный плюс – наличие парковки.</li> </ul> <p><em>Кандидатов оцениваем по знаниям и навыкам, финальную заработную плату обговариваем после собеседования.</em></p> <p><strong>Если заинтересовала вакансия - ждём резюме откликом или по электронной почте! :)</strong></p>',\n",
       " '79172870': '<strong>Обязанности:</strong> <ul> <li>Участвовать в построении хранилищ данных: проектирование, определение сущностей, формирование витрин данных;</li> <li>Разрабатывать и оптимизировать процессы выгрузки данных из различных источников;</li> <li>Разрабатывать процессы обработки данных;</li> <li>Проводить оркестрацию ETL/ELT процессов в Airflow;</li> </ul> <strong>Требования:</strong> <ul> <li>Опыт работы с Airflow;</li> <li>Опыт работы с одной из баз: GreenPlum, Vertica</li> <li>Знание SQL;</li> <li>Опыт работы с реляционными СУБД;</li> <li>Опыт разработки ETL/ELT процессов.</li> </ul> <strong>Условия:</strong> <ul> <li>Комфортный график, вы можете работать удаленно с любой точки мира!</li> <li>Мы предлагаем современную технику на выбор для удобной работы</li> <li>Наша компания является аккредитованной ИТ-компанией.</li> <li>Мы работает с разными проектами (банки, крупные ритейлеры, обучающие платформы, сети ресторанов) с различным технологическим стеком. Это отличная возможность для роста компетенций наших сотрудников!</li> <li>Мы предлагаем работу в команде профессионалов с богатым опытом в крутых технологичных проектах, готовых делится своими знаниями.</li> <li>У нас в компании проектная организационная структура, все возникшие вопросы решаются оперативно. Мы готовы слышать своих сотрудников!</li> <li>Мы предлагаем прозрачный карьерный рост, программу личного развития, включающую внешнее и внутреннее обучение.</li> <li>Заработная плата по итогам собеседования. Все выплаты включаются в договор.</li> </ul>',\n",
       " '79192759': '<p>Команда профессионалов, за плечами которой создание нескольких успешных FinTech проектов в странах Юго-Восточной Азии, развивает новый, еще более амбициозный проект. Мы решаем интересную и крутую задачу – построение онлайн-банка в Азии.</p> <p>Мы ищем Middle+ Data Engineer-а в нашу команду DWH, которая на текущий момент состоит из 7 человек. Мы отвечаем за данные, их качество и строим аналитику, то есть принимаем важные бизнес решения на их основе.</p> <p><strong>Основные обязанности:</strong></p> <ul> <li>Разрабатывать и улучшать среду DWH (ETL / ELT pipelines, DDL, и тд), обеспечивать производительность и стабильность, при необходимости участвовать в анализе инцидентов</li> <li>Внедрять и сопровождать Data Catalog, Data linage, Metadata management tool, Data Quality project, решать сложные задачи по реализации никем ранее не реализованного функционала</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Опыт создания архитектуры ETL/ELT потоков данных</li> <li>Опыт работы с airflow, написание собственных операторов, хуков</li> <li>Опыт промышленного программирования на Python от 3х лет, умение оптимизировать существующий код, уменьшение boilerplate</li> <li>Опыт работы с DWH от 2х лет в роли аналитике, разработчика, понимание архитектуры DWH, его слоев и компонент</li> <li>Опыт моделирования данных, понимание что такое ERD</li> <li>Опыт работы с системами DQ, выстраивания собственных DQ инструментов как плюс</li> <li>Стрессоустойчивость, самостоятельность</li> </ul> <p><strong>Преимущества:</strong></p> <ul> <li>Опыт работы с dbt</li> <li>Опыт работы с datahub</li> </ul> <p><strong>Образование:</strong></p> <ul> <li>Оконченное высшее образование - математика, физика, computer science</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li>Комфортный офис в Москва-Сити с возможностью работать из любой точки мира;</li> <li>Официальное трудоустройство, конкурентная заработная плата;</li> <li>Гибкий рабочий график для оптимального баланса между работой и личной жизнью;</li> <li>Высокий уровень корпоративной культуры, корпоративные мероприятия;</li> <li>Прекрасная возможность профессионального развития в молодой и динамично развивающейся компании.</li> </ul>',\n",
       " '79226963': '<p><strong>LATOKEN is the supermarket of assets where it is easy to discover, exchange, earn and spend crypto.</strong></p> <p><strong>Mission:</strong> Well-structured and secure data and monitoring.</p> <p><br /><strong>Story</strong>: Product, Finance, OKRs, and Dash Teams need well-structured and secure data and monitoring to analyze and track the improvement of the services and product performance.</p> <p><strong>Key PROBLEM&#39;s</strong>:</p> <ul> <li>The monitoring system alerts for any downtime, latency, or success rate issues of the services.</li> <li>All company data and events are merged and consolidated into the Data Base (DWH, PostgreSQL, Segment, Indicative, Google Analytics) to support the Monitoring system, Token Discovery, OKRs and Analytical Dashes.</li> <li>Acquisition channels, conversions, and users are correctly mapped and easy to access.</li> <li>Token DB is representative of the market and token data is correct and complete with on-chain and smm data. (30k tokens by end of April)</li> </ul> <p><strong>How</strong>:</p> <ul> <li>Create new data pipelines for business need.</li> <li>Maintain scripts and pipelines in case of problems or new requirements.</li> <li>Define necessary events, set up recording and make available for reports and dashboards.</li> <li>Maintain and update token DB.</li> </ul> <p><strong>Constraints</strong>:</p> <ul> <li>All OKRs of the company are consolidated into Platform Ops.</li> <li>All scripts are consolidated into Gitlab.</li> </ul> <p><strong>Main performance number:</strong> Ops OKR Health<br /><strong>Second performance number:</strong> Free Space DWH<br /><strong>Third performance number:</strong> Query response time, sec avg<br /><br /><strong>Functions:</strong><br />• OKRs Health : Calculate OKRs correctly, daily and in time<br />• DWH objects audit<br />• Queries : Advice on optimization and writing queries to databases.<br />• Data quality : Keep data in order and provide reliable reporting<br />• Automation : Automation of OKRs and slack bots for automated ops processes<br />• Audit : Audit of DWH state and automated scripts<br /><br /><strong>Requirement skills and experience:</strong></p> <p>2+ years of PostgresSQL DB administration experience;</p> <p>2+ years of experience writing Python scripts and applications for loading data;</p> <p>Experience in configuring a database in a high-availability architecture;</p> <p>Experience in database optimization for different load profiles;</p> <p>Experience in automating data loading from different sources (CRM, ERP, Web resorses)</p> <p>Eager to work with people with high-performance standards</p> <p><strong>Will be a plus:</strong></p> <p>Experience with financial data</p> <p>Experience as a data/product analyst</p>',\n",
       " '78370985': '<p><strong>У нас все организовано в виде продуктов, имеющих бесконечный срок жизни. Продуктов очень много – несколько сотен.</strong></p> <p>Если делить их по группам – получится так:</p> <p><strong>Клиентский опыт:</strong></p> <ul> <li>обобщаем все клиентские взаимодействия с компанией в одну историю, под одним универсальным идентификатором;</li> <li>прогнозируем и корректируем общую выручку от клиента на всем периоде жизни с компанией;</li> <li>боремся с фродом на стороне клиентских устройств сотовой связи;</li> <li>предсказываем отток и next-best-action для клиентов.</li> </ul> <p><strong>Управление оборудованием и качеством услуг связи:</strong></p> <ul> <li>собираем и анализируем метрики качества предоставления связи;</li> <li>выполняем интеллектуальное планирование постройки базовых станций;</li> <li>осуществляем предиктивное обслуживание оборудования.</li> </ul> <p><strong>Для продуктовых команд:</strong></p> <ul> <li>создаем для себя фреймворки и утилиты;</li> <li>развиваем сервис мониторинга как единую точку сбора и просмотра метрик.</li> </ul><br /><br /><strong>Обязанности:</strong> <ul> <li>Анализ требований к витринам данных (взаимодействие с владельцем продукта, BI-разработчиками, data scientist-ами)</li> <li>Поиск и исследование источников данных для последующей интеграции</li> <li>Оценка пригодности, качества исходных данных</li> <li>Разработка ETL процессов на Spark</li> <li>Оркестрация ETL процессов в Airflow</li> <li>Проектирование баз данных</li> <li>Создание конвейеров данных NiFi</li> </ul> <p><strong>Мы понимаем, что каждые DE индивидуален. Поэтому даем описание как бы выглядел идеальный кандидат. Все недостающие навыки можно подтянуть у нас.</strong></p> <ul> <li>Любит работать в команде и умеет это делать</li> <li>Проработал от 1 года и более в таких областях как: коммуникационные технологии, безопасность, маркетинг и продажи, финансы.</li> <li>Знает SQL на высоком уровне (в т. ч. DDL, табличные выражения, оконные функции)</li> <li>Работал с Hive, PostgreSQL</li> <li>Умеет разрабатывать ETL процессы Spark на Scala (потоковая обработка как преимущество)</li> <li>Пользовался AirFlow или другими оркестраторами – Oozie, Luigi, ну или cron</li> <li>Может что-то написать на Python – в объеме чтобы пользоваться AirFlow или еще круче</li> <li>Имеет опыт потоковой разработки конвейеров данных в NiFi или Flink</li> <li>Интересуется Flink, пробовал применять его в проектах</li> <li>Умеет проектировать базы данных (знает Data Vault 2.0 например)</li> <li>Понимает принципы работы реляционных СУБД и HDFS</li> <li>Имеет представление о колоночных и NoSQL СУБД</li> <li>Понимает подходы к работе с качеством данных</li> <li>Применяет системный подход к работе, думает о конечной бизнес-задаче, мыслит логически, уделяет внимание деталям</li> </ul> <p><strong>Стек и технологии</strong></p> <p>В своей работе DE используют следующий стек технологий:</p> <ul> <li>Экосистема Hadoop – HDFS, YARN, Hive, HBase</li> <li>ETL-процессы – Spark (Scala)</li> <li>Потоковая обработка – NiFi, Flink</li> <li>Брокер сообщений – Kafka</li> <li>Оркестрация ETL процессов – Airflow</li> <li>СУБД – PostgreSQL, Greenplum, Aerospike, Oracle, SQL Server</li> <li>CI/CD – GitLab</li> </ul> <strong>Условия:</strong> <ul> <li>В компании существует и растет сообщество DE. Сейчас там около 300 человек с разным уровнем навыков от Junior до Senior. Инженеры помогают друг другу бороться с трудностями и развиваться, делятся друг с другом кодом, всякими лайфхаками.</li> <li>Периодически проводятся митапы по инфраструктурным и софтовым темам, где коллеги делятся опытом, помогают разобраться в востребованных темах.</li> <li>Мы готовы оплачивать любые активности по развитию и обучению – конференции, подписки, книги, курсы – все что помогает расти профессионально.</li> <li>Предлагаем всем удаленный формат работы, но можно и гибридный - в зависимости от того, как Вам более удобно.</li> </ul> <p> </p>',\n",
       " '78340750': '<p><strong>Обязанности:</strong></p> <ul> <li>Построение и проектирование ETL процессов, потоков обработки данных;</li> <li>Разработка витрин данных со сложными аналитическими показателями;</li> <li>Обеспечение точности и бесперебойности расчетов;</li> <li>Участие в тестировании и внедрении реализованных решений</li> <li>Работа с большими объемами данных;</li> <li> <p>Помощь в подготовке документации.</p> </li> </ul> <p><strong>Наш стек:</strong></p> <p>Аналитический движок Luxms BI, PostgreSQL, Clickhouse, Oracle, MSSQL, MySQL.</p> <p><strong>Требования:</strong></p> <ul> <li>Опыт работы с СУБД, <strong>отличное знание SQL</strong>;</li> <li>Хорошее знание Linux - уровень опытного пользователя;</li> <li>Опыт работы в команде, желание активно изучать и применять новые технологии.</li> </ul> <p><strong>Желательно:</strong></p> <ul> <li>Знание JavaScript;</li> <li>Опыт с PL/SQL или PL/pgSQL, другими процедурными расширениями SQL;</li> <li>Опыт разработки скриптов (bash, awk, python);</li> <li>Опыт работы с Luxms BI.</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>оформление по ТК РФ;</li> <li>заработная плата полностью официальная;</li> <li>испытательный срок до 3-х месяцев (устанавливается по итогам собеседования);</li> <li>ежегодный оплачиваемый отпуск в соответствии с законодательством РФ;</li> <li>офис в 10-15 мин. ходьбы от ст. м. Московские ворота/Электросила;</li> <li>дружный сплоченный коллектив.</li> </ul>',\n",
       " '79148004': '<p><strong>Каруна</strong> — это сообщество, где твои идеи становятся IT-проектами.</p> <p>С 2015 года мы создаём софт, мобильные приложения, инструменты мониторинга, антифрод-продукты, медиабиблиотеки, крутой маркетинг и клиентский сервис.</p> <p><br />Главная ценность Каруны — люди.</p> <p>Вкладываемся в развитие и обучение. Всегда открыты для идей и инициатив. Премируем, одариваем. У нас команда, как семья, офис 99 из 10, удалёнка, если хочешь, и фееричные кутежи.</p> <p>В нашу дружную команду мы ищем <strong>Anti-Fraud Data Engineer</strong>!</p> <p><strong>Что нужно делать:</strong></p> <ul> <li> <p>Разработка, построение и поддержание витрин данных (DWH);</p> </li> <li> <p>Развитие и поддержание части аналитической инфраструктуры,<br />сервисов на python, их ci/cd и т.п.</p> </li> </ul> <p><strong>Основные требования:</strong></p> <ul> <li>Понимание принципов работы СУБД;</li> <li>Опыт работы с ETL/ELT.</li> <li>Опыт работы с python.</li> <li>Знание SQL.</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Оформим официально. Офис или удалёнка — решать тебе.</li> <li>Офис — 99 из 10. Как дома, только круче: караоке, плоечная, музыкальная студия, комната для сна, спортзал, и ещё много всего — в историческом центре Питера.</li> <li>Накормим, как бабушка: каждое утро завтрак в офисе. Во вторник и четверг — свежие фрукты. В конце недели пятничный ужин: вкуснейшие пицца, пироги, суши, супы и горячее приезжают в офис — и ты приезжай!</li> <li>Кроме отпуска, можешь брать оплачиваемые days off — сколько понадобится.</li> <li>Позаботимся о твоём здоровье: откроем ДМС с первого месяца, обеспечим тренировки в офисном зале. А ещё у нас есть психолог.</li> <li>Компенсируем ежемесячный кутёж с коллегами.</li> </ul>',\n",
       " '79162833': '<p>Наша команда объединяет более 1600 IT специалистов в 13 офисах по всей стране. Мы гордимся нашими проектами:</p> <ul> <li>Портал gosuslugi.ru, число пользователей которого достигает 130 млн</li> <li>Высоконагруженные системы электронного правительства</li> <li>Инновационная платформа биометрической идентификации</li> <li>Передовые отраслевые и бизнес-решения</li> </ul> <p>Мы ищем профессионалов, способных мыслить глобально и создавать тренды.</p> <p><em>Сейчас нам требуется <strong>Data Engineer</strong> на проект АНАЛИТИЧЕСКАЯ ПЛАТФОРМА.</em></p> <p><strong>Стек технологий: Python, Airflow, NiFi, Hadoop, Spark, Kafka, Greenplum<br /><br />ЧЕМ ТЫ БУДЕШЬ ЗАНИМАТЬСЯ: </strong></p> <ul> <li>Участвовать в разработке Data Lake и хранилища данных в роли Data Engineer;</li> <li>Участие в разработке архитектуры ETL процессов и инструментов для работы с данными;</li> <li>Участие в тестировании;</li> <li>Обеспечение контроля качества данных, мониторинг, анализ и устранение проблем с качеством данных.</li> </ul> <p><br /><strong>ТЫ НАШ ИДЕАЛЬНЫЙ КАНДИДАТ, ЕСЛИ У ТЕБЯ ЕСТЬ:</strong></p> <ul> <li>Знание SQL и оптимизация запросов;</li> <li>Опыт разработки с использованием Python;</li> <li>Опыт работы с реляционными СУБД (Oracle / PostgreSQL / MSSQL);</li> <li>Опыт разработки и внедрения алгоритмов работы с данными (матчинг данных и др.);</li> <li>Опыт визуализации данных (BI, Python: Pandas+matplotlib и т.д<strong>) </strong></li> </ul> <p><br /><strong>СОВСЕМ КРУТО, ЕСЛИ:</strong></p> <ul> <li>Знаком с экосистемой Hadoop (HDFS, Hive, Spark, HBase, etc.)</li> <li>Знаком с MPP системами (Greenplum, Teradata, Vertica и т.д.)</li> </ul> <p> </p> <p><strong>КОМПАНИЯ ПРЕДОСТАВЛЯЕТ:</strong></p> <ul> <li>Социально значимые проекты;</li> <li>Конкурентная зарплата;</li> <li>Квартальные и годовые премии;</li> <li>Расширенный полис ДМС со стоматологией и международной страховкой;</li> <li>Компенсация 16 видов расходов на выбор: КАСКО, ОСАГО, ДМС для родственников, билеты на концерты, оплаты дет. сада, авиа и жд билеты;</li> <li>Классная программа корпоративного обучения и оплата любых проф. курсов;</li> <li>Гибкое начало рабочего дня: с 8 до 11.</li> <li>Выбор комфортного формата работы: удаленный или гибридный (2-3 дня в офисе);</li> <li>Увеличенное количество дней отпуска.</li> </ul>',\n",
       " '78879859': '<p><strong>Стань частью команды знаменитого горного курорта России!</strong></p> <p>Ищем Data Engineer’а, который подхватит унаследованную систему (MS стек) аккумулирующую информацию в DWH из 10 систем отелей с визуализацией в Power BI. На этом базисе закроем текущие\\\\горящие потребности бизнеса и начнем строить свою систему обработки данных с блэк-джеком и Industry Standard стеком. Не штампа ради, а для констатации факта – позиция на которой можно будет наглядно видеть какой вклад в бизнес вносит команда и ты лично: померить, посчитать, получить обратную связь! Плюс построить систему обработки данных по своему, базируясь на удобстве, эффективности и целесообразности, а не «потому что так исторически сложилось».</p> <p><strong>Какие задачи необходимо решать:</strong></p> <ul> <li>Разработка, поддержка коннекторов к системам-источникам данных</li> <li>Весь набор манипуляций с пайплайнами: проектирование, разработка, поддержка</li> <li>Администрирование инструментов и собственно обеспечение качества данных</li> <li>Строительство DWH: проектирование, оптимизация, мониторинг</li> <li>Создание витрин и визуализация данных в BI</li> <li>Ad-hoc запросы</li> </ul> <p><strong>Для решения этих задач требуется:</strong></p> <ul> <li>Опыт работы DE от 2-х лет</li> <li>Хорошее знание Python и SQL, опыт оптимизации запросов</li> <li>Опыт проектирования и разработки промышленной DWH</li> <li>Опыт оркестрации ETL на Airflow или аналогах</li> <li>Знания и опыт администрирования MySQL, PostgreSQL, MSSQL</li> <li>Знание принципов и подходов к обеспечению качества данных и владение соответствующими инструментами</li> <li>Навыки работы с BI-системами, значительным плюсом будет уверенное владение Power BI</li> </ul> <p><strong>Вашим преимуществом будут:</strong></p> <ul> <li>Навыки в работе с аналитическими BD</li> <li>Понимание принципов потоковой обработки данных</li> <li>Облачные технологии</li> </ul> <p><strong>Условия:</strong></p> <ul> <li> <p>Работа на территории уникальной природной зоны в г. Сочи, курорт Красная Поляна, офис с видом на горы и реку</p> </li> <li> <p>Предоставляется проживание на территории курорта на льготных условиях</p> </li> <li> <p>Корпоративный трансфер из Сочи/Адлера</p> </li> <li> <p>Бесплатный ски-пасс на все канатные дороги курорта</p> </li> <li> <p>Предоставление скидок на услуги курорта, в кафе/ресторанах/магазинах и развлекательных объектах на территории курорта</p> </li> <li> <p>График работы: 5/2 с 9.00 до 18.00 (возможен гибрид в среднесрочной перспективе)</p> </li> <li>Оклад + квартальный KPI. Итоговый уровень заработной платы обсуждается с финальным кандидатом</li> </ul> <p><em>Благодарим за интерес к нашей Компании!<br />Просмотр отклика на вакансию означает, что резюме находится на рассмотрении, однако не является приглашением на собеседование.<br />Если в течение 14 дней после отклика не поступает приглашение на собеседование, то Ваше резюме автоматически попадает в резервную базу кандидатов.<br />Мы свяжемся с Вами при открытии более подходящей вакансии.</em></p> <p><em>С уважением,</em></p> <p><em>Группа по подбору персонала НАО «Красная поляна»</em></p>',\n",
       " '79323507': '<p>ГК «Самолёт» — активно растущая компания, входящая в топ-3 крупнейших российских девелоперов, строит свою экосистему цифровых сервисов; занимается развитием, автоматизацией своих продуктов и процессов. Есть планы по дальнейшей коммерциализации части цифровых продуктов и выводу их на российский и международный рынок. В рамках реализации данных целей приглашаем на позицию – <strong>Data Engineer</strong> в отдел маркетинга.</p> <p> </p> <p><strong>Ваша зона ответственности:</strong></p> <ul> <li>Организация инфраструктуры ETL процессов на базе Яндекс.Облака, Google Cloud и стека Hadoop;</li> <li>Организация процесса обмена данными с Calltouch, Exponea, CRM, BigQuery, Clickhouse, Hadoop;</li> <li>Управление и документирование процессов сквозной аналитики и медиапланирования;</li> <li>Планирование и реализация архитектуры хранилища и процессов обработки данных.</li> </ul> <p><strong>Наши пожелания:</strong></p> <ul> <li>Опыт работы с данными - от 2 лет;</li> <li>Понимание терминологии маркетинга (цифровая реклама, охватная реклама, CPL, ROI, ДРР, когорты и др.);</li> <li>Опыт работы с инфраструктурой ETL процессов на базе Яндекс.Облака, Google Cloud и(ли) стека Hadoop;</li> <li>Опыт работы с с Calltouch, Exponea, CRM, BigQuery, Clickhouse, Hadoop и(ли) другими системами маркетинговых данных;</li> <li>Понимание процессов сквозной аналитики и медиапланирования;</li> <li>Продвинутое знание Python (pandas, requests, sklearn).\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b</li> </ul> <p><strong>Будет плюсом</strong></p> <ul> <li>Знание архитектуры хранилища и процессов обработки данных (ETL);</li> <li>Продвинутое знание SQL (представления, оконные функции).</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Гибкий график работы;</li> <li>Полная удаленка (при желании можно работать из офиса БЦ Кунцево Плаза, м. Молодежная (кстати, наш офис получил премию «Трансформация года» в 2021г.)</li> <li>Расширенный пакет ДМС и страхование от несчастных случаев с первого месяца работы, льготное ДМС для близких родственников, страхование имущества и пр.</li> <li>Компенсация мобильной связи (корпоративная сим-карта) для тех, кому нужно всегда быть на связи</li> <li>Рост и развитие в компании: регулярная обратная связь каждому сотруднику, обучающие мероприятия и широкий доступ к материалам, в том числе ведущих образовательных платформ</li> <li>Скидки для сотрудников на недвижимость группы “Самолет”, индивидуальные условия по рассрочке</li> </ul>',\n",
       " '79261592': '<p>Команда Axenix (ex-Accenture) продолжает работу на российском рынке и аккумулирует 30-ти летний консалтинговый опыт внедрения инновационных решений.<br />Наша экспертиза - стратегия и консалтинг, технологии и операции, направленные на цифровизацию бизнеса.<br />В России мы работаем в офисах в Москве, Твери и Ростове-на-Дону, а также удаленно. Постоянно обмениваемся опытом и экспертизой.</p> <p>На данный момент мы ищем<strong> Data Engineer в команду Data&amp;AI</strong>.</p> <p><strong>Чем Вы будете заниматься?</strong></p> <ul> <li>Настройка и модификация конфигурационных файлов при подключении нового источника или изменении уже подключённого;</li> <li>Создание соответствующего дистрибутива;</li> <li>Установка дистрибутива на ДЕВ/ПСИ/ПРОМ стендах;</li> <li>Тестирование, ПСИ, вывод в ПРОМ.</li> </ul> <p><strong>Что мы ожидаем от кандидата:</strong></p> <ul> <li>Oracle GoldenGate. Знакомство с принципами работы сервиса</li> <li>Apache Hadoop. Понимание на уровне теории (в т.ч. распределённое хранение данных), минимальный опыт работы с HDFS через консоль Linux или HUE</li> <li>Apache HBase. Понимание, что собой представляет собой NO-SQL база данных, принцип работы</li> <li>Основы работы с реляционными БД. SQL – на уровне junior аналитика данных</li> <li>Apache Kafka. Принципы работы</li> <li>Linix, GIT - практический опыт.</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Возможность профессионального и карьерного роста;</li> <li>Гибридный/удаленный формат работы;</li> <li>Конкурентоспособный уровень дохода и регулярное повышение по результатам Performance Review;</li> <li>ДМС с первого дня работы, включая стоматологию, в лучших клиниках для тебя, твоего партнера и детей до 18 лет;</li> <li>Страхование жизни в размере годового оклада;</li> <li>Дополнительные 5 дней оплачиваемого отпуска в год;</li> <li>Ежемесячная денежная компенсация на питание;</li> <li>Программа корпоративных привилегий PrimeZone (более 1000 ведущих поставщиков продуктов и услуг);</li> <li>Возможность обучения и сертификации за счет компании.</li> </ul>',\n",
       " '78448054': '<p><strong><em>ПК ЛИДЕРГРУПП – это успешная и динамичная команда профессионалов, реализующая проекты в области проектирования линейных объектов по всей территории России в качестве Генерального проектировщика.</em></strong></p> <p> </p> <p><strong>Условия</strong>:</p> <ul> <li>Персональный график работы (офис, удаленно, гибридный).</li> <li>Любая форма трудоустройства (Официально, ГПХ, Самозанятость).</li> <li>Уровень оплаты обсуждается индивидуально.</li> <li>Территориально: ст.м. Киевская офис класса А (в пешей доступности).</li> <li>Возможность участвовать в реализации новых уникальных проектов на базе ИИ.</li> <li>Профессиональный творческий коллектив.</li> </ul> <p><strong>Задачи:</strong></p> <ul> <li>Аналитика рынка AI-сервисов под бизнес-задачи кампании.</li> <li>Проведение тестирований на основе технологий искусственного интеллекта (ИИ-Сервисов).</li> <li>Подготовка протоколов и результатов исследований.</li> <li>Разработка и внедрение актуальных технологий AI для оптимизации бизнес-процессов Компании.</li> <li>Подбор и координация работы подрядчиков-разработчиков систем ИИ-Сервисов.</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Профильное высшее образование и навыки работы с разными языками программирования.</li> <li>Знание техник и алгоритмов AI (Data Scientist, Big Data, Machine learning, Нейронные сети, ChatGPT).</li> <li>Практический опыт разработки и внедрения программных продуктов.</li> <li>Опыт управления внутренними и внешними командами разработчиков.</li> </ul> <p> </p>',\n",
       " '78952166': '<p>Центр изучения и сетевого мониторинга молодёжной среды –<strong> аккредитованная IT-компания</strong>, учреждённая по поручению Президента России в октябре 2018 года.</p> <p>Деятельность организации ориентирована на создание IT-решений, направленных на формирования комплексной системы по защите детей и подростков от воздействия негативной информации в сети.</p> <p><strong>Будем рады, если Вы:</strong></p> <ul> <li>разрабатываете на Python;</li> <li>умеете строить ETL на Apache Airflow;</li> <li>знаете SQL и активно его применяете в деле, имеете опыт работы с различными БД (rowstore, columnstore), не боитесь работы с терабайтами данных;</li> <li>понимаете принципы построения DWH;</li> </ul> <p><strong>Будет замечательно, если Вы:</strong></p> <ul> <li>умеете работать с AMQP, Kafka и Redis;</li> <li>знаете про CDC-инструменты;</li> <li>готовы обогащать данными Datalake на Minio; имеете опыт работы с ELK стеком;</li> <li>готовы к активному взаимодействию и командной работе, готовы понять, что нужно DS и DA, и сделать лучше;</li> <li>не забываете про мониторинг всего написанного, цените Data Quality и тесты;</li> <li>умеете отлаживать и валидировать свой или чужой код;</li> <li>работаете с Git.</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>опыт работы с ClickHouse, ELK, K8s;</li> <li>желание развиваться и не останавливаться на достигнутом;</li> <li>на данный момент готовы рассматривать кандидатов разных уровней, нам есть, чем с Вами поделиться в области знаний/опыта, а также будем рады узнать Ваше видение.</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>Оформление в соответствии с ТК РФ;</li> <li>Заработная плата по результатам собеседования;</li> <li>График работы: 5/2, с 10:00 до 19:00;</li> <li>Формат работы – в офисе в Москве или удаленно.</li> </ul>',\n",
       " '78687348': '<p>Сейчас мы в поиске сильного инженера в наше подразделение, занимающееся Data Engineering.</p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>проектировать архитектуру АХД, разрабатывать и запускать сервисы c DЕ-решениями;</li> <li>развивать и улучшать существующие продукты и сервисы;</li> <li>участвовать в проработке и реализации интеграций с другими сервисами и командами (кстати, у нас полностью микросервисная архитектура);</li> <li>обеспечивать высокое качество кода и улучшать процессы в своей команде;</li> <li>участвовать в развитии инфраструктурных решений для работы с big data.</li> </ul> <p><strong>От вас мы ожидаем:</strong></p> <ul> <li>отличное знание SQL, Python;</li> <li>опыт построения аналитического хранилища данных.</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>опыт работы с Greenplum, PostgreSQL, Clickhouse.</li> </ul> <p><strong>Почему мы:</strong></p> <ul> <li>интересный и без сомнений востребованный продукт;</li> <li>мы используем современные технологии; практикуем continuous integration и continuous delivery; код в bitbucket, задачи в jira и т.д.;</li> <li>взрослый девопс, highload;</li> <li>бизнес-руководство, нацеленное на результат;</li> <li>понимающее IT-руководство, например, у нас есть квота 1 день в неделю на борьбу с тех. долгом/саморазвитие/рисеч/решение задач других команд;</li> <li>высокие темпы работы, разработанные решения очень быстро попадают в продакшн.</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>работу в аккредитованной IT компании;</li> <li>конкурентную заработную плату;</li> <li>полис ДМС с первого месяца работы;</li> <li>корпоративный университет, онлайн-курсы для повышения квалификации, конференции, митапы;</li> <li>фитнес-зал в здании офиса;</li> <li>льготную программу ипотеки для сотрудников;</li> <li>MacBook (или другой ноутбук на выбор) + дополнительные мониторы и т.д.;</li> <li>комфортный офис класса А в 5 минутах от станции метро и МЦК Кутузовская;</li> <li>гибкое начало рабочего дня и возможность работать в гибридном или удаленном формате.</li> </ul>',\n",
       " '79115441': '<p><strong>Ключевые задачи:</strong></p> <p>- Проектирование и разработка витрин данных на Greenplum/Teradata (и в перспективе Hadoop (Hive, Spark))</p> <p>- Расчет сложных аналитических показателей в витринах данных</p> <p>- Разработка и поддержка ETL-процессов загрузки данных в/из хранилища с использованием Python</p> <p>- Контроль качества загружаемых данных (реализация сложных проверок качества, проведение корректировок данных большого объема)</p> <p>- Поиск и предоставление данных по запросу внутренних и внешних заказчиков</p> <p> </p> <p><strong>Навыки:</strong></p> <p>- Опыт построения корпоративных хранилищ данных (DWH, DL), включая разработку ETL-процессов</p> <p>- Понимание планов запросов, навык их оптимизации.</p> <p>- Знание SQL</p> <p>- Продвинутые аналитические навыки и вариативное видение решения задач</p> <p> </p> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Deep diving в предметную область, много разработки по задачам имеющим прямой эффект на бизнес;</li> <li>Возможность привносить новые идеи и нестандартные решения;</li> <li>Сообщество D-people– поддержка, развитие и возможность учиться у профессионалов;</li> <li>Достойную оплату труда;</li> <li>Возможность расти в самой крупной DS-команде – пересматриваем доход каждые полгода));</li> <li>Возможности саморазвития: оплата курсов популярных платформ, периодика;</li> <li>ДМС, сниженные ставки по кредитованию, программы лояльности для сотрудников, льготная ипотека;</li> <li>Скидки на услуги экосистемы Сбера и компаний-партнеров.</li> </ul> <p> </p> <p><strong>Приходи к нам, расти вместе с нами!</strong></p> <p> </p>',\n",
       " '78427104': '<p>\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b<strong>Каруна</strong> — это сообщество, где твои идеи становятся IT-проектами.</p> <p>С 2015 года мы создаём софт, мобильные приложения, инструменты мониторинга, антифрод-продукты, медиабиблиотеки, крутой маркетинг и клиентский сервис.<br /><br />Главная ценность Каруны — люди.</p> <p>Вкладываемся в развитие и обучение. Всегда открыты для идей и инициатив. Премируем, одариваем. У нас команда, как семья, офис 99 из 10, удалёнка, если хочешь, и фееричные кутежи.</p> <p>В нашу дружную команду мы ищем <strong>Data Engineer</strong>!</p> <p><strong>Что нужно делать:</strong></p> <ul> <li>Разработка, построение и поддержание витрин данных (DWH);</li> <li>Очистка, обработка данных при загрузке;</li> <li>Оптимизация запросов.</li> </ul> <p><strong>Основные требования:</strong></p> <ul> <li>Понимание принципов работы СУБД;</li> <li>Опыт работы с ETL/ELT;</li> <li>Опыт работы с Python (от 2 лет);</li> <li>Уверенные знание SQL.</li> </ul> <p><strong>Будет бонусами:</strong></p> <ul> <li>работа с Clickhouse, Vertica;</li> <li>работа с AirFlow, Kubernetes;</li> <li>работа с Docker;</li> <li>знакомство с Jira, Confluence;</li> <li>желателен командный опыт работы по Agile: Scrum/Kanban.</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Оформим официально. Офис или удалёнка — решать тебе.</li> <li>Офис — 99 из 10. Как дома, только круче: караоке, плоечная, музыкальная студия, комната для сна, спортзал, и ещё много всего — в историческом центре Питера.</li> <li>Накормим, как бабушка: каждое утро завтрак в офисе. Во вторник и четверг — свежие фрукты. В конце недели пятничный ужин: вкуснейшие пицца, пироги, суши, супы и горячее приезжают в офис — и ты приезжай!</li> <li>Кроме отпуска, можешь брать оплачиваемые days off — сколько понадобится.</li> <li>Позаботимся о твоём здоровье: откроем ДМС с первого месяца, обеспечим тренировки в офисном зале. А ещё у нас есть психолог.</li> <li>Компенсируем ежемесячный кутёж с коллегами.</li> </ul>',\n",
       " '79145131': '<p><strong>«ЛЕНТА»</strong> – федеральная розничная сеть, занимающая первое место в России по количеству гипермаркетов. Наши информационные системы перерабатывают огромные объемы информации от движения автотранспорта до покупки товара покупателем.</p> <p>Ищем <strong>Data </strong><strong>инженера</strong> для работы с большими объемами данных, open source решениями и сложными задачами.</p> <p>Используем<strong> Hadoop </strong>(MapReduce, HDFS, Spark, Kafka, YARN), БД <strong>Oracle</strong>, БД <strong>Hana</strong>, SAP DS - ETL, открыты к новым идеям и технологиям.</p> <p> </p> <p><strong>Задачи:</strong></p> <ul> <li>разработка архитектуры доставки, хранения и обработки данных</li> <li>разработка и поддержка автоматизированных регулярных ETL процессов</li> <li>организация хранения данных с использованием инфраструктуры Hadoop (HDFS, Hive), оптимизация хранимых данных </li> </ul> <p><strong>Какие навыки нужны:</strong></p> <ul> <li>опыт работы в big data</li> <li>Python (Java/scala тоже можно)опыт программирования от 2-х лет на SQL, Python</li> <li>знание инструментов ETL и CDC</li> <li>опыт работы с Hadoop будет преимуществом</li> </ul> <p><strong>Тебя ждет:</strong></p> <ul> <li>активное участие в реализации значимых Big data проектов в одной из лидирующих розничных сетей</li> <li>ДМС класса Люкс со стоматологией</li> <li>оформление по ТК РФ, полный спектр социальных льгот</li> <li>возможность самостоятельно принимать решения по архитектуре и стеку</li> <li>дружный коллектив профессионалов с европейской корпоративной культурой</li> <li>программа личного развития, включающая внешнее и внутреннее обучение</li> <li>возможность работать удаленно до 100% времени</li> <li>годовое премирование на основании результатов оценки деятельности</li> </ul> <p> </p>',\n",
       " '77577033': '<p><strong>SpectrumData </strong>- аккредитованная ИТ-компания, которая создала группу федеральных сервисов по проверке: автомобилей, физических лиц , контрагентов, объектов недвижимости и кредитных историй. Нашими клиентами являются Яндекс, Mail group, Газпромнефть, ВСК, Кардиф, Согласие, Росгосстрах и многие другие.</p> <p>Мы работаем со зрелыми сервисами и при этом сохраняют динамику стартапа, мы активно взаимодействуем со внешней средой (агрегируем и обогащаем данные из различных источников), и нам приходится постоянно держать руку на пульсе всех изменений! Поэтому наши продукты “живые”, постоянно находятся в стадии развития и совершенствования, скучать не приходится :)</p> <p><br /><em><strong>Задачи, которые предстоит решать:</strong></em></p> <ul> <li> <p>Анализировать требования, приходящие от бизнес-подразделения;</p> </li> <li> <p>Проектировать, разрабатывать и поддерживать пайплайны работы с данными (data pipelines, ETL);</p> </li> <li> <p>Разрабатывать правила и контролировать качества данных;</p> </li> <li> <p>Автоматизировать процессы работы с данными.</p> </li> </ul> <p><em><strong>Наши ожидания от кандидата:</strong></em></p> <ul> <li> <p>Опыт работы от 2-х лет на аналогичной должности;</p> </li> <li> <p>Опыт построения пайплайнов данных (data pipelines), включая ETL;</p> </li> <li> <p>Опыт анализа и очистки данных;</p> </li> <li> <p>Опыт работы с SQL и NoSQL СУБД, умение создавать сложные запросы и оптимизировать их производительность (мы используем Clickhouse, PostgreSQL, MongoDB, Cassandra и др.);</p> </li> <li> <p>Хорошее знание Python и основных библиотек для работы с данными;</p> </li> <li> <p>Умение работать с Git;</p> </li> <li> <p>Коммуникабельность, готовность общаться с коллегами по возникшим вопросам.</p> </li> </ul> <p><em><strong>Как будет построена твоя работа:</strong></em></p> <ul> <li>Официальное трудоустройство, белая заработная плата;</li> <li>Офис в центре города;</li> <li>Регулярный пересмотр заработной платы по мере вашего развития в компании;</li> <li>Современное &quot;железо&quot; и любые необходимые сервисы.</li> </ul>',\n",
       " '79122630': '<p>В отдел бизнес-аналитики и финансового планирования требуется Data Engineer.</p> <p><strong>Обязанности</strong></p> <p>· сбор, обработка, анализ, загрузка данных для аналитических исследований</p> <p>· работа с базами данных на MS SQL, Postgre, GreenPlum, Hadoop</p> <p>· формирование аналитики по направлениям бизнеса</p> <p>· построение факторных моделей, выявление трендов, интерпретация результатов</p> <p><strong>Требования</strong></p> <p>· высшее образование (IT, математическое, техническое)</p> <p>· знание SQL, Hadoop</p> <p><strong>Будет дополнительным преимуществом:</strong></p> <p>· Опыт в программировании на python: pyspark, pandas, numpy</p> <p>· Навык работы с bash</p> <p>· git</p> <p><strong>Условия</strong></p> <p>· официальное трудоустройство</p> <p>· график работы 5/2 с 09.00-18.00</p> <p>· ДМС, страхование жизни и здоровья, льготное кредитование, корпоративный фитнес</p> <p>· заработная плата - оклад + квартальное и годовое премирование</p> <p>· уровень оклада обсуждается на собеседовании</p>',\n",
       " '79147234': '<strong>Обязанности:</strong> <ul> <li>Разработка и поддержка витрин БД;</li> <li>Разработка механизмов интеграций;</li> <li>Проектирование и разработка SQL-процедур, обработчиков данных на Spark (Scala) и ETL-процессов (в т.ч. AirFlow);</li> <li>Оптимизация производительности кода;</li> <li>Разработка и ведение схемы данных;</li> <li>Участие в проведении тестирований.</li> </ul> <strong>Требования:</strong> <ul> <li> <p>Опыт работы с промышленными DWH на основе MPP СУБД (Teradata, Exadata, Greenplum);</p> </li> <li> <p>ETL - практический опыт проектирования и разработки регламентных потоков данных при помощи Airflow;</p> </li> <li> <p>SQL - уверенные знания (DML,DDL), опыт разработки хранимых процедур;</p> </li> <li> <p>Опыт с Git и Gitlab;</p> </li> <li> <p>Будет плюсом опыт работы с Kubernetes и Docker.</p> </li> </ul> <p>\\u200b\\u200b\\u200b\\u200b</p>',\n",
       " '78852846': '<p><strong>Мы - аккредитованная IT-компания «Сеть партнерств».</strong></p> <p><strong>В июле 2021 наша команда запустила флагманский продукт – подписку «Огонь», ogon.ru<br />Это первая на рынке мультиподписка с собственным мобильным приложением.<br />На данный момент в подписку входят предложения более чем от 45 партнёров, включая &quot;VK Музыка + Wink и другие сервисы&quot;.</strong><br /><br />Для реализации RoadMap на 2023 приглашаем в команду <strong>Lead Data Engineer</strong></p> <p><strong>Что предстоит делать:</strong></p> <ul> <li>Взаимодействие с внешними и внутренними коллегами с целью определения способов интеграций систем</li> <li>Участие в проектировании архитектуры аналитического контура</li> <li>Менторство над менее опытными коллегами, в том числе постановка задач</li> <li>Реализация наиболее сложных компонентов платформы</li> <li>Вникать в суть проектов. Написание документации по проделанной работе</li> <li>Определение и реализация метрик работы сервисов и пайплайнов обработки данных Постановка их на мониторинг и передача в поддержку</li> <li>Код-ревью. Приемка задач. Валидация выполняемых работ на момент соответствия целевой архитектуре</li> <li> <p>Лидировать процессы построения корпоративного хранилища данных</p> </li> <li> <p>Влиять на внутренние процессы и используемый стек технологий</p> </li> </ul> <p><strong>Стек технологий:</strong></p> <p>· Python</p> <p>· ClickHouse, PostgreSQL, Kafka</p> <p>· Prometheus + Grafana</p> <p>· Kubernetes + Docker</p> <p>· Gitlab</p> <p>· Nifi, Tableau</p> <p>· JupyterHub</p> <p>· Influx db</p> <p><strong>Что ожидаем увидеть:</strong></p> <ul> <li>Опыт промышленной разработки на python в части работы с данными от 3-х лет</li> <li>Навыки коммуникации с заказчиками и контрагентами</li> <li>Понимание принципов работы data science проектов</li> <li>Опыт работы с Nifi, Airflow, PostgreSQL, ClickHouse</li> <li>Умение писать сложные sql-запросы</li> <li>Понимание принципов работы с аналитическими и транзакционными базами данных</li> <li>Желателен опыт руководства</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li>Проект с финансовой поддержкой крупных инвесторов</li> <li>Инновационные сложные продукты</li> <li>В команде: разработчики, продуктовые аналитики, бизнес-аналитики, системные аналитики, тестировщики - обеспечивают высокий уровень результата и профессионализм команды</li> <li>Белая заработная плата + бонусы в течение года до +30% к окладу</li> <li>Гибридный формат работы: удаленно + из офиса (м. Калужская)</li> <li>ДМС расширенный</li> <li>Увеличенный размер отпуска - 31 день</li> </ul>',\n",
       " '78519105': '<p>Наши продукты - AI/ML-решения для крупных банков, ритейла и промышленности: системы предсказания спроса, сервисы персонализации и рекомендательные системы, поисковые системы для крупных ритейлеров и банков.</p> <p>Мы ищем data engineer&#39;а для усиления нашего направления разработки систем прогнозирования спроса на товары в крупнейшем ритейле.</p> <p><strong>Задачи:</strong></p> <ul> <li>Разработка и развитие платформы прогнозирования - архитектура и реализация инструментов для построения ML-пайплайнов (подготовка данных, сбор фичей, обучение моделей, применение моделей, ведение ML-экспериментов и т.п.);</li> <li>Помощь коллегам/DS в оптимизации их работающих пайплайнов, проявлять про-активность с предложениями оптимизации;</li> <li>Взаимодействие с коллегами/Devops по настройке окружений, деплоя кода, работы с инфраструктурой компании;</li> <li>Общение с коллегами/DQ, выяснение деталей наполнения данных, участие в составлении БТ к данным;</li> <li>Решение неожиданных проблем с данными, задержками их доставки.</li> </ul> <p><strong>Минимальные требования:</strong></p> <ul> <li>Уверенные знания Python 3+ (структуры данных, алгоритмы, концепции языка);</li> <li>Уверенные знания SQL: агрегации, джойны, вложенные запросы, индексы, оптимизации запросов;</li> <li>Опыт работы с Hadoop, Spark;</li> <li>Опыт разворачивания, настройки мониторинга и передача на поддержку разработанных решений.</li> </ul> <p><strong>На что ещё смотрим:</strong></p> <ul> <li>Опыт работы с Airflow и другими подобными инструментами для запуска регулярных задач;</li> <li>Опыт Devops (Docker, Kubernetes, Gitlab-CI, настройка окружения на серверах и др.);</li> <li>Опыт разработки сервисов (Flask, Django, Asyncio и др.);</li> <li>Опыт проектирования высоконагруженных приложений и/или приложений работы с большими данными;</li> <li>Будет плюсом опыт использования машинного обучения.</li> </ul> <p><strong>У нас:</strong></p> <ul> <li>Участие в быстром росте компании, работающей на перспективном AI рынке;</li> <li> <p>Возможность удаленное работы в любой точки мира или офис МСК;</p> </li> <li>Совместная работа с опытными разработчиками, аналитиками данных, менеджерами, продуктологами;</li> <li>Гибкий график работы;</li> <li>Оформление ТК РФ или ГПХ (вне рф);</li> <li>ДМС (включая стоматологию) после прохождения испытательного срока;</li> <li>Уютный офис в центре Москвы (2 минуты от м. Сухаревская) со всем необходимым для комфортной работы.</li> </ul> <p><strong>Особенно актуальное</strong>:</p> <ul> <li>Мы аккредитованная ИТ-компания со всеми вытекающими льготами.</li> </ul>',\n",
       " '78273747': '<p>Платформа ОФД - продуктовая IT-компания, крупнейший в России оператор фискальных данных. Мы создаем полезные и удобные сервисы для предпринимателей.</p> <p>Делаем рыночную аналитику на основе данных из магазинных чеков.</p> <p>Компания резидент Сколково, входит в Экосистему Сбера.</p> <p><strong>Что у нас есть:</strong></p> <ul> <li>Много данных: каждый 3-й чек, пробиваемый в России, находится на наших серверах. Мы принимаем до 50 млн чеков в день, имеем 2 млрд уникальных названий товаров в базе.</li> <li>Развитая инфраструктура: есть несколько кластеров Hadoop, у DS есть несколько мощных машин, GPU делают бр-бр</li> <li>Команда из 4 DE и сильная экспертиза</li> <li>80% кода мы пишем на Scala</li> <li>Оформление по ТК РФ, белая зп</li> <li>ДМС с госпитализацией, скорой и стоматологией</li> <li>Офис близко от м. Спортивная/Лужники. Можно работать из дома</li> <li>Удобная кухня, релакс-зона с тренажером, массажным креслом, приставкой и караоке</li> <li>Пицца-пати раз в месяц и корпоративные праздники</li> </ul> <p><strong>Наш стек:</strong></p> <p>Hadoop, Spark, Hive, SCALA, Python, Java, PostgreSQL, ClickHouse, Zeppelin/IntelliJ, AirFlow, ElasticSearch, Apache Superset</p> <p>GitLab, k8s, Docker, Jira, Confluence</p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Разрабатывать ETL-процессы с использованием библиотеки Apache Spark на Scala</li> <li>Анализировать, проектировать и создавать витрины данных в соответствии с требованиями конкретного проекта</li> <li>Развивать инфраструктуру для обработки больших данных и кодовой базы Scala</li> <li>Работать с DS для внедрения математических алгоритмов и ML-моделей в промышленные процессы</li> </ul> <p><strong>Откликайся, если ты:</strong></p> <ul> <li>Живешь в Москве или МО</li> <li>Не менее<strong> 2-х лет</strong> решаешь задачи в области сбора, хранения и анализа данных</li> <li>Пишешь код на Scala</li> <li>Отлично знаешь SQL</li> <li>Применяешь в работе инструменты BigData: Airflow, Hadoop, Spark, Hive, Zeppelin</li> <li>Будет плюсом опыт работы с Docker и k8s</li> </ul> <p>Выбор кандидата проходит по итогам выполненного тестового задания</p>',\n",
       " '78029161': '<p>В дружный коллектив, занимающийся цифровизацией промышленных предприятий «Росатома», требуется аналитик данных (data scientist) уровня middle, готовый брать ответственность за блок задач создания и оценки моделей предиктивной аналитики.</p> <p>Мы готовы предложить достойную компенсацию и интересные рабочие задачи человеку, который:<br />- имеет опыт работы с полным циклом разработки моделей;<br />- знаком с MLOps;<br />- имеет опыт работы с производственными данными (заполнение пропусков, фильтрация «плохих значений» и т.п.);<br />- умеет конструировать признаки для ML-моделей (feature engineering);<br />- может обосновать выбор модели для решения бизнес-задач;<br />- не боится участвовать в написании соответствующих разделов технических заданий;<br />- не постесняется консультировать пользователей системы предиктивного анализа в части работы с моделями.</p> <p>В идеале нам нужен человек, знакомый с терминами: time-series, gradient boosting (CatBoost/XGBoost), random forest, autoencoder, а также понимающий специфику промышленных предприятий (опыт в банковской сфере и ритейле малоприменим к нашим задачам).</p> <p>Предполагается, что вы знаете Python, умеете работать с Jupyter notebook, PySpark, TensorFlow, PyTorch и знакомы со средой Linux (bash).</p> <p>В остальном:<br />- график работы с 8 до 17 (можно с 9 до 18), в пятницу – на 1:15 меньше; по окончанию испытательного срока можно рассмотреть гибридный график;<br />- премии по результатам выполненных проектов и по году;<br />- ДМС после испытательного срока в 3 месяца;<br />- все прочие плюсы работы в «Росатоме» - регулярный пересмотр заработной платы, специальные программы для сотрудников (банки, ипотека, фитнес);<br />- работа в аккредитованной ИТ-компании.</p>',\n",
       " '79089912': '<p>Зона ответственности команды DWH - качественные и своевременные данные, решающие потребности бизнеса. За платформу данных и подключение источников к озеру данных отвечает отдельная платформенная команда.<br />Объемы данных в компании исчисляются петабайтами. Основной процессинг данных на текущий момент осуществляется на облачной платформе AliBaba MaxCompute.<br />На этот год перед командой стоит задача перенести процессинг данных на свою платформу данных, которая основывается на Hadoop-стеке (Hive, Spark, Airflow). В рамках этой активности мы планируем не просто переосмыслить процесс загрузки данных, но и внедрить новые подходы и практики. В частности, планируем в ближайшее время опробовать Dbt поверх Spark. Поэтому у вас будет возможность практически с нуля выстроить процесс обработки данных.</p> <p><strong>Обязанности:</strong></p> <ul> <li>Быть драйвером процесса переноса текущего DWH на платформу Hadoop. Определять архитектуру, внедрять и развивать инженерные подходы и практики;</li> <li>Проектирование и моделирование слоев обработки данных;</li> <li>Менторинг разработчиков (код ревью, развитие технических навыков);</li> <li>Оптимизация производительности сложных процессов загрузки данных;</li> <li>Обеспечивать контроль за качеством данных и соблюдение SLA;</li> </ul> <strong>Требования:</strong> <ul> <li>Глубокое знание стека Hadoop (Hive, Spark, Kafka);</li> <li>Опыт работы с Python или Java от 3-х лет;</li> <li>Практический опыт проектирования хранилищ данных (Kimball, DataVault);</li> <li>Опыт написания дата пайплайнов, опыт работы с Airflow;</li> <li>Понимание подходов к организации разработки (CI/CD, DevOps)</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>Опыт работы c Dbt;</li> <li>Понимание предметной области электронной коммерции;</li> <li>Опыт разработки потоковой обработки данных (Spark Streaming / Flink);</li> <li>Опыт работы с форматами delta/hudi/iceberg.</li> </ul> <strong>Условия:</strong> <ul> <li>Сильная команда, с которой можно расти;</li> <li>Сложные, нетривиальные задачи для маркетплейса с миллионами пользователей;</li> <li>ДМС со стоматологией;</li> <li>Гибкий график работы;</li> <li>Достойная зарплата;</li> <li>Комфортный офис в Cити.</li> <li>Возможность работать удаленно.</li> </ul>',\n",
       " '73888449': '<p>Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность на основании их строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, часть - разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform.</p> <p><strong>Чем предстоит заниматься</strong></p> <ul> <li>Участие в разработке ядра продуктов Data Platform</li> <li>Разработка и оптимизация процессов выгрузки данных</li> </ul> <p><strong>Что Вам необходимо</strong></p> <ul> <li>Знание одного из языков программирования: Python, Java, Go</li> <li>Знание основ ООП, теории алгоритмов, структур данных</li> <li>Знание SQL</li> <li>Опыт написания процессов загрузки данных (ETL)</li> <li>Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL</li> <li>Будет плюсом знакомство с: Pandas, Numpy, Groovy</li> <li>Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе у метро «Водный стадион». График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li> <p>Бесплатный фитнес-зал Tinkoff Sport. Тренируйтесь, посещайте групповые программы, грейтесь в сауне и участвуйте в спортивных турнирах</p> </li> <li>Бесплатные обеды в Tinkoff Cafe. А если захотите перекусить, на каждом этаже есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '79226964': '<p><strong>LATOKEN is the supermarket of assets where it is easy to discover, exchange, earn and spend crypto.</strong></p> <p><strong>Mission:</strong> Well-structured and secure data and monitoring.</p> <p><br /><strong>Story</strong>: Product, Finance, OKRs, and Dash Teams need well-structured and secure data and monitoring to analyze and track the improvement of the services and product performance.</p> <p><strong>Key PROBLEM&#39;s</strong>:</p> <ul> <li>The monitoring system alerts for any downtime, latency, or success rate issues of the services.</li> <li>All company data and events are merged and consolidated into the Data Base (DWH, PostgreSQL, Segment, Indicative, Google Analytics) to support the Monitoring system, Token Discovery, OKRs and Analytical Dashes.</li> <li>Acquisition channels, conversions, and users are correctly mapped and easy to access.</li> <li>Token DB is representative of the market and token data is correct and complete with on-chain and smm data. (30k tokens by end of April)</li> </ul> <p><strong>How</strong>:</p> <ul> <li>Create new data pipelines for business need.</li> <li>Maintain scripts and pipelines in case of problems or new requirements.</li> <li>Define necessary events, set up recording and make available for reports and dashboards.</li> <li>Maintain and update token DB.</li> </ul> <p><strong>Constraints</strong>:</p> <ul> <li>All OKRs of the company are consolidated into Platform Ops.</li> <li>All scripts are consolidated into Gitlab.</li> </ul> <p><strong>Main performance number:</strong> Ops OKR Health<br /><strong>Second performance number:</strong> Free Space DWH<br /><strong>Third performance number:</strong> Query response time, sec avg<br /><br /><strong>Functions:</strong><br />• OKRs Health : Calculate OKRs correctly, daily and in time<br />• DWH objects audit<br />• Queries : Advice on optimization and writing queries to databases.<br />• Data quality : Keep data in order and provide reliable reporting<br />• Automation : Automation of OKRs and slack bots for automated ops processes<br />• Audit : Audit of DWH state and automated scripts<br /><br /><strong>Requirement skills and experience:</strong></p> <p>2+ years of PostgresSQL DB administration experience;</p> <p>2+ years of experience writing Python scripts and applications for loading data;</p> <p>Experience in configuring a database in a high-availability architecture;</p> <p>Experience in database optimization for different load profiles;</p> <p>Experience in automating data loading from different sources (CRM, ERP, Web resorses)</p> <p>Eager to work with people with high-performance standards</p> <p><strong>Will be a plus:</strong></p> <p>Experience with financial data</p> <p>Experience as a data/product analyst</p>',\n",
       " '79314682': '<p><strong>Ключевые задачи:</strong></p> <ul> <li>Подготовка и обеспечение процедур построения моделей данными из внутрибанковских и внешних источников;</li> <li>Разработка требований к составу, форматам и качеству данных, поступающих для моделирования, к организации их сбора и хранения;</li> <li>Постановка задач на подготовку и выгрузку выборок;</li> <li>Выявление текущих недостатков в поступивших данных, необходимости и направлений доработок в этой области; -</li> <li>Участие в разработке и совершенствовании методик и моделей ПВР;</li> <li>Взаимодействие со смежными подразделениями (IT/ BI, обеспечения качества данных, внутренней валидации, внутреннего аудита) по вопросам обеспечения разработчиков данными;</li> <li>Участие в разработке и актуализации внутренних технологических документов банка по указанному направлению.</li> </ul> <strong>Что важно для нас:</strong> <ul> <li>Опыт работы в профильном ПО (SAS Base/ R/ Python/ …);</li> <li>Продвинутое понимание архитектуры БД/ ХД;</li> <li>Уверенное знание SQL (преимущественно MS SQL, PostrgeSQL) обязательно. Умение оптимально написать сложный запрос;</li> <li>Продвинутый опыт в сфере Big Data;</li> <li>Желательно опыт работы в сфере банковского кредитного риск-менеджмента;</li> <li>Желательно знание основ статистики и мат. моделирования, понимание общих основ подходов BCBS, EBA/ECB и требований Банка России (483-П, 3752-У, 730-П) в сферах ПВР/ МСФО;</li> <li>Желательно стремление к профессиональному развитию в области моделирования кредитного риска, к участию в проектах внедрения ПВР, т.е. к перепрофилированию со сферы IT на кредитный риск-менеджмент.</li> </ul> <strong>Что предлагаем:</strong> <ul> <li>График работы: пн-пт 09:00-18:00</li> <li>Официальное оформление в соответствии с ТК РФ</li> <li>Конкурентный уровень дохода: оклад + премии</li> <li>Медицинская страховка, страховка для выезжающих за границу</li> <li>Доплата к отпускному и больничному листу</li> <li>Дополнительные льготы при заключении брака и рождении детей</li> <li>Социальная поддержка при сложных жизненных ситуациях</li> <li>Льготное кредитование для сотрудников</li> <li>Обучение в корпоративном университете банка</li> <li>Корпоративная библиотека</li> </ul>',\n",
       " '79224520': '<p>Мы занимаемся развитием и поддержкой платформы Big Data на vk.com. Платформа построена как на проверенных решениях с открытым исходным кодом (Hadoop, Kafka, Spark, Zeppelin), так и на собственных разработках, заточенных под работу 24/7 в условиях высоких нагрузок.</p> <p>ВКонтакте — самая большая социальная сеть в России, поэтому у нас самая большая Big Data:</p> <ul> <li>Kafka ~ 1 Пбайт;</li> <li>HDFS ~ 50 Пбайт;</li> <li>Clickhouse ~ 3 Пбайт (NVMe).</li> </ul> <p>Ищем специалиста, который отлично владеет любым из этих инструментов.</p> <p><strong>Вам предстоит:</strong></p> <ul> <li>развивать платформу хранения и обработки Big Data;</li> <li>внедрять новые инструменты для анализа данных и машинного обучения;</li> <li>решать задачи производительности и отказоустойчивости инфраструктуры Big Data;</li> <li>строить новые и оптимизировать существующие ETL-процессы.</li> </ul> <p><strong>У нас интересно, потому что:</strong></p> <ul> <li>действительно много данных (десятки петабайт), настоящие и сложные задачи;</li> <li>мы не зацикливаемся на работе с одним инструментом или хранилищем, а всегда ищем наиболее подходящее решение;</li> <li>мы используем весь стек технологий — от железа и настроек инструментов до реализации собственных разработок.</li> </ul> <p><strong>Мы рассчитываем, что вы:</strong></p> <ul> <li>хорошо знаете Java/Scala, Python;</li> <li>разбираетесь в принципах работы баз данных, распределённых систем хранения и обработки данных;</li> <li>работали и понимаете внутреннее устройство Hadoop, HDFS, Kafka, Spark, Zeppelin, Airflow, ZooKeeper, ClickHouse.</li> </ul> <p><em><strong>Приглашаем специалиста, который сможет посещать офис в Москве или Санкт-Петербурге или работать в комбинированном режиме. Ждём ваших откликов. Удачи!</strong></em></p>',\n",
       " '79090742': '<p>Зона ответственности команды DWH - качественные и своевременные данные, решающие потребности бизнеса. За платформу данных и подключение источников к озеру данных отвечает отдельная платформенная команда.<br />Объемы данных в компании исчисляются петабайтами. Основной процессинг данных на текущий момент осуществляется на облачной платформе AliBaba MaxCompute.<br />На этот год перед командой стоит задача перенести процессинг данных на свою платформу данных, которая основывается на Hadoop-стеке (Hive, Spark, Airflow). В рамках этой активности мы планируем не просто переосмыслить процесс загрузки данных, но и внедрить новые подходы и практики. В частности, планируем в ближайшее время опробовать Dbt поверх Spark. Поэтому у вас будет возможность практически с нуля выстроить процесс обработки данных.</p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Разработка ELT-процессов по загрузке данных в DWH и витрины данных;</li> <li>Проектирование и моделирование слоев обработки данных;</li> <li>Оптимизация производительности процессов загрузки данных;</li> <li>Разработка проверок за качеством данных;</li> </ul> <strong>Мы ожидаем от Вас:</strong> <ul> <li>Хорошее знание SQL и опыт работы с MPP базами данных;</li> <li>Уверенное владение Python;</li> <li>Понимание принципов работы БД и построения хранилищ данных (Kimball, DataVault);</li> <li>Опыт работы Airflow и ETL-инструментами;</li> <li>Опыт работы с большими объемами данных и оптимизации производительности;</li> <li>Ответственность, самостоятельность, коммуникативность.</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>Опыт написания батч пайплайнов на Spark и опыт работы с Hadoop стеком;</li> <li>Опыт работы c Dbt;</li> <li>Опыт разработки потоковой обработки данных (Spark Streaming / Flink).</li> </ul> <strong>Условия:</strong> <ul> <li>Сильная команда, с которой можно расти;</li> <li>Сложные, нетривиальные задачи для маркетплейса с миллионами пользователей;</li> <li>ДМС со стоматологией;</li> <li>Гибкий график работы;</li> <li>Достойная зарплата;</li> <li>Комфортный офис в Cити.</li> <li>Возможность работать удаленно.</li> </ul>',\n",
       " '79279390': '<p><strong>Для крупного банка на проект по переходу на ПВР подбираем - Data-инженера (Ведущего специалиста).</strong></p> <p><strong>Обязанности:</strong></p> <ul> <li>подготовка и обеспечение процедур построения моделей данными из внутрибанковских и внешних источников;</li> <li>разработка требований к составу, форматам и качеству данных, поступающих для моделирования, к организации их сбора и хранения;</li> <li>постановка задач на подготовку и выгрузку выборок;</li> <li>выявление текущих недостатков в поступивших данных, необходимости и направлений доработок в этой области;</li> <li>участие в разработке и совершенствовании методик и моделей ПВР;</li> <li>взаимодействие со смежными подразделениями (IT/ BI, обеспечения качества данных, внутренней валидации, внутреннего аудита) по вопросам обеспечения разработчиков данными;</li> <li>участие в разработке и актуализации внутренних технологических документов банка по указанному направлению.</li> </ul> <strong>Требования:</strong> <ul> <li>продвинутый уровень владения SQL - обязательно;</li> <li>понимание архитектуры БД/ ХД – обязательно;</li> <li>опыт в сфере Big Data;</li> <li>опыт работы в профильном ПО (SAS Base / MS SQL / PostgreSQL / Python/ …) – желательно.</li> </ul> <strong>Условия:</strong> <ul> <li>Абсолютно белое оформление, все по ТК РФ</li> <li>Заработная плата - оклад 210.000 - 250.000 рублей гросс + квартальные премии 20% от квартального оклада + годовая премия 10% от годового оклада</li> <li>График работы - Пнд-Птн 09:00-18:00, формат работы - гибридный (1-2 дн в офисе, 3-4 дн дома)</li> <li>Медицинская страховка, страховка для выезжающих за границу после испытательного срока</li> <li>Доплата к отпускному и больничному листу</li> <li>Дополнительные льготы при заключении брака и рождении детей</li> <li>Социальная поддержка при сложных жизненных ситуациях</li> <li>Льготное кредитование для сотрудников</li> <li>Обучение в корпоративном университете банка</li> </ul>',\n",
       " '79174677': '<p>Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, другую часть разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform.</p> <p><strong>Обязанности:</strong></p> <ul> <li>Участие в разработке ядра продуктов Data Platform</li> <li>Разработка и оптимизация процессов выгрузки данных</li> </ul> <strong>Требования:</strong> <ul> <li>Знание одного из языков программирования: Python, Java, Go</li> <li>Знание основ ООП, теории алгоритмов, структур данных</li> <li>Знание SQL</li> <li>Опыт написания процессов загрузки данных (ETL)</li> <li>Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL</li> <li>Будет плюсом знакомство с: Pandas, Numpy, Groovy</li> <li>Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul> <p> </p>',\n",
       " '79305476': '<p><strong>Вам предстоит</strong>:</p> <ul> <li>Исследование источников и разработка ETL процессов по загрузке данных из источников в DWH;</li> <li>Разработка и поддержка витрин;</li> <li>Участие в проектировании модели данных хранилища;</li> <li>Оптимизация и поддержка существующей системы.</li> </ul> <p><strong>Требуемые знания и опыт:</strong></p> <ul> <li>Опыт sql-разработки от 3х лет;</li> <li>Опыт разработки на python / любом другом ООП языке как плюс;</li> <li>Понимание преимуществ и недостатков различных подходов к проектированию DWH;</li> <li>Умение оптимизировать SQL-запросы.</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>Знание принципов и ограничений DataDriven концепции опыт в реализации подобных сервисов - как плюс;</li> <li>Опыт работы с брокерами сообщения, NiFi, airflow - как плюс;</li> <li>Опыт работы с *Nix системами как плюс.</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li>Оформление в соответствии с трудовым законодательством РФ;</li> <li>Конкурентный уровень дохода (оклад + годовой бонус);</li> <li>ДМС со стоматологией и возможностью подключения к программе своих детей и родственников;</li> <li>Прозрачную систему мотивации, которая позволяет влиять на уровень дохода;</li> <li>Работу в команде профессионалов;</li> <li>Участие в создании инновационных продуктов;</li> <li>Гибкое начало рабочего дня, пятница - сокращённый рабочий день;</li> <li>Офис в центре Москвы;</li> <li>Корпоративную мобильную связь;</li> <li>Льготную программу ипотечного и потребительского кредитования.</li> </ul> <p><strong>Ещё у нас:</strong></p> <ul> <li>Возможность вертикального и горизонтального роста;</li> <li>Бонусные программы от компаний партнёров;</li> <li>Возможность получения бонуса за закрытие вакансии по вашей рекомендации;</li> <li>Материальная помощь при рождении детей и др. семейных обстоятельствах;</li> <li>Обучение в Корпоративном университете за счёт компании;</li> <li>Участие в профильных конференциях в качестве спикера или слушателя;</li> <li>Корпоративная жизнь: спортивные комьюнити, клубы по интересам (настолки, интеллектуальные игры).</li> </ul>',\n",
       " '78664757': '<p><strong>EnjoyPro</strong> – это команда экспертов в области высоких технологий и разработки программного обеспечения.</p> <p><strong>Проект</strong> в сфере информационной безопасности корпоративных и киберфизических систем, предназначен для поведенческого анализа пользователей и сущностей с функциями выявления аномалий методами статистики и машинного обучения, а также для аналитической поддержки технического расследования (экспертизы) инцидентов безопасности.</p> <p><strong><strong>Вам предстоит:</strong></strong></p> <ul> <li> </li> <li>Работа с asyncio и библиотеками для работы с большим объемом данных и машинным обучением, такими как, pandas, scikit-learn (bigdata);</li> <li>Разработка модулей обработки (парсинга) данных от источников данных;</li> <li>Разработка и сопровождение аналитических модулей на базе технологий анализа больших данных (в т.ч. применения ML);</li> <li>Подробное документирование кода;</li> <li>Участие в code review.</li> </ul> <p><strong>Мы ожидаем от Вас:</strong></p> <p> </p> <ul> <li>Опыт разработки прикладного ПО на Python3 в парадигме ООП от 3-х лет;</li> <li>Опыт разработки асинхронных и многопоточных приложений (напр., с asyncio);</li> <li>Опыт обработки больших объемов данных на потоке (bigdata);</li> <li>Опыт взаимодействия с базами данных, хранящими более 1 млрд. записей;</li> </ul> <p><strong>Специализированные знания и навыки:</strong></p> <ul> <li>Знание алгоритмов обработки и структур данных (в т.ч. библиотек pandas, scikit-learn и др.);</li> <li>Понимание микросервисной архитектуры, распределенных систем;</li> <li>Навыки работы с Docker;</li> <li>Навыки работы с Git</li> <li>Знание JavaScript\\\\TypeScript, WebSocket, gRPC</li> <li>Навыки работы с брокерами сообщений Kafka, RabbitMQ и Redis</li> <li>Навыки работы с ClickHouse\\\\MongoDB\\\\PostgresSQL</li> <li>Навыки работы с Kubernetes</li> </ul> <p><strong>Мы готовы предложить:</strong></p> <ul> <li><strong>Аккредитованная</strong> <strong>ИТ Компания</strong>, предоставляющая право на льготную ипотеку, также и право подачи документов на <strong>отсрочку от мобилизации</strong> для профильных ИТ специалистов в соответствие с приказом!</li> <li>Удаленная работа;</li> <li>График работы 5/2 с гибким началом рабочего дня;</li> <li>Оформление на выбор - ТК РФ, ИП, Самозанятость;</li> <li>Амбициозные проекты, интересные с профессиональной точки зрения задачи;</li> <li>Широкий технологический стек;</li> <li>Возможность профессионального и карьерного роста - в динамично развивающейся компании;</li> </ul> <p><strong>Ждём тебя в нашей команде!</strong></p>',\n",
       " '79127417': '<p><strong>Обязанности:</strong></p> <ul> <li>Разработка и поддержка ETL задач при обмене данными между компаниями крупного холдинга, а также внутри Data Lake в рамках развития корпоративной BI системы на базе SQL + Python.</li> <li>Разработка и поддержка хранилищ данных и витрин данных, построенных на технологиях: PostgresSQL/MS SQL и MongoDB по методологии Data Vault (при необходимости будет организовано обучение по Data Vault 2.0)</li> <li>Поддержка пользователей по возникающим вопросам в части использования отчетов и сервисов BI системы.</li> <li>Поддержка процессов Data Cleansing</li> <li>Поддержка работоспособности BI системы в рамках SLA</li> </ul> <p><strong>Ключевые требования:</strong></p> <ul> <li>Знание SQL (включая SQL Procedures) на среднем уровне</li> <li>Знание Python для решения ETL задач на среднем/продвинутом уровне</li> <li>Знание английского языка для свободного чтения документации, форумов и статей</li> <li>Хорошие коммуникативные навыки, самостоятельность и стрессоустойчивость.</li> <li>Готовность и умение осваивать новые технологии</li> </ul> <p><strong>Существенными преимуществами кандидата будут являться:</strong></p> <ul> <li>Опыт применения Python для решения задач машинного обучения</li> <li>Опыт построения хранилища по методологии Data Vault 2.0</li> <li>Опыт работы на PostgreSQL и/или ClickHouse</li> <li>Опыт работы с MongoDB на начальном/среднем уровне</li> <li>Опыт работы с ElasticSerach на начальном/среднем уровне</li> <li>Знание JavaScript на начальном/среднем уровне</li> <li>Опыт администрирования баз данных Postgres</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>Трудовой договор</li> <li>Удаленная работа</li> <li>Работа в успешной быстроразвивающейся компании</li> <li>Активный профессиональный рост</li> </ul>',\n",
       " '79232504': '<p>Siberian Wellness – больше, чем просто производственная компания с 26-летней историей. Это – международный бренд, чьи продукты любят Клиенты в более чем 60 странах мира. А сотни тысяч предпринимателей в Европе, Азии, Америке, ведут бизнес с нами. Мы производим биологически активные добавки, витамины, косметику, парфюмерию, спортивное и функциональное питание для активного образа жизни и отличного самочувствия. Мы стремительно развиваемся, и не боимся ломать привычные рамки и стереотипы.</p> <p><strong>О компании:</strong></p> <ul> <li> <p>Полный цикл – разработка, собственное производство, продвижение и продажи</p> </li> <li> <p>Мы имеем собственные торговые представительства в 17 странах, торговые представительства по франчайзингу в 10 странах, 5 распределительных центров, фирменную сеть из 99 офлайн-магазинов и 20 интернет-магазинов, а также 1285 магазинов, работающих с нами по франчайзингу.</p> </li> </ul> <p>В команду ждем компетентного коллегу на позицию инженера данных.</p> <p><strong>Наша цель:</strong> обеспечить аналитику бизнеса качественными и бесперебойно поставляемыми данными.</p> <p>Нам необходим владелец процесса, полностью отвечающий за хранилище и витрины данных для целей BI и ML.</p> <p>Под владением процессом мы подразумеваем:</p> <ul> <li>Выбор оптимального стека технологий,</li> <li>Проектирование баз данных/хранилищ,</li> <li>Построение DW/DW+Data Lake,</li> <li>Реализация процессов ETL/ELT; загрузка данных batch/streaming,</li> <li>Подготовка витрин данных для BI в связке с Oracle-разработчиком,</li> <li>Разработка и поддержка системы мониторинга качества данных. Алерты о проблемах с данными бизнесу / ДИТ, помощь в изменении культуры данных компании,</li> <li>Контроль обновлений витрин данных, системное устранение проблем с обновлениями,</li> <li>Тестирование качества витрин данных после вносимых изменений,</li> <li>Контроль и оптимизация производительности.</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>у вас есть практический опыт работы на позиции senior data engineer. Опыт работы в связке с аналитиками и data scientists,</li> <li>опыт построения пайплайнов сбора и обработки данных,</li> <li>разработка автоматизированных инструментов оценки качества данных, повышение их эффективности и надежности / создание и развитие платформы мониторинга качества данных / поддержка тестовых баз данных,</li> <li>технический бэкграунд: PL/pgSQL, чистый SQL, Python,</li> <li>Самостоятельность и инициативность,</li> <li>Высокая внутренняя мотивация наводить порядок и совершенствовать мир данных вокруг себя,</li> <li>Системность,</li> <li>Аналитический склад ума,</li> <li>Креативность,</li> <li>Хорошие коммуникативные способности.</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>Возможна удаленная работа и\\\\или работа в комфортном офисе в г. Новосибирске,</li> <li>График работы: с 9-00 до 18-00, сб, вс - выходные.</li> <li>Возможности для профессионального и личностного роста.</li> <li>Сильный и дружелюбный коллектив.</li> <li>Скидка 50% на продукцию Компании,</li> <li> <p>Высокий уровень вознаграждения: должностной оклад + бонусы по итогам достижения KPI.<br /><br />По итогам знакомства и обсуждения вакансии готовим индивидуальный job-offer.</p> </li> </ul> <p> </p> <p> </p>',\n",
       " '77426640': '<p><strong>Платформа ОФД (аккредитованная ИТ-компания) – крупнейший в России оператор фискальных данных. Резидент Сколково.</strong><br />У нас много данных: каждый 3-й чек, пробиваемый в России, находится на наших серверах (50 млн. чеков в день, 2 млрд уникальных названий товаров).<br /><br />Одним из направлений бизнеса является аналитика, основанная на чековых данных. Направление BigData занимается тем, что из плохо структурированной информации делает аналитику, которая превращается в отчеты и продается.<br /><br />Приглашаем на работу Tech Lead Data Engineer</p> <p><strong>Что у нас есть:</strong></p> <ul> <li>График работы гибрид, (1-2 раза в неделю в офисе, остальное время дома)</li> <li>Много данных: каждый 3-й чек, пробиваемый в России, находится на наших серверах. Мы принимаем до 50 млн чеков в день, имеем 2 млрд уникальных названий товаров в базе</li> <li>Развитая инфраструктура: есть несколько кластеров Hadoop, мощные машины, GPU</li> <li>Команда Big Data: продакты, аналитики, ресечеры, DS и DE, java-разработчики</li> <li>80% кода мы пишем на Scala</li> <li>Железо и лицензии от компании</li> <li>Оформление по ТК РФ, белая зп</li> <li>Отсрочка от мобилизации, согласно условиям Минцифры</li> <li>ДМС (поликлиника, стоматология, госпитализация, скорая)</li> <li>Скидки в фитнес-клубы, мерч, подарки детям к праздникам</li> <li>Офис в 50 метрах от м. Спортивная/ МЦК Лужники.</li> <li>Удобная кухня, релакс-зона с тренажером, массажным креслом, приставкой и караоке</li> <li>Открытая рабочая атмосфера: ежемесячные статус-митинги с топами</li> <li>Пицца-пати за счет компании и корпоративные праздники</li> </ul> <p><strong>Наш стек:</strong></p> <p>Hadoop, Spark, Hive, SCALA, Python, Java, PostgreSQL, ClickHouse, Zeppelin/IntelliJ, AirFlow, ElasticSearch, Apache Superset</p> <p>GitLab, Docker, Jira, Confluence</p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Управлять командой, состоящей из 4 Data Engineer`s</li> <li>Оптимизировать инфраструктуру и внутренние сервисы по обработке больших данных</li> <li>Разрабатывать ETL-процессы с использованием библиотеки Apache Spark на Scala</li> <li>Сотрудничать с DS для внедрения математических алгоритмов и ML-моделей в промышленные процессы</li> </ul> <p><strong>Откликайся, если ты:</strong></p> <ul> <li>Аналогичный опыт работы от <strong>3-х лет</strong>, обязателен опыт управления командой</li> <li>Имеешь опыт управления командой</li> <li>Пишешь на Scala, Python SQL</li> <li>Знание Java будет плюсом</li> <li>Используешь инструменты BigData: Airflow, Hadoop, Spark, Hive, Zeppelin</li> </ul> <p> </p>',\n",
       " '79221567': '<p>Компания, специализирующаяся на бизнес-аналитике (BI), расширенной аналитике данных (Advanced Analytics) и архитектуре данных для бизнес-аналитики, приглашает Data Engineer</p> <p> </p> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Заработная плата от 80 000 рублей оклад+бонусы;</li> <li>полностью удаленно;</li> <li>график 5/2, ненормированный рабочий день;</li> <li>дружная профессиональная команда;</li> <li>официальной трудоустройство, гарантии и компенсации в соответствии с ТК РФ;</li> <li>внутренние и внешние конференции, форумы, курсы повышения квалификации;</li> <li>возможность быстрого профессионального и карьерного роста.</li> </ul> <p> </p> <p><strong>Обязанности:</strong></p> <ul> <li>Участие в полном цикле разработки Data engineering;</li> <li>проектирование и разработка архитектуры баз данных, структур данных, таблиц, словарей;</li> <li>разработка процессов с использованием средств ETL Informatica Power Center, SQL Server Integration, IICS, AzureDataFactory(ADF), Databricks.</li> </ul> <p> </p> <p>.</p> <p><strong>Требования:</strong></p> <ul> <li>Опыт работы от года;</li> <li>высшее техническое профильное образование (Прикладная математика, Прикладная информатика, Бизнес информатика…);</li> <li>разговорный английский.</li> </ul> <p> </p> <p>Отличное знание:</p> <ul> <li>IICS,</li> <li>AzureDataFactory (ADF),</li> <li>Databricks,</li> <li>SQL,</li> <li>Excel</li> </ul>',\n",
       " '78879858': '<p><strong>Стань частью команды знаменитого горного курорта России!</strong></p> <p>Ищем Data Engineer’а, который подхватит унаследованную систему (MS стек) аккумулирующую информацию в DWH из 10 систем отелей с визуализацией в Power BI. На этом базисе закроем текущие\\\\горящие потребности бизнеса и начнем строить свою систему обработки данных с блэк-джеком и Industry Standard стеком. Не штампа ради, а для констатации факта – позиция на которой можно будет наглядно видеть какой вклад в бизнес вносит команда и ты лично: померить, посчитать, получить обратную связь! Плюс построить систему обработки данных по своему, базируясь на удобстве, эффективности и целесообразности, а не «потому что так исторически сложилось».</p> <p><strong>Какие задачи необходимо решать:</strong></p> <ul> <li>Разработка, поддержка коннекторов к системам-источникам данных</li> <li>Весь набор манипуляций с пайплайнами: проектирование, разработка, поддержка</li> <li>Администрирование инструментов и собственно обеспечение качества данных</li> <li>Строительство DWH: проектирование, оптимизация, мониторинг</li> <li>Создание витрин и визуализация данных в BI</li> <li>Ad-hoc запросы</li> </ul> <p><strong>Для решения этих задач требуется:</strong></p> <ul> <li>Опыт работы DE от 2-х лет</li> <li>Хорошее знание Python и SQL, опыт оптимизации запросов</li> <li>Опыт проектирования и разработки промышленной DWH</li> <li>Опыт оркестрации ETL на Airflow или аналогах</li> <li>Знания и опыт администрирования MySQL, PostgreSQL, MSSQL</li> <li>Знание принципов и подходов к обеспечению качества данных и владение соответствующими инструментами</li> <li>Навыки работы с BI-системами, значительным плюсом будет уверенное владение Power BI</li> </ul> <p><strong>Вашим преимуществом будут:</strong></p> <ul> <li>Навыки в работе с аналитическими BD</li> <li>Понимание принципов потоковой обработки данных</li> <li>Облачные технологии</li> </ul> <p><strong>Условия:</strong></p> <ul> <li> <p>Работа на территории уникальной природной зоны в г. Сочи, курорт Красная Поляна, офис с видом на горы и реку</p> </li> <li> <p>Предоставляется проживание на территории курорта на льготных условиях</p> </li> <li> <p>Корпоративный трансфер из Сочи/Адлера</p> </li> <li> <p>Бесплатный ски-пасс на все канатные дороги курорта</p> </li> <li> <p>Предоставление скидок на услуги курорта, в кафе/ресторанах/магазинах и развлекательных объектах на территории курорта</p> </li> <li> <p>График работы: 5/2 с 9.00 до 18.00 (возможен гибрид в среднесрочной перспективе)</p> </li> <li>Оклад + квартальный KPI. Итоговый уровень заработной платы обсуждается с финальным кандидатом</li> </ul> <p><em>Благодарим за интерес к нашей Компании!<br />Просмотр отклика на вакансию означает, что резюме находится на рассмотрении, однако не является приглашением на собеседование.<br />Если в течение 14 дней после отклика не поступает приглашение на собеседование, то Ваше резюме автоматически попадает в резервную базу кандидатов.<br />Мы свяжемся с Вами при открытии более подходящей вакансии.</em></p> <p><em>С уважением,</em></p> <p><em>Группа по подбору персонала НАО «Красная поляна»</em></p>',\n",
       " '78258371': '<p>АНО &quot;Аналитический центр города Нижнего Новгорода&quot; - первый в России муниципальный аналитический центр.</p> <p>Организация занимается разработкой информационных систем для Администрации города Нижнего Новгорода и имеет статус аккредитованной IT компании. Наша основная задача - сбор данных из информационных систем Нижнего Новгорода, их систематизация, хранение, и предоставление администрации города для анализа и принятия управленческих решений.</p> <p>Всвязи с расширением, мы открываем набор на позицию &quot;Инженер по аналитике и работе с большими данными&quot;.</p> <p><strong>Обязанности:</strong></p> <ul> <li>Сбор и обработка данных из различных источников;</li> <li>Разработка автоматизированных решений для получения данных;</li> <li>Проектирование и разработка интеграций между информационными системами;</li> <li>Подготовка BI отчетности с использованием Yandex.DataLens;</li> <li>Работа с базой даных PostgreSQL;</li> <li>Поддержка и доработка REST API.</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Наличие высшего технического/экономического образования будет плюсом;</li> <li>Знание Python с библиотеками для обработки данных;</li> <li>Понимание архитектуры REST API;</li> <li>Навыки работы с PostgreSQL. Навыки работы с ClickHouse будут преимуществом.</li> <li>Умение строить визуализацию данных с использованием BI систем. Понимание концепции DDDM. Опыт работы с Yandex.DataLens будет преимуществом;</li> <li>Опыт работы с большими данными.</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>Официальное трудоустройство;</li> <li>Льготы и бонусы аккредитованной IT компании;</li> <li>Заработная плата: оклад и надбавки стимулирующего характера;</li> <li>График работы: пн-чт 9:00 - 18:00, пт 9:00 - 17:00;</li> <li>Работа в офисе АНО &quot;АЦГ&quot;, расположенном в историческом центре города;</li> <li>Дружная команда и широкие возможности карьерного роста;</li> <li>Профессиональное личностное развитие: тренинги личностного роста, возможны профессиональные курсы;</li> <li>Корпоративные мероприятия.</li> </ul> <ul> </ul>',\n",
       " '79314135': '<p>У нас в DWH очень много данных: 6000 объектов, 200 тб в Greenplum и 2 пт в Hadoop. С данными активно работают более 3000+ аналитиков в бизнес командах. На стороне команды DWH созданы инструменты, которые позволяют аналитику не зависеть от etl разработчика и самостоятельно собирать данные для своих задач. Сейчас мы ищем экспертов sql, готовых помогать аналитикам оптимально работать с данными.</p> <p>Наша инфраструктура</p> <p>• Greenplum / Hadoop / Clickhouse в качестве ядра обработки данных</p> <p>• Собственные решения на базе Apache Zeppelin, Apache Airflow и Apache Flink в качестве инструментов трансформации данных для аналитиков (selfservice etl)</p> <p>Мы не требуем знания этих инструментов от кандидатов. Наша задача сделать инструменты работы с данными максимально удобными и доступными. Но нам важно, чтобы кандидат на эту позицию имел большой опыт работы с данными.</p> <p><strong>Обязанности</strong></p> <ul> <li>Быть экспертом по данным: помогать аналитикам в решении вопросов с источниками данных, моделью данных DWH</li> <li>Разрабатывать витрины в помощь аналитикам</li> <li>Выступать заказчиком для разработки витрин в смежных командах</li> <li>Оптимизировать существующие запросы</li> <li>Внедрять и развивать культуру написания оптимальных запросов</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Высшее техническое образование</li> <li>Опыт работы с базами данных в качестве разработчика от 1 года</li> <li>Свободное владение SQL</li> <li>Опыт проектирования объектов БД на основании бизнес требований</li> <li>Понимание теории СУБД и ETL-процессов</li> <li>Знакомство с ETL-инструментами будет плюсом</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '78306430': '<p><em>DataGo! (ex OWOX Russia) — одна из сильнейших команд в сфере маркетинговой аналитики в СНГ со своими собственными продуктами, решениями и центром экспертизы с многолетним опытом. DataGo! дает качественные данные аналитикам и прикладные отчеты маркетологам и product менеджерам.</em></p> <p><em>• Нам доверяют более 100 крупнейших клиентов РФ в сферах: E-commerce, Banking, Telecom, Pharma.</em></p> <p><em>• Официальный партнер Яндекс.Метрика и Яндекс.Cloud.</em></p> <p><em>• Самое большое количество успешных кейсов с использованием Google Cloud и Google Analytics среди всех партнеров Google в СНГ.</em></p> <p><em>Сейчас мы ищем человека с опытом на стыке бэкенд-разработки и дата-инженерии, который усилит нашу продуктовую команду и попадет в самое сердце разработки DataGo!</em></p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Разрабатывать и улучшать наш продукт – стриминг данных – с помощью <strong>FastAPI</strong>; Сервис высоконагруженный – около 4000 RPS;</li> <li>Работать с интеграциями – разрабатывать коннекторы и пайплайны на <strong>aiohttp</strong><strong>, </strong><strong>asyncio</strong>;</li> <li>Собирать, обрабатывать и доставлять данные в базы заказчиков (<strong>CLickhouse</strong><strong>/</strong><strong>GBQ</strong>) с помощью <strong>Airflow</strong>.</li> </ul> <p><strong>Для нас важно:</strong></p> <ul> <li><strong>Наличие опыта разработки на </strong><strong>Python</strong><strong> (</strong><strong>Pandas</strong><strong>, </strong><strong>Numpy</strong><strong>), с асинхронным стэком в частности (</strong><strong>FastAPI</strong><strong>, </strong><strong>asyncio</strong><strong>, a</strong><strong>iohttp</strong><strong>) – от 2-ух лет;</strong></li> <li><strong>Опыт работы с БД: </strong><strong>Clickhouse</strong><strong>, </strong><strong>PostgreSQL</strong><strong>, будет плюсом – </strong><strong>GBQ</strong><strong>;</strong></li> <li><strong>Опыт разработки и общее понимание архитектуры высоконагруженных, масштабируемых сервисов;</strong></li> <li>Навыки работы с виртуальными машинами, Сelery, Redis, RabbitMQ, Nginx;</li> <li>Навыки работы с облачными серверами – Google Cloud, Яндекс Облако.</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li><strong>Оформление в аккредитованную ИТ-компанию со всеми преимуществами или оформление сотрудничества за рубежом, в любом случае - полностью удаленная работа из любой точки мира;</strong></li> <li>Конкурентную заработную плату <strong>до 250 000 рублей на руки</strong>;</li> <li>Возможность работать в команде, которая сохранила <strong>дух стартапа</strong> – нас немного, мы очень классные и не позволяем никакой бюрократии влезать в наши процессы.</li> </ul> <p><em>Если считаешь, что все вышесказанное про тебя — ждём отклик!</em></p>',\n",
       " '71220088': '<p><strong>Мы специализируемся на разработке программного обеспечения больших, высоконагруженных и производительных систем, созданием сервисов и клиентских приложений для букмекерских компаний.</strong><br /><br />В настойщий момент в команду Data Science ищем опытного Data Engineer для решения вопросов с данными для построения моделей оттока, антифрода, рекомендаций, разработки дашбородов для мониторинга ключевых клиентских метрик.</p> <p>Вас ждут миллионы клиентов и миллиарды транзакций, возможность раскрыть свой потенциал и получить удовлетворение от того, как результаты вашего труда дают эффект в реальном бизнесе.</p> <p><br /><strong>Чем предстоит заниматься:</strong></p> <ul> <li> <p>Извлечение, преобразование, загрузка данных и их обработка</p> </li> <li> <p>Построение датасетов для data science моделей и аналитики</p> </li> <li> <p>Построение надежных и оптимальных пайплайнов обработки данных</p> </li> <li> <p>Интеграции с новыми источниками данных</p> </li> <li> <p>Оптимизация вычислений и затрат на хранение данных</p> </li> </ul> <p><strong>Что мы ожидаем от кандидата:</strong></p> <ul> <li> <p>Уверенные знания SQL и опыт работы с базами данных;</p> </li> <li> <p>Опыт написания процессов загрузки данных ETL</p> </li> <li> <p>Уверенное знание Python (Pandas, Numpy), опыт разработки, желание писать аккуратный и красивый код</p> </li> <li> <p>Желателен опыт работы с командной строкой Linux (Bash) и GitHub</p> </li> <li> <p>Опыт работы с Hadoop, Pyspark будет преимуществом</p> </li> <li> <p>Желательно понимание терминов Data Sciencе при работе с данными (обучающая и валидационная выборки, Data Leak)</p> </li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li> <p>Оформление в штат компании, полное соблюдение ТК РФ (отпуск, больничный), официальная заработная плата</p> </li> <li> <p>Персональный гибкий график</p> </li> <li> <p>После испытательного срока мы подключаем ДМС и еженедельное посещение спорт зала</p> </li> </ul> <p><em><strong>А еще:</strong></em></p> <ul> <li> <p>Лучшая техника (Apple) и софт</p> </li> <li> <p>Интересная предметная область и сложные технические задачи, возможность развития и роста</p> </li> <li> <p>Просторный и современный офис в 10 минутах пешком от м. Домодедовская, в котором комфортно работать и приятно общаться с коллегами</p> </li> <li> <p>Бизнес-ланчи в нескольких ресторанах рядом с офисом Компании</p> </li> <li> <p>Уютные кухни-столовые с чаем, кофе и конфетами<br /><br /><strong>Для того, чтобы мы быстрее связались с Вами, просим ответить на простой вопрос при отклике.</strong></p> </li> </ul>',\n",
       " '79174702': '<p>Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, другую часть разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform.</p> <p><strong>Обязанности:</strong></p> <ul> <li>Участие в разработке ядра продуктов Data Platform</li> <li>Разработка и оптимизация процессов выгрузки данных</li> </ul> <strong>Требования:</strong> <ul> <li>Знание одного из языков программирования: Python, Java, Go</li> <li>Знание основ ООП, теории алгоритмов, структур данных</li> <li>Знание SQL</li> <li>Опыт написания процессов загрузки данных (ETL)</li> <li>Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL</li> <li>Будет плюсом знакомство с: Pandas, Numpy, Groovy</li> <li>Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul> <p> </p>',\n",
       " '75825082': '<p><em>Крупная компания (РФ и РБ), дистрибьютор запасных частей к европейским грузовым автомобилям приглашает на работу инженера по данным (Data Engineer)</em></p> <p><em>Мы развиваем сложные информационные системы, чтобы грузовой транспорт мог ежедневно доставлять товары в разные точки страны.<br />Мы разрабатываем собственное комплексное решение с функциями товародвижения, системы управления складом и двором (WMS,YMS), блоком взаимоотношений с клиентом (CRM) и продажами (POS), с блоком аналитики.<br />Имеем развитую систему филиалов и магазинов, интегрированных в корпоративную сеть и связанных с ЦОДами.</em></p> <p><em>Ищем в команду инженера по данным, который готов продолжить своё развитие в высоко технологичной и динамически развивающейся компании</em></p> <p><strong>Обязанности:</strong></p> <p>- создание корпоративного хранилища данных (DWH);</p> <p>- развитие и переработка существующих OLAP-кубов;</p> <p>- развитие ETL процессов;</p> <p>- подготовка и сопровождение витрин данных для BI визуализации и отчетов;</p> <p>- построение плоской, многомерной отчетности и дашбордов;</p> <p>- технологический стек:</p> <p>Основа - MS SQL Server/Integration/Analysis/Reporting Services/Power BI</p> <p><strong>Требования:</strong></p> <p>- опыт работы в качестве разработчика баз данных, DWH, ETL или OLAP в течение 2 лет;</p> <p>- знание теории и практический опыт построения баз данный, хранилищ данных;</p> <p>- отличное знание T-SQL (планы выполнения, оптимизация);</p> <p>- MS SQL Server (Database Engine, Integration Services, Analysis Services, Reporting Services, Power BI);</p> <p>- базовые знания MDX, DAX;</p> <p><strong><em>Желательно (будет плюсом):</em></strong></p> <p>- базовые знания платформы .NET;</p> <p>- знание одной из торговых учетных систем (1С, SAP, и др.);</p> <p>- экспертное знание принципов работы MS SQL (хранение данных, индексы, статистика, транзакции, уровни изоляции транзакций, блокировки)</p> <strong>Условия:</strong> <p>- на испытательные срок – 5/2 в офисе;<br />- Возможен гибридный график работы после испытательного срока;<br />- офис расположен по адресу 2-я Мелитопольская ул. метро Бульвар Дмитрия Донского (в шаговой доступности от жд. ст. Бутово;<br />- позитивная, созидательная среда и интересные проекты;<br />- работа в команде профессионалов, неравнодушных, активных, ответственных за свой результат.</p> <p>Почему у нас хорошо?<br />Стабильность. Компания экономически устойчива и растёт каждый год.<br />Честность. Мы работаем с полным соблюдением законодательства.<br />Масштаб. Вы станете участником крупных проектов, полностью меняющих бизнес-процессы.<br />Интерес. Вы попадёте в насыщенную событиями рабочую атмосферу крупной команды ИТ.<br />Доверие. Лояльное руководство, отсутствие «чайка»-менеджмента.<br />Комфорт. Мы за work-life balance – стараемся жить без переработок и отдыхать в выходные.</p>',\n",
       " '78617302': '<p><strong>Rubius</strong> – IT-компания со смелым характером. Мы разрабатываем софт для клиентов из различных отраслей – от промышленности и нефтегаза до ритейла и медицины. Обосновались в Томске, работаем по всему миру: наше программное обеспечение используют в США, Европе и Азии. Наши решения используют Apple, Tesla, Kaspersky, Amazon, IBM, Uber, Netflix, Газпром, РЖД и другие. В группу компаний входят представительства в США (Нью-Йорк), Казахстане (Алматы, резиденты Astana hub), ОАЭ (Дубай). В нашем профиле на hh.ru мы постарались подробно рассказать о нас, обязательно загляните:)</p> <p>Одна из наших команд, которая занимается искусственным интеллектом, приглашает <strong>Data Engineer, так как команда расширяется</strong>. Вам предстоит работать с большими данными совместно с аналитиками и командой ML. Мы ищем человека, который умеет собирать витрины, джойнить данные и проверять полученный результат. У нас есть экспертиза по синтезу аудио и видео, анализу изображений и видео, компьютерному зрению, предиктивной аналитике, обработке текстовых данных и тд. Мы исследуем разные сферы от логистики до медицины, погружаемся в сферы наших крупных заказчиков с головой, чтобы помочь оптимизировать процессы. Также команда разработала свой продукт для видео аналитики Visius.</p> <p><strong>Чем предстоит заниматься: </strong></p> <ul> <li> <p>проектировать и собирать витрины данных по разработанному ТЗ</p> </li> <li> <p>проектировать, разрабатывать и поддерживать ETL-процессы для загрузки данных из/в Data Lake</p> </li> <li> <p>тестировать результаты преобразования данных и проверять их целостность</p> </li> <li> <p>писать документацию - комментировать код</p> </li> <li> <p>работать с data-аналитиками для создания новых и оптимизации существующих витрин</p> </li> </ul> <p><strong>Добро пожаловать к нам в команду, если есть:</strong></p> <ul> <li> <p>понимание основных операций ДБ и DWH</p> </li> <li> <p>опыт работы с Hadoop технологиями (Spark, Hive и тд)</p> </li> <li> <p>хорошее знание SQL, Python</p> </li> <li> <p>опыт работы с Azure/Yandex облачными платформами</p> </li> <li> <p>опыт работы с Airflow, Kafka будем плюсом</p> </li> </ul> <p><strong>Что мы предлагаем:</strong></p> <p>Сотрудники компании – главная ценность Rubius. Мы поддерживаем свободу творчества и полёт инженерной мысли. Стремимся, чтобы каждый участник нашей команды раскрыл свой потенциал. Мы стараемся максимально заботиться о наших сотрудниках. Здесь удалённые и офисные команды чувствуют себя максимально комфортно.</p> <p><strong>Про работу и оплату</strong></p> <ul> <li> <p>белая и своевременная заработная плата в зависимости от компетенций и уровня</p> </li> <li> <p>официальное трудоустройство</p> </li> <li> <p>до 10% ежемесячной премии за хорошие результаты</p> </li> <li> <p>помощь с home office</p> </li> <li> <p>возможно трудоустройство в нашей компании в Казахстане (для желающих получить заветную карту Visa)</p> </li> </ul> <p><strong>Про рост и развитие</strong></p> <ul> <li> <p>индивидуальный трек развития по желанию</p> </li> <li> <p>бесплатное обучение английскому языку</p> </li> <li> <p>бонус за профессиональное развитие (курсы, подкасты, литература по хард и софт скиллам)</p> </li> <li> <p>компенсация 50% за профессиональную сертификацию</p> </li> <li> <p>внутренние митапы на разные темы</p> </li> </ul> <p><strong>Про офис, плюшки и атмосферу</strong></p> <ul> <li> <p>оплачиваемые занятия спортом (даже в домашних условиях)</p> </li> <li> <p>программа ДМС после испытательного срока</p> </li> <li> <p>скидка для вас и родственников в Rubius Academy</p> </li> <li> <p>бонусы к рождению детей и свадьбе</p> </li> <li> <p>классные корпоративы и активности</p> </li> <li> <p>развитая и комфортная корпоративная культура, без иерархии и бюрократии</p> </li> </ul> <p>А ещё у нас есть лучший офис в Томске, где тебя всегда ждут, сообщества по интересам (футбол, теннис, своя музыкальная группа, шахматный клуб...) и коллектив, где прислушиваются к мнению каждого. Подробнее о нашей компании можно почитать в нашем профиле на hh.ru.</p> <p>Откликайтесь!</p> <p> </p>',\n",
       " '79179730': '<p><strong>LATOKEN is the supermarket of assets where it is easy to discover, exchange, earn and spend crypto.</strong></p> <p><strong>Mission:</strong> Well-structured and secure data and monitoring.</p> <p><br /><strong>Story</strong>: Product, Finance, OKRs, and Dash Teams need well-structured and secure data and monitoring to analyze and track the improvement of the services and product performance.</p> <p><strong>Key PROBLEM&#39;s</strong>:</p> <ul> <li>The monitoring system alerts for any downtime, latency, or success rate issues of the services.</li> <li>All company data and events are merged and consolidated into the Data Base (DWH, PostgreSQL, Segment, Indicative, Google Analytics) to support the Monitoring system, Token Discovery, OKRs and Analytical Dashes.</li> <li>Acquisition channels, conversions, and users are correctly mapped and easy to access.</li> <li>Token DB is representative of the market and token data is correct and complete with on-chain and smm data. (30k tokens by end of April)</li> </ul> <p><strong>How</strong>:</p> <ul> <li>Create new data pipelines for business need.</li> <li>Maintain scripts and pipelines in case of problems or new requirements.</li> <li>Define necessary events, set up recording and make available for reports and dashboards.</li> <li>Maintain and update token DB.</li> </ul> <p><strong>Constraints</strong>:</p> <ul> <li>All OKRs of the company are consolidated into Platform Ops.</li> <li>All scripts are consolidated into Gitlab.</li> </ul> <p><strong>Main performance number:</strong> Ops OKR Health<br /><strong>Second performance number:</strong> Free Space DWH<br /><strong>Third performance number:</strong> Query response time, sec avg<br /><br /><strong>Functions:</strong><br />• OKRs Health : Calculate OKRs correctly, daily and in time<br />• DWH objects audit<br />• Queries : Advice on optimization and writing queries to databases.<br />• Data quality : Keep data in order and provide reliable reporting<br />• Automation : Automation of OKRs and slack bots for automated ops processes<br />• Audit : Audit of DWH state and automated scripts<br /><br /><strong>Requirement skills and experience:</strong></p> <p>2+ years of PostgresSQL DB administration experience;</p> <p>2+ years of experience writing Python scripts and applications for loading data;</p> <p>Experience in configuring a database in a high-availability architecture;</p> <p>Experience in database optimization for different load profiles;</p> <p>Experience in automating data loading from different sources (CRM, ERP, Web resorses)</p> <p>Eager to work with people with high-performance standards</p> <p><strong>Will be a plus:</strong></p> <p>Experience with financial data</p> <p>Experience as a data/product analyst</p>',\n",
       " '79267194': '<p><em>Мы - IT компания, специализирующаяся на разработке «умных» информационных систем для медицины и ищем в свою команду Data Engineer. Занимаемся реализацией проекта федерального масштаба в сфере здравоохранения.</em></p> <p><strong>Обязанности:</strong></p> \\u200b <ul> <li>Взаимодействие с data scientist’ами и аналитиками, работа над поиском новых способов генерации и улучшения качества данных.</li> <li>Data modeling, создание и поддержка датасетов и инструментов для их генерации.</li> <li>Развитие ETL и data ingestion пайплайнов.</li> <li>Развитие и улучшение существующих сервисов.</li> <li>Участие в проработке и реализации интеграций с другими сервисами и командами.</li> </ul> <p>Стек: Python, Pandas, Postgres, Airflow, S3.</p> <p><strong>Требования:</strong></p> <ul> <li>Опыт работы Data Engineer от 2х лет, предпочтительно с подготовкой датасетов для ML</li> <li>Представление о статистическом анализе данных, методах ML</li> <li>Отличное знание SQL, Python;</li> <li>Опыт построения аналитического хранилища данных.</li> </ul> <strong>Условия:</strong>\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b <ul> <li>Вы погрузитесь в специализацию IT в медицине и станете экспертом в этой области.</li> <li>Будете участвовать в реализации глобальных изменений и вместе с нами будете менять мир.</li> <li>Будете работа в коллективе профессионально зрелых и интересных коллег.</li> <li>Работа в атмосфере результативности и системности в сочетании с гибкими подходами к решению задач.</li> <li>Работа в офисе, гибридный и дистанционный форматы.</li> <li>Комфортный дресс-код smart casual и джинсы, чай на уютной кухне.</li> <li>Офис в 2 минутах от метро Менделеевская.</li> </ul>',\n",
       " '78386771': '<p><strong>AgentApp</strong> — платформа для бизнеса, которая позволяет запустить продажи страховок со своего сайта или моб. приложения для клиентов со всей страны. Над продуктом работает единая команда разработчиков, аналитиков, маркетологов, а также экспертов в страховом бизнесе.</p> <p><strong>200 000 000р.+</strong> — ежемесячный объем продаж через платформу.</p> <p><strong>AgentApp</strong> — активно развивающийся стартап, позволяющий сотрудникам быстро развиваться профессионально и прокачивать скиллы. Наш продукт интегрирован в моб.приложения <strong>Сбербанк, OZON, ВТБ, МТС, Райффайзен, Газпром</strong> и имеет аудиторию свыше 70 млн пользователей.</p> <p>Мы находимся в поиске крутого специалиста, который поможет развивать наш Big Data проект!</p> <p><strong>Что нужно будет делать:</strong></p> <ul> <li>Разработка моделей скоринга;</li> <li>Построение упрощенной модели по характеристикам авто и данным об убытках за пару лет;</li> <li>Разработка моделей сегментации клиентов на основании их профилей и анализа их поведения.</li> <li>Мониторинг текущих показателей, обеспечением чистоты входящих данных, улучшением текущих моделей.</li> <li>Разработка инструментов мониторинга;</li> <li>Исследование тенденций и технологий в сфере Data Science, AI и Machine Learning с целью их последующего применения.</li> </ul> <p><strong>Ждем от кандидата:</strong></p> <ul> <li> <p>Опыт подготовки неструктурированных данных для обучения модели (feature engineering);</p> </li> <li> <p>Прикладной опыт выведения моделей в продакшн;</p> </li> <li> <p>Навык работы с базами данных: владение SQL / Hive / Hadoop / SQL Alchemy;</p> </li> <li> <p>Опыт реализации классических ML-моделей: регрессия, классификация, кластеризация, понижение размерности данных;</p> </li> <li> <p>Навыки Python-разработчика: работа с файлами - прочитать/записать, работа с АПИ http-сервисов через xml / json (SOAP/REST).</p> </li> <li> <p>Опыт работы с python-библиотеками (logging, yaml, xml, json, requests, threading) + ORM (sqlalchemy, ponyORM, tortoise) + навыки в ООП (работа через классы).</p> </li> </ul> <p><strong>Этапы интервью:</strong></p> <p>1. HR интервью (30 мин). Знакомство с вакансией.</p> <p>2. Техническое интервью с CTO и тимлидом. (1-2 часа)</p> <p>3. Знакомство с СЕО (30 мин)</p> <p>4. Оффер.</p> <p><strong>С нас:</strong></p> <ul> <li>Работа над инновационном продуктом, который востребован на рынке;</li> <li>Стабильная зарплата с потенциальными бонусами и регулярной индексацией;</li> <li>Удаленная работа (при необходимости использование офиса в пешей доступности</li> </ul> <p>от ст.м. Василеостровская и ст.м. Спортивная (СПб))</p> <ul> <li>Прокачка скиллов по запросу или по необходимости (курсы, книжки, семинары);</li> <li>Официальное трудоустройство.</li> </ul>',\n",
       " '79006335': '<p><strong>Компания «Napa Labs» - </strong>один из ведущих российских разработчиков систем мониторинга и управления трафиком операторского класса. На российском телекоммуникационном рынке в 2009 году.</p> <p>Компания имеет солидный опыт работы с российскими операторами связи, в том числе Интернет-провайдерами из списка 5 крупнейших в России. На текущий момент портфель проектов компании насчитывает более 70 успешно установленных аппаратно-программных комплексов в более чем 40 городах РФ</p> <p><strong>Обязанности:</strong></p> <ul> <li>Написание\\\\оптимизация batch, streaming pipelined;</li> <li>разработка\\\\подготовка источников и хранилищ данных;</li> <li>автоматизация , оркестрация, мониторинг дата процессинга.</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Опыт продуктовой разработки Python/Scala/Java от года</li> <li>Опыт написания data pipelines, batch, streaming jobs</li> <li>Знание SQL на уровне DML JOINS/WIndowsFunctions</li> <li>Опыт работы с Hadoop(HDFS,YARN,MR)/Kafka/Spark/Airflow</li> <li>Желательно опыт работы с Сlickhouse</li> <li>Приветствуется знание и построение data quality/data vault/data mesh</li> </ul> <p>Бонусы:</p> <p>- Работа в офисе или Удаленная работа на территории РФ;</p> <p>- Заработная плата по итогам собеседования;</p> <p>- Оформление официальное ;</p> <p>- Компания внесена в реестр аккредитованных организаций, осуществляющих деятельность в области информационных технологий.</p> <p> </p>',\n",
       " '67854831': '<p>Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность на основании их строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, часть - разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform.</p> <p><strong>Чем предстоит заниматься</strong></p> <ul> <li>Участие в разработке ядра продуктов Data Platform</li> <li>Разработка и оптимизация процессов выгрузки данных</li> </ul> <p><strong>Что вам необходимо</strong></p> <ul> <li>Знание одного из языков программирования: Python, Java, Go</li> <li>Знание основ ООП, теории алгоритмов, структур данных</li> <li>Знание SQL</li> <li>Опыт написания процессов загрузки данных (ETL)</li> <li>Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL</li> <li>Будет плюсом знакомство с: Pandas, Numpy, Groovy</li> <li>Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '79120973': '<p><strong>LATOKEN is the supermarket of assets where it is easy to discover, exchange, earn and spend crypto.</strong></p> <p><strong>Mission:</strong> Well-structured and secure data and monitoring.</p> <p><br /><strong>Story</strong>: Product, Finance, OKRs, and Dash Teams need well-structured and secure data and monitoring to analyze and track the improvement of the services and product performance.</p> <p><strong>Key PROBLEM&#39;s</strong>:</p> <ul> <li>The monitoring system alerts for any downtime, latency, or success rate issues of the services.</li> <li>All company data and events are merged and consolidated into the Data Base (DWH, PostgreSQL, Segment, Indicative, Google Analytics) to support the Monitoring system, Token Discovery, OKRs and Analytical Dashes.</li> <li>Acquisition channels, conversions, and users are correctly mapped and easy to access.</li> <li>Token DB is representative of the market and token data is correct and complete with on-chain and smm data. (30k tokens by end of April)</li> </ul> <p><strong>How</strong>:</p> <ul> <li>Create new data pipelines for business need.</li> <li>Maintain scripts and pipelines in case of problems or new requirements.</li> <li>Define necessary events, set up recording and make available for reports and dashboards.</li> <li>Maintain and update token DB.</li> </ul> <p><strong>Constraints</strong>:</p> <ul> <li>All OKRs of the company are consolidated into Platform Ops.</li> <li>All scripts are consolidated into Gitlab.</li> </ul> <p><strong>Main performance number:</strong> Ops OKR Health<br /><strong>Second performance number:</strong> Free Space DWH<br /><strong>Third performance number:</strong> Query response time, sec avg<br /><br /><strong>Functions:</strong><br />• OKRs Health : Calculate OKRs correctly, daily and in time<br />• DWH objects audit<br />• Queries : Advice on optimization and writing queries to databases.<br />• Data quality : Keep data in order and provide reliable reporting<br />• Automation : Automation of OKRs and slack bots for automated ops processes<br />• Audit : Audit of DWH state and automated scripts<br /><br /><strong>Requirement skills and experience:</strong></p> <p>2+ years of PostgresSQL DB administration experience;</p> <p>2+ years of experience writing Python scripts and applications for loading data;</p> <p>Experience in configuring a database in a high-availability architecture;</p> <p>Experience in database optimization for different load profiles;</p> <p>Experience in automating data loading from different sources (CRM, ERP, Web resorses)</p> <p>Eager to work with people with high-performance standards</p> <p><strong>Will be a plus:</strong></p> <p>Experience with financial data</p> <p>Experience as a data/product analyst</p>',\n",
       " '77720409': '<p>В команду разработки ищем инженера данных.</p> <p>Команда выполняет одну из ключевых функций в компании, развивая следующие решения:</p> <p>- Единый «узел» для интеграционных решений инфраструктуры;</p> <p>- Хранилище данных для аналитических задач компании;</p> <p>- Кубы данных (MDX) как self-service инструмент для сотрудников.</p> <p>Каждое решение встраиваем в систему контроля качества данных.</p> <p>Обмениваемся опытом, вместе осваиваем на практике новые технологии и инструменты.</p> <p>Возможность удаленки или работы из офиса А-класса.</p> <p><strong>Ваши задачи:</strong></p> <ul> <li>Разработка SQL скриптов. Анализ и доработка чужого кода;</li> <li>Подготовка ETL/ELT пакетов для загрузки данных;</li> <li>Развитие и сопровождение контроля качества данных;</li> <li>Документация разрабатываемых продуктов;</li> <li>Развитие и сопровождение MDX кубов.</li> </ul> <p><strong>Мы ожидаем:</strong></p> <ul> <li>Опыт работы от 1 года на позициях SQL разработчика, аналитика или инженера данных;</li> <li>Знание TSQL, DML;</li> <li>Умение анализировать и доработать чужой код;</li> <li>Знание основ построения хранилищ данных;</li> <li>Знание любого языка программирования (желательно Python).</li> </ul> <p><strong>Мы предлагаем все, что есть в стабильной компании:</strong></p> <ul> <li>Трудоустройство в нашу дочернюю аккредитованную IT-компанию со всеми полагающимися льготами;</li> <li>Стабильный и прозрачный доход: размер заработной платы обсуждается по итогам собеседования;</li> <li>Премия по результатам работы за квартал;</li> <li>Сокращенный рабочий день в пятницу;</li> <li>ДМС после успешного прохождения испытательного срока;</li> <li>Возможность удаленной работы.</li> </ul> <p><strong>... и даже немного больше:</strong></p> <ul> <li>Развиваем наших сотрудников с помощью профильных курсов, тренингов, мастер-классов, лекций и курсов для прокачки soft-skills;</li> <li>Предлагаем инструменты для самообучения: онлайн библиотека, онлайн платформа K-AMPUS;</li> <li>Поможем легче адаптироваться и успешно пройти испытательный срок с закрепленным наставником;</li> <li>Опытные и вовлеченные коллеги, готовые оказать поддержку в любой ситуации;</li> <li>Возможность проявлять инициативу, воплощать свои идеи и привносить свой вклад в развитие транспортно-логистической отрасли.</li> </ul>',\n",
       " '79255243': '<p>A/B эксперименты – ключевой инструмент для принятия решений в компании Okko. Все гипотезы мы проверяем через эксперименты. Сейчас наш бизнес довольно быстро растёт, растёт и количество запускаемых экспериментов, которые нужно качественно и быстро анализировать. Во многом от того, насколько точно будут проанализированы эксперименты, зависят принимаемые решения и, как следствие, будущее нашего бизнеса. Если ты понимаешь, что соответствуешь требованиям и веришь в то, что вместе мы сможем построить лучшую платформу экспериментов на рынке СНГ, откликайся на вакансию!</p> <p>Ищем коллегу, который энергично погрузится в тонкости работы экосистемы данных в Okko и поможет в решении следующих задач:</p> <ul> <li>внедрение ETL-процессов на Airflow;</li> <li>участие в проектировании горячего и холодного хранилища для платформы эксперименты;</li> <li>участие в построении DWH;</li> <li>участие в проектировании модели данных;</li> <li>администрирование существующих BI-инструментов;</li> </ul> <p><strong>Что мы используем:</strong></p> <ul> <li> <p>Clickhouse, PostgreSQL, Trino, AWS, SberCloud;</p> </li> <li> <p>Python и основные ML библиотеки: Pandas, NumPy, SciPy, Scikit-Learn.</p> </li> </ul> <p><strong>Требования:</strong></p> <ul> <li> <p>уверенное знание Clickhouse, Postgres, HDFS;</p> </li> <li> <p>знание Python;</p> </li> <li> <p>понимание, чем отличается data vault, якорная модель, снежинка, звезда;</p> </li> <li> <p>опыт работы с kafka;</p> </li> <li> <p>умение организовывать ETL-процессы;</p> </li> <li> <p>опыт построения DWH;</p> </li> <li> <p>опыт проектирования модели данных;</p> </li> <li> <p>опыт работы с большими объемами данных;</p> </li> <li> <p>умение работать в командной строке Unix-систем.</p> </li> </ul> <p><strong>Условия:</strong></p> <ul> <li>работа в сильной команде, состоящей из топовых аналитиков, аналитиков-разработчиков и инженеров;</li> <li>топовое оборудование и весь необходимый софт;</li> <li>официальное трудоустройство;</li> <li>ДМС со стоматологией, офисный врач, доплата больничного листа, корпоративные скидки;</li> <li>льготные условия ипотеки в рамках зарплатного проекта;</li> <li>бесплатная подписка на сервисы партнеров.</li> <li>насыщенная корпоративная жизнь.</li> </ul>',\n",
       " '79177815': '<p>Мы — Сбер ID. Будущий личный кабинет всей экосистемы Сбера, а также удобный способ авторизации в различные сервисы.</p> <p><strong>Задачи:</strong></p> <ul> <li>Участвовать в построении хранилищ данных: проектирование, определение сущностей, формирование витрин данных;</li> <li>Разрабатывать и оптимизировать процессы выгрузки данных из различных источников;</li> <li>Разрабатывать процессы обработки данных;</li> <li>Проводить оркестрацию ETL-процессов.</li> </ul> <p><strong><strong>Наши ожидания:</strong></strong></p> <ul> <li>Опыт в направлении от трёх лет;</li> <li>Сильные компетенции в области реляционных СУБД и хранилищ данных;</li> <li>Уверенное знание SQL: сложные запросы, аналитически функции, понимание физической реализации join’ов, оптимизация производительности запросов;</li> <li>Знание одного или нескольких ETL-инструментов: Informatica, MS SSIS, SAS, ODI;</li> <li>Понимание принципов организации хранилищ данных, подходов к проектированию логической и физической моделей, понимание основной проблематики хранилищ и подходов к решению;</li> </ul> <p><em>Наш стек:</em></p> <ul> <li>Bigdata: Hadoop, Hive, Impala, Spark, Scala/Java;</li> <li>СУБД: Teradata, Greenplum;</li> <li>ETL: Informatica, Golden Gate;</li> <li>BI: Qlik.</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li>Много интересных задач, которые влияют на сотни тысяч людей и позволяют вырасти профессионально;</li> <li>Можно работать в офисе или гибридном графике;</li> <li>Конференции и обучение на корпоративных курсах за наш счёт;</li> <li>Отличная ДМС, включая несчастные случаи и тяжелые заболевания;</li> <li>Возможность сменить команду или проект при желании;</li> <li>Льготные условия по ипотеке и кредитам;</li> <li>Материальная помощь и социальная поддержка;</li> <li>Корпоративная пенсионная программа;</li> <li>Шикарный офис в бизнес-центре &quot;Президент Плаза&quot; рядом с метро Кутузовская.</li> </ul>',\n",
       " '79240771': '<p><strong>СберАвто — уникальный для российского рынка сервис для выбора, покупки и доставки автомобилей онлайн. В СберАвто можно подобрать автомобиль, заказать независимый осмотр автомобиля и доставку авто прямо к дому, а также онлайн оформить кредит и страховку. И все это – не выходя из дома, с помощью нескольких сообщений в чате с личным помощником.</strong></p> <p><strong>Наша миссия – превратить покупку автомобиля в простой, быстрый и безопасный процесс; стать единой точкой входа по всем вопросам, связанным с покупкой авто – от его выбора, проверки и покупки до страхования, оформления кредита и доставки в любой регион России.</strong></p> <p> </p> <p><strong>Вам предстоит:</strong></p> <ul> <li> <p>Разработка, тестирование витрин данных с использованием инструментов Hadoop и Greenplum, вывод их в промышленное использование и дальнейшая поддержка в соответствии с требованиями группы</p> </li> <li> <p>Тесное взаимодействие с командой BI-аналитики, подготовка витрин данных для построения дэшбордов</p> </li> <li> <p>Разработка интеграционных решений между хранилищем данных и ДЗО группы «Сбер». Консолидация данных по кластеру «Автоидустрия» в аналитическом слое хранилища данных</p> </li> <li> <p>Оптимизация существующих разработок: оптимизация вычислений, Refactoring, code review</p> </li> <li> <p>Ad-hock задачи</p> </li> </ul> <p> </p> <p><strong>Наши ожидания:</strong></p> <ul> <li>Опыт программирования на Scala /Java / Python</li> <li>Опыт работы с инструментами Hadoop (Hive, Spark, Oozie, HDFS, YARN), понимание внутренней архитектуры</li> <li>Опыт работы с ведущими СУБД (PostgreSQL, Teradata, Oracle)</li> <li>Знание SQL на уровне оптимизации сложных запросов</li> <li>Знания принципов построения распределенных систем хранения и обработки данных</li> <li>Знание классических алгоритмов и структур данных</li> <li>Понимание архитектуры хранилищ данных</li> <li>Уверенная работа с git, bitbucket, jenkins</li> <li>Умение тестировать и документировать собственный код, а также работать с существующим кодом</li> <li>Опыт работы в Greenplum будет преимуществом</li> </ul> <p> </p> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Участие в амбициозном проекте экосистемы Сбер</li> <li>Работу в команде профессионалов, реализацию идей по улучшению сервиса</li> <li> <p>Интересные и амбициозные задачи</p> </li> <li> <p>Достойный уровень ежемесячного дохода</p> </li> <li> <p>ДМС + Стоматология</p> </li> <li> <p>Полное соблюдение ТК РФ</p> </li> <li> <p>Материальную поддержку: у нас принято финансово помогать при рождении ребенка или в сложной жизненной ситуации</p> </li> <li> <p>Скидки на фитнес, английский язык</p> </li> <li> <p>Оборудование и другие ресурсы</p> </li> </ul>',\n",
       " '79314029': '<p>Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, другую часть разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform.</p> <p><strong>Обязанности</strong></p> <ul> <li>Участие в разработке ядра продуктов Data Platform</li> <li>Разработка и оптимизация процессов выгрузки данных</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Знание одного из языков программирования: Python, Java, Go</li> <li>Знание основ ООП, теории алгоритмов, структур данных</li> <li>Знание SQL</li> <li>Опыт написания процессов загрузки данных (ETL)</li> <li>Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL</li> <li>Будет плюсом знакомство с: Pandas, Numpy, Groovy</li> <li>Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ компании</li> <li>Работу в офисе или удаленно — по договоренности</li> <li>Профессиональное развитие. Вы получите доступ к библиотеке с технической литературой, тренингам и мастер-классам для сотрудников</li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Бесплатный фитнес-зал или компенсацию затрат на спортивные занятия</li> </ul>',\n",
       " '79218689': '<p><strong>Компания Axenix (ранее Accenture)</strong> продолжает работу на российском рынке и аккумулирует 30-ти летний консалтинговый опыт внедрения инновационных решений.<br />Наша экспертиза - стратегия и консалтинг, технологии и операции, направленные на цифровизацию бизнеса.</p> <p>В своей деятельности мы сочетаем обширные знания, опыт в различных отраслях и глубокое понимание специфики и возможностей российского бизнеса.</p> <p>Офисы компании находятся в Москве, Твери и Ростове-на-Дону, Алматы и Ереване.<br />Помимо этого, у нас есть команды в Санкт-Петербурге, Краснодаре и Воронеже, а также сотрудники, работающие удаленно из других городов.</p> <p>И сейчас мы в поиске талантливых <strong>Data Engineer</strong> в нашу команду.</p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Оптимизация времени выполнения джобов на pySpark, унификация кода между Trino и Spark версиями;</li> <li>Технические ДАГи для сбора стастики и сжатию файлов;</li> <li>Динамическая генерация ДАГа проверок (итератор и раннер на Python);</li> <li>Реализация проверок с помощью фреймворка Amazon Deequ на Spark;</li> <li>Настройка CDC репликации с использованием Debezium и Spark Streaming джобы для применения изменений в сырой слой.</li> </ul> <p><strong>Мы ждем что у тебя есть:</strong></p> <ul> <li><strong>В идеале </strong>- pySpark, Python, Trino/Presto/Athena, Microsoft Azure Databricks или AWS EMR с поддержкой DeltaLake / Apache Iceberg, Yandex.Cloud, Airflow, git, JIRA/Confluence</li> <li><strong>Желательный минимум</strong> - PySpark (или любой Spark) + Hadoop</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li> <p>Динамичную работу без рутины в ведущей IT компании;</p> </li> <li> <p>Возможность использовать передовые технологии и стратегии и менять бизнес наших клиентов к лучшему;</p> </li> <li> <p>Проекты в разных индустриальных направлениях с передовыми технологическими решениями и современной архитектурой приложений;</p> </li> <li> <p>Конкурентоспособный уровень дохода, годовые бонусы и регулярное повышение по результатам Performance Review;</p> </li> <li> <p>Культуру непрерывного обучения: сертификация, online и offline обучение в России, менторство в профессиональном развитии;</p> </li> <li> <p>ДМС с первого дня работы, включая стоматологию, в лучших клиниках Москвы и МО для cотрудника и его семьи (жена/муж, дети до 18 лет);</p> </li> <li> <p>Страхование жизни в размере годового оклада сотрудника;</p> </li> <li> <p>Программу поддержки сотрудника и его родственников по психологическим, юридическим, финансовым вопросам;</p> </li> <li> <p>Дополнительные дни оплачиваемого отпуска в год;</p> </li> <li> <p>Ежемесячную денежную компенсацию на питание.</p> </li> </ul>',\n",
       " '79300938': '<p>Наш клиент - международная компания, которая разрабатывает надежные и актуальные IT-решения для b2c и b2b сегмента находится в поисках <strong>Data-scientist.</strong></p> <p><em><strong>Вакансия с релокацией на Кипр!</strong></em></p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li> <p>Оценка целей бизнес задач, бизнес метрик, алгоритмов и методов машинного обучения, имеющихся и необходимых данных.</p> </li> <li> <p>Формирование и проверка гипотез.</p> </li> <li> <p>Извлечение признаков из данных для применения алгоритмов машинного обучения (Feature Engineering).</p> </li> <li> <p>Разработка предиктивных моделей.</p> </li> <li> <p>Планирование проведения и анализ A/B тестирования моделей в продакшн.</p> </li> <li> <p>Разработка необходимых компонентов функционала ML pipelines.</p> </li> <li> <p>Ведение документации с описанием алгоритмов и моделей, с соответствующей аналитикой.</p> </li> </ul> <p><strong>От тебя:</strong></p> <ul> <li> <p>Уверенное владение Python.</p> </li> <li> <p>Опыт работы с Data Science библиотеками и фреймворками такими как pandas, sklearn, scipy, matplotlib, catboost и др.</p> </li> <li> <p>Знания основ статистического анализа и его применение.</p> </li> <li> <p>Понимание проведения A/B тестирования.</p> </li> <li> <p>Понимание классических методов и алгоритмов машинного обучения, опыт их применения на практике.</p> </li> <li> <p>Базовые знания SQL запросов.</p> </li> <li> <p>Опыт работы с Git.</p> </li> <li> <p>Опыт работы с Docker.</p> </li> </ul> <p><strong>Плюсом будет:</strong></p> <ul> <li> <p>Опыт работы с MLflow / KubeFlow / ClearML.</p> </li> <li> <p>Опыт работы с GitLab CI/CD.</p> </li> <li> <p>Опыт работы с Airflow / Argo.</p> </li> <li> <p>Опыт работы с разными базами данных, создание витрин.</p> </li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li> <p>Белую заработную плату на уровне Ваших профессиональных навыков и пожеланий;</p> </li> <li> <p>Помощь с релокацией на Кипр г. Лимассол, (визовое сопровождение, медицинское страхование для сотрудника и членов его семьи)</p> </li> <li> <p>Интересные задачи, перспективные проекты, продвинутые технологии;</p> </li> <li> <p>Работу в русскоязычной команде.</p> </li> </ul> <p><strong>Современный подход к управлению процессами, временем, задачами. Отсутствие ненужной бюрократии, возможность реализовывать свои идеи. Для нас сотрудник – это не просто исполнитель, а думающий и амбициозный человек, которому важен результат.</strong></p> <p>\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b</p>',\n",
       " '78672698': '<strong>Обязанности:</strong> <ul> <li> <p>Проектирование, разработка и поддержка пайплайнов для загрузки источников данных (пакетная и потоковая загрузка данных)</p> </li> <li> <p>Проектирование, разработка и поддержка пайплайнов для расчета витрин данных (пакетный расчет, подготовка near real time витрин данных)</p> </li> <li> <p>Разработка фреймворков, инструментов, шаблонных пайплайнов для более эффективной загрузки и обработки данных в корпоративный dwh</p> </li> <li> <p>Обеспечение SLA и качества данных</p> </li> </ul> <strong>Требования:</strong> <ul> <li> <p>Отличные знания в SQL (в том числе оконные функции), Python.</p> </li> <li> <p>Хорошие знания ClickHouse, DBT, Ni-Fi, Kafka, инструментов экосистемы Hadoop (Hdfs, Hive, Spark, Spark Streaming, Yarn), инструментов оркестрации потоков данных (Prefect, AirFlow, oozie).</p> </li> <li> <p>Опыт работы с потоковыми данными</p> </li> <li> <p>Опыт работы на проектах построения хранилищ данных/озер данных 3+ лет</p> </li> <li> <p>Будет плюсом знания одного или нескольких языков программирования для решения задач обработки данных: Python, Java, Scala</p> </li> <li> <p>Будет плюсом знания GreenPlum</p> </li> </ul> <strong>Условия:</strong> <ul> <li>Интернациональный коллектив;</li> <li>Официальное оформление по ТК РФ, либо неофициальное (на Ваш выбор);</li> <li>Полностью удаленная работа, либо гибридный график;</li> <li>Предоставляем рабочую технику;</li> <li>Уровень заработной платы обсуждается по итогам интервью.</li> </ul>',\n",
       " '72234398': '<p><strong>Мы открыли офис в Омске!</strong></p> <p>Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, другую часть разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform.</p> <p><strong>Обязанности:</strong></p> <ul> <li>Участие в разработке ядра продуктов Data Platform</li> <li>Разработка и оптимизация процессов выгрузки данных</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Знание одного из языков программирования: Python, Java, Go</li> <li>Знание основ ООП, теории алгоритмов, структур данных</li> <li>Знание SQL</li> <li>Опыт написания процессов загрузки данных (ETL)</li> <li>Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL</li> <li>Будет плюсом знакомство с: Pandas, Numpy, Groovy</li> <li>Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, на каждом этаже есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul> <p> </p>',\n",
       " '79155951': '<p><strong>Кто мы:</strong></p> <p>BestDoctor — экосистема медицинских и страховых сервисов, созданная экспертами для удобного управления здоровьем. Мы меняем рынок медицинского страхования и отношение людей к своему здоровью с помощью качественного сервиса и принципиально нового клиентского опыта.</p> <p>Благодаря глубокой экспертизе в создании медицинских решений и любви к своему делу, нам удалось создать ту самую экосистему полезных, эффективных и, главное, простых медицинских сервисов для управления здоровьем компаний и людей.</p> <p><strong>Чего мы достигли:</strong></p> <p>⚡️ За семь лет мы вышли на второе место по числу клиентов среди insurtech компаний в Европе. Сегодня с нами уже 100 000+ застрахованных пользователей (40 000 из них мы подключили в 2021 году), а в списке клиентов — Мегафон, Aviasales, МТС Банк, Нетология, InDriver, Эвотор, Ivi.ru, Ostrovok.ru, VC.ru и еще 140 компаний.</p> <p>⚡️ Создали экосистему медицинских сервисов, которая включает: сервис онлайн-поддержки 24/7, сервис онлайн записи в оффлайн клинику, своя виртуальная клиника, сервис второго мнения специалиста, чекапы, сервис поддержки психологов и др.</p> <p><strong>Наша следующая цель</strong> — сделать все продукты, входящие в экосистему BestDoctor, мультисервисными, разработать подбор индивидуальных программ, создать возможность управлять бюджетом, улучшить HR-кабинет и умную маршрутизацию каждого сотрудника для B2B сегмента. И для этого ищем лучших экспертов, чтобы вместе захватить рынок.</p> <p><strong>Подробнее о сервисе BestDoctor:</strong> https://bestdoctor.ru/ и https://hh.ru/article/29681</p> <p><strong>О проекте:</strong></p> <p>За время существования компании мы накопили много данных и разных инструментов аналитики. На этих данных мы строим предложения для новых клиентов и продлеваем старых, проводим переговоры с клиниками и непосредственно помогаем нашим пациентам. Главная задача - весь этот информационный поток перенести в чётко организованную систему сбора, обработки и анализа данных любого объёма.</p> <p>Сейчас мы планируем вести сборку data lake house на базе GreenPlum, куда будут сливаться данные всех источников, таких как, PostgreSQL, Yandex,Google drive, сторонние API и др.). Мы мигрируем туда с Postgres+Astroniomer. Также у нас будет большой проект с фичастором и MLFLOW. В твоих задачах будет много архитектуры и хорошего продакшн кода, перенос, рефакторинг старого и написание очень динамического и автоматизированого нового, а также опыт с очень крутым датасайнсом, аналитикой и продуктом.</p> <p><strong>В целом, тебе предстоит:</strong></p> <ul> <li>Мигрировать даги с Airflow c Астрономера на Кубер;</li> <li>Развивать и оптимизировать GreenPlum(PXF) даги;</li> <li>Пилить Data Managment нового поколения;</li> <li>Интегрировать сторонние API.</li> </ul> <p><strong>Что для нас важно:</strong></p> <ul> <li>Опыт программирования на Python 3;</li> <li>Опыт работы с Airflow;</li> <li>Отличные знания и опыт работы с SQL.</li> <li>Опыт работы с GreenPlum и PostgreSQL;</li> </ul> <p><strong>Дополнительным плюсом будет:</strong></p> <ul> <li>Опыт работы с Apache Kafka;</li> <li>Навыки в DevOps / опыт работы с Docker;</li> <li>Опыт работы с Apache Spark;</li> <li>Опыт работы с хранилищами (DataVault Anchor LakeHouse FeatureStore)</li> </ul> <p><strong>Как мы нанимаем:</strong></p> <p>Мы готовы оперативно выходить с оффером, если понимаем, что подходим друг другу.</p> <ul> <li>1 этап - телефонное интервью с HR (15-20 минут);</li> <li>2 этап - техническое интервью с лидом аналитики;</li> <li>3 этап - финальное интервью с CDO и HR.</li> </ul> <p><strong>Почему с нами круто:</strong></p> <ul> <li>Мы меняем рынок медицинского страхования, и у нас это отлично получается;</li> <li>В нас поверили и проинвестировали топовые венчурные фонды в России: российский Winter Capital, шведский VNV Global и австрийская страховая компания Uniqa;</li> <li>Удаленный формат работы (будем рады тебя видеть у нас в офисе (м.Савёловская);</li> <li>У нас гибкий график работы, который подойдет как жаворонку, так и сове;</li> <li>Тебя будет окружать команда талантливых и мотивированных людей;</li> <li>Мы предоставим тебе ноутбук и всю необходимую технику, которая позволит эффективно и комфортно работать;</li> <li>Медицинское обслуживание через систему BestDoctor.</li> </ul> <p>❤️ Ты будешь частью <strong>большого социально значимого дела.</strong> Мы реализуем амбициозную задачу — меняем рынок здравоохранения, действительно помогаем людям, и у нас это отлично получается.</p> <p>И у тебя получится!</p>',\n",
       " '79178458': '<strong>Задачи:</strong> <ul> <li>проектирование, разработка и поддержка пайплайнов для сбора и обработки данных из различных источников (пакетная и потоковая загрузка данных);</li> <li>автоматизация регулярных процессов и отчетности;</li> <li>проектирование и модернизация схем хранения данных;</li> <li>обеспечение непрерывной работы, диагностика и устранение неполадок ClickHouse;</li> <li>восстановление кластера в случае аварии;</li> <li>внедрение и развитие мониторинга различных сервисов;</li> <li>проектирование и организация сбора логов от различных сервисов;</li> <li>участие в проработке и реализации интеграций с другими сервисами и командами;</li> <li>взаимодействие с подрядчиками и клиентами;</li> <li>участие в развитии инфраструктурных решений подразделения.</li> </ul> <strong>Требования:</strong> <ul> <li>опыт работы с большими данными (у нас это &gt; 100Тб);</li> <li>обязательно твердые знания ClickHouse (понимание архитектуры Clickhouse, администрирование);</li> <li>опыт с масштабированием данных (шардирование/репликация) в Clickhouse;</li> <li>знание SQL, Python;</li> <li>опыт работы с Airflow\\\\dagster, dbt;</li> <li>знание Docker;</li> <li>опыт работы в BI-системах;</li> <li>опыт работы в unix системах;</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>опыт работы с PostgreSQL, MSSQL, ansible</li> </ul> <strong>Условия:</strong> <ul> <li>гибридный или удаленный формат работы;</li> <li>прозрачная система мотивации: оклад + квартальные премии + годовой бонус.</li> </ul>',\n",
       " '79259932': '<p><strong>Интер РАО-Онлайн</strong> – российская IT-компания, которая разрабатывает цифровые решения в сфере энергетики и ЖКХ. Мы работаем как над B2C, так и B2B продуктами: внедряем единую платформу для 50 миллионов клиентов энергосбытовых компаний и ЕИРЦ и запускаем экосистему бытовых сервисов</p> <p>В связи с расширением IT команды мы ищем к себе<strong> Data Engineer.</strong></p> <p><strong>Обязанности:</strong></p> <ul> <li> <p>Проектирование и разработка конвейеров обработки данных (data processing pipelines), включая разработку и оптимизацию архитектуры данных (data architecture);</p> </li> <li> <p>Построение и поддержание инфраструктуры, необходимой для оптимального извлечения, преобразования и загрузки данных из различных источников (ETL/ELT);</p> </li> <li> <p>Создание и обновление документации по архитектуре и схемам обработки данных;</p> </li> <li> <p>Дебагинг, тестирование и исправление ошибок в своём и чужом коде;</p> </li> <li> <p>Взаимодействие с членами команды, включая ревью технических спецификаций и кода;</p> </li> <li> <p>Участие в обсуждениях решений для обработки данных, процессов ETL/ELT и автоматизации рабочих процессов по обработке данных, включая координацию с бэкенд разработчиками, специалистами по аналитике данных и QA инженерами;</p> </li> <li> <p>Взаимодействие с заказчиком (внутренним и внешним), поиск оптимальных решений для получения качественных данных.</p> </li> </ul> <strong>Требования:</strong> <ul> <li> <p>Понимание принципов работы с данными;</p> </li> <li> <p>Опыт работы с реляционными и колоночными СУБД, уверенное владение SQL</p> </li> <li> <p>Владение Python (requests, regex, scrapy, sqlalchemy, иные библиотеки для работы с данными)</p> </li> <li> <p>Опыт разработки ETL/ELT процессов (Airflow, NiFi, иные).</p> </li> <li> <p>Будет плюсом наличие опыта работы с Loginom</p> </li> </ul> <strong>Условия:</strong> <ul> <li>Комфортный офис в пешей доступности от метро Фрунзенская и Спортивная</li> <li>Гибкий график</li> <li>Официальное трудоустройство по ТК РФ и стабильная заработная плата</li> <li>Годовой бонус, который будет зависеть от личных KPI и результатов деятельности компании</li> <li>Расширенный пакет ДМС со стоматологией после испытательного срока</li> <li>Ежемесячные посиделки в офисе, на которых мы едим пиццу и знакомимся с новыми коллегами</li> <li>Сильная команда специалистов, у которых можно многому научиться и которые всегда готовы прийти на помощь</li> <li>Интересные задачи и возможность создавать продукты для миллионов пользователей</li> </ul>',\n",
       " '79314008': '<p>У нас в DWH очень много данных: 6000 объектов, 200 тб в Greenplum и 2 пт в Hadoop. С данными активно работают более 3000+ аналитиков в бизнес командах. На стороне команды DWH созданы инструменты, которые позволяют аналитику не зависеть от etl разработчика и самостоятельно собирать данные для своих задач. Сейчас мы ищем экспертов sql, готовых помогать аналитикам оптимально работать с данными.</p> <p>Наша инфраструктура</p> <p>• Greenplum / Hadoop / Clickhouse в качестве ядра обработки данных</p> <p>• Собственные решения на базе Apache Zeppelin, Apache Airflow и Apache Flink в качестве инструментов трансформации данных для аналитиков (selfservice etl)</p> <p>Мы не требуем знания этих инструментов от кандидатов. Наша задача сделать инструменты работы с данными максимально удобными и доступными. Но нам важно, чтобы кандидат на эту позицию имел большой опыт работы с данными.</p> <p><strong>Обязанности</strong></p> <ul> <li>Быть экспертом по данным: помогать аналитикам в решении вопросов с источниками данных, моделью данных DWH</li> <li>Разрабатывать витрины в помощь аналитикам</li> <li>Выступать заказчиком для разработки витрин в смежных командах</li> <li>Оптимизировать существующие запросы</li> <li>Внедрять и развивать культуру написания оптимальных запросов</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Высшее техническое образование</li> <li>Опыт работы с базами данных в качестве разработчика от 1 года</li> <li>Свободное владение SQL</li> <li>Опыт проектирования объектов БД на основании бизнес требований</li> <li>Понимание теории СУБД и ETL-процессов</li> <li>Знакомство с ETL-инструментами будет плюсом</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '78371688': '<p>НПКЦ диагностики и телемедицины («Радиология Москвы»), – государственная компания с более чем 20-летним опытом работы в здравоохранении.</p> <p>Наш Центр - ведущая экспертная организация по развитию и повышению эффективности службы лучевой и инструментальной диагностики в России.</p> <p><strong>Обязанности:</strong></p> <ul> <li> <p><strong>Обязанности:</strong></p> </li> <li>Построение и оптимизация ETL скриптов, получающих данные от источников (базы данных, файлы, облако). Получение выгрузок при помощи запросов при помощи SQL</li> <li>Формирование выгрузок в Excel из ЕРИС, ЕМИАС и других информационных систем</li> <li>Обработка выгрузок на Python (или с использованием других программных приложений)</li> <li>Формирование и ведение баз данных</li> <li>Статистическая обработка и анализ данных</li> <li>Подготовка аналитических отчетов и справок</li> <li>Подготовка презентаций</li> <li>Оптимизация отчетных форм</li> <li>Анализ показателей работы оборудования для лучевой и инструментальной диагностики</li> <li>Выявление требования и подготовка Dashboard</li> <li>Описание и анализ бизнес-процессов</li> <li>Визуализация результатов статистической обработки данных в Yandex DataLens (построение графиков, диаграмм)</li> <li>Разработка методик прогнозирования</li> <li>Поддержка НИР в части сбора, статистической обработки и анализа данных</li> <li>Постановка вычислительных экспериментов</li> <li> <p> </p> <p><strong>Требования:</strong></p> </li> <li>Высшее образование: техническое, физико-математическое</li> <li>Опыт работы с современным программным обеспечением для статистического анализа данных</li> <li>Знание современных методов статистического анализа</li> <li>Работа с данными: знание SQL</li> <li>Опыт разработки на Python, умение читать и анализировать чужой код (code review)</li> <li>Знание MS Excel - продвинутый пользователь</li> <li>Опыт анализа и структурирования информации</li> <li>Опыт работы с большим объемом информации (Big data), в условиях многозадачности</li> <li>Аналитическое мышление</li> <li>Английский язык не ниже В1.</li> </ul> <p><strong>Мы ценим:</strong></p> <ul> <li>Стремление достигать результата;</li> <li>Желание постоянно развиваться;</li> <li>Готовность к динамичным изменениям;</li> <li>Искренность и открытость;</li> <li>Готовность к высокой нагрузке стартапов;</li> <li>Умение работать в команде;</li> <li>Навыки самоорганизации.</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Оформление по ТК РФ</li> <li>График 5/2 с 9.00 до 17.30, выходные и праздничные соответственно государственным</li> <li>Выплаты 2 раза в месяц</li> <li>Участие в амбициозных проектах в масштабах отрасли и страны;</li> <li>Сопричастность к решению «сверхзадач» в системе здравоохранения;</li> <li>Наставничество и поддержка в развитии на международном уровне;</li> <li>Возможность развиваться опережающими темпами, участвуя в программах обучения;</li> <li>Высокий «белый» доход;</li> <li>Стать частью яркой профессиональной команды.</li> </ul>',\n",
       " '78617195': '<p><strong>Rubius</strong> – IT-компания со смелым характером. Мы разрабатываем софт для клиентов из различных отраслей – от промышленности и нефтегаза до ритейла и медицины. Обосновались в Томске, работаем по всему миру: наше программное обеспечение используют в США, Европе и Азии. Наши решения используют Apple, Tesla, Kaspersky, Amazon, IBM, Uber, Netflix, Газпром, РЖД и другие. В группу компаний входят представительства в США (Нью-Йорк), Казахстане (Алматы, резиденты Astana hub), ОАЭ (Дубай). В нашем профиле на hh.ru мы постарались подробно рассказать о нас, обязательно загляните:)</p> <p>Одна из наших команд, которая занимается искусственным интеллектом, приглашает <strong>Data Engineer, так как команда расширяется</strong>. Вам предстоит работать с большими данными совместно с аналитиками и командой ML. Мы ищем человека, который умеет собирать витрины, джойнить данные и проверять полученный результат. У нас есть экспертиза по синтезу аудио и видео, анализу изображений и видео, компьютерному зрению, предиктивной аналитике, обработке текстовых данных и тд. Мы исследуем разные сферы от логистики до медицины, погружаемся в сферы наших крупных заказчиков с головой, чтобы помочь оптимизировать процессы. Также команда разработала свой продукт для видео аналитики Visius.</p> <p><strong>Чем предстоит заниматься: </strong></p> <ul> <li> <p>проектировать и собирать витрины данных по разработанному ТЗ</p> </li> <li> <p>проектировать, разрабатывать и поддерживать ETL-процессы для загрузки данных из/в Data Lake</p> </li> <li> <p>тестировать результаты преобразования данных и проверять их целостность</p> </li> <li> <p>писать документацию - комментировать код</p> </li> <li> <p>работать с data-аналитиками для создания новых и оптимизации существующих витрин</p> </li> </ul> <p><strong>Добро пожаловать к нам в команду, если есть:</strong></p> <ul> <li> <p>понимание основных операций ДБ и DWH</p> </li> <li> <p>опыт работы с Hadoop технологиями (Spark, Hive и тд)</p> </li> <li> <p>хорошее знание SQL, Python</p> </li> <li> <p>опыт работы с Azure/Yandex облачными платформами</p> </li> <li> <p>опыт работы с Airflow, Kafka будем плюсом</p> </li> </ul> <p><strong>Что мы предлагаем:</strong></p> <p>Сотрудники компании – главная ценность Rubius. Мы поддерживаем свободу творчества и полёт инженерной мысли. Стремимся, чтобы каждый участник нашей команды раскрыл свой потенциал. Мы стараемся максимально заботиться о наших сотрудниках. Здесь удалённые и офисные команды чувствуют себя максимально комфортно.</p> <p><strong>Про работу и оплату</strong></p> <ul> <li> <p>белая и своевременная заработная плата в зависимости от компетенций и уровня</p> </li> <li> <p>официальное трудоустройство</p> </li> <li> <p>до 10% ежемесячной премии за хорошие результаты</p> </li> <li> <p>помощь с home office</p> </li> <li> <p>возможно трудоустройство в нашей компании в Казахстане (для желающих получить заветную карту Visa)</p> </li> </ul> <p><strong>Про рост и развитие</strong></p> <ul> <li> <p>индивидуальный трек развития по желанию</p> </li> <li> <p>бесплатное обучение английскому языку</p> </li> <li> <p>бонус за профессиональное развитие (курсы, подкасты, литература по хард и софт скиллам)</p> </li> <li> <p>компенсация 50% за профессиональную сертификацию</p> </li> <li> <p>внутренние митапы на разные темы</p> </li> </ul> <p><strong>Про офис, плюшки и атмосферу</strong></p> <ul> <li> <p>оплачиваемые занятия спортом (даже в домашних условиях)</p> </li> <li> <p>программа ДМС после испытательного срока</p> </li> <li> <p>скидка для вас и родственников в Rubius Academy</p> </li> <li> <p>бонусы к рождению детей и свадьбе</p> </li> <li> <p>классные корпоративы и активности</p> </li> <li> <p>развитая и комфортная корпоративная культура, без иерархии и бюрократии</p> </li> </ul> <p>А ещё у нас есть лучший офис в Томске, где тебя всегда ждут, сообщества по интересам (футбол, теннис, своя музыкальная группа, шахматный клуб...) и коллектив, где прислушиваются к мнению каждого. Подробнее о нашей компании можно почитать в нашем профиле на hh.ru. Там же есть ссылочки на наши сайты и соцсети.</p> <p>Откликайтесь!</p> <p> </p>',\n",
       " '76712124': '<p>В связи с активным развитием корпоративной дата-платформы нам требуется <strong>Инженер данных</strong>, который улучшит процессы разработки дата-пайплайнов.</p> <p><strong>Наш стек: </strong>Airflow, nifi, dbt, Object storage, greenplum, dataproc, datalens, Analysis Services.</p> <p><strong>Мы ожидаем:</strong></p> <ul> <li>уверенные знания SQL (PostgreSQL);</li> <li>владение Python;</li> <li>опыт работы с Airflow, DBT либо аналогичными инструментами;</li> <li>будет плюсом понимание принципов работы Apache Kafka, опыт участия в разработке кубов или BI-отчетов, использование DevOPS методик.</li> </ul> <p><strong>Ваши задачи:</strong></p> <ul> <li>разработка и поддержка пайплайнов данных;</li> <li>поддержка ETL-процессов;</li> <li>формирование и доработка витрин данных;</li> <li>сопровождение процессов тестирования качества данных.</li> </ul> <ul> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>официальное трудоустройство с первого дня работы;</li> <li>график работы — 5/2 и гибкое начало дня и гибридный режим;</li> <li>современный офис рядом с метро Звёздная: кухни, вкусный кофе, места для отдыха;</li> <li>ДМС с момента трудоустройства;</li> <li>программа адаптации и welcome-тренинг;</li> <li>бонусы для сотрудников: реферальные программы, корпоративные мероприятия, мерч, подарки для детей;</li> <li>корпоративные скидки: английский, спорт, кафе и рестораны в ТРЦ;</li> <li>разделяем принципы устойчивого развития: раздельный сбор отходов, контейнеры для батареек и своп-вечеринки в офисе.</li> </ul> <p><strong>Компания входит в список аккредитованных ИТ компаний, на сотрудников распространяются все социальные льготы для профильных специалистов, в том числе льготная ипотека и освобождение от призыва на военную службу.</strong></p>',\n",
       " '79255076': '<p>A/B эксперименты – ключевой инструмент для принятия решений в компании Okko. Все гипотезы мы проверяем через эксперименты. Сейчас наш бизнес довольно быстро растёт, растёт и количество запускаемых экспериментов, которые нужно качественно и быстро анализировать. Во многом от того, насколько точно будут проанализированы эксперименты, зависят принимаемые решения и, как следствие, будущее нашего бизнеса. Если ты понимаешь, что соответствуешь требованиям и веришь в то, что вместе мы сможем построить лучшую платформу экспериментов на рынке СНГ, откликайся на вакансию!</p> <p>Ищем коллегу, который энергично погрузится в тонкости работы экосистемы данных в Okko и поможет в решении следующих задач:</p> <ul> <li>внедрение ETL-процессов на Airflow;</li> <li>участие в проектировании горячего и холодного хранилища для платформы эксперименты;</li> <li>участие в построении DWH;</li> <li>участие в проектировании модели данных;</li> <li>администрирование существующих BI-инструментов;</li> </ul> <p><strong>Что мы используем:</strong></p> <ul> <li> <p>Clickhouse, PostgreSQL, Trino, AWS, SberCloud;</p> </li> <li> <p>Python и основные ML библиотеки: Pandas, NumPy, SciPy, Scikit-Learn.</p> </li> </ul> <p><strong>Требования:</strong></p> <ul> <li> <p>уверенное знание Clickhouse, Postgres, HDFS;</p> </li> <li> <p>знание Python;</p> </li> <li> <p>понимание, чем отличается data vault, якорная модель, снежинка, звезда;</p> </li> <li> <p>опыт работы с kafka;</p> </li> <li> <p>умение организовывать ETL-процессы;</p> </li> <li> <p>опыт построения DWH;</p> </li> <li> <p>опыт проектирования модели данных;</p> </li> <li> <p>опыт работы с большими объемами данных;</p> </li> <li> <p>умение работать в командной строке Unix-систем.</p> </li> </ul> <p><strong>Условия:</strong></p> <ul> <li>работа в сильной команде, состоящей из топовых аналитиков, аналитиков-разработчиков и инженеров;</li> <li>топовое оборудование и весь необходимый софт;</li> <li>официальное трудоустройство;</li> <li>ДМС со стоматологией, офисный врач, доплата больничного листа, корпоративные скидки;</li> <li>льготные условия ипотеки в рамках зарплатного проекта;</li> <li>бесплатная подписка на сервисы партнеров.</li> <li>насыщенная корпоративная жизнь.</li> </ul>',\n",
       " '76006484': '<p><strong>Сравни</strong> – финансовый супермаркет, мы создаем удобные сервисы и рекомендации для того чтобы помочь людям принимать правильные решения при выборе банковских и страховых продуктов - ОСАГО, страховок путешественника, страхования недвижимости, Каско, вкладов, кредитов, кредитных карт или подобрать лучший вариант ипотеки. Мы первыми в Рунете придумали и запустили сервисы оформления электронного ОСАГО и подбора кредита.</p> <p><strong>Сейчас у нас:</strong></p> <ul> <li> <p>14 млн уникальных пользователей в месяц;</p> </li> <li> <p>8 000 предложений от банков и страховых компании;</p> </li> <li> <p>140 000 отзывов о банках и страховых компаниях;</p> </li> <li> <p>более 7000 оформленных страховок в день.</p> </li> </ul> <p><strong>В чем ценность данной вакансии:</strong></p> <ul> <li> <p>Участие в развитии высоконагруженного DWH на современной и высокотехнологичной платформе;</p> </li> <li> <p>Возможность дать простор своим идеям, команда всегда приветствует инициативу и прислушивается к идеям;</p> </li> <li> <p>Возможность выбирать наиболее интересный для тебя стек задач;</p> </li> <li> <p>Результат работы будет влиять на процесс принятия решений в компании.</p> </li> </ul> <p><strong>Основной стек технологий на наших проектах:</strong></p> <ul> <li> <p>Хранилище на базе Snowflake с 6тб сжатых поколоночно данных;</p> </li> <li> <p>Более 150 активных пользователей;</p> </li> <li> <p>60+ источников данных в виде MSSQL, MySql, Postgres, MongoDB, API и тд;</p> </li> <li> <p>Stream через Kafka с точкой входа в виде API;</p> </li> <li> <p>ETL на Python+Prefect с ~15к сборок в сутки;</p> </li> <li> <p>Системы отчетности: PowerBI.</p> </li> </ul> <p><strong>Обязанности:</strong></p> <ul> <li> <p>Построение слоя data mart (витрины данных) в DWH;</p> </li> <li> <p>Покрытие кода автотестами, ci/cd разработка;</p> </li> <li> <p>Выстраивание технических “правил игры” для дашбордов в PowerBI и Snowsign;</p> </li> <li> <p>Поддержка существующей инфраструктуры, скриптов, отчетов.</p> </li> </ul> <p><strong>Что мы хотим видеть в вас:</strong></p> <ul> <li> <p>Опыт работы на позиции Data Engineer/SQL Developer/BI Developer не менее двух лет;</p> </li> <li> <p>Глубокое знание SQL и умение разобраться в Python-коде;</p> </li> <li> <p>Знать все (или почти все) о построении витрин данных, понимание различных моделей данных;</p> </li> <li> <p>Хороший опыт в оптимизации OLAP запросов;</p> </li> <li> <p>Опыт в разработке отчетов в одной из BI систем Power BI/Superset/Tableu/Qlik и др.</p> </li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li> <p>Понимание устройства инфраструктуры DWH проектов;</p> </li> <li> <p>Опыт работы со Snowflake, в частности разработка дашбордов в Snowsign;</p> </li> <li> <p>Опыт разработки на python.</p> </li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li>Работу в аккредитованной IT компании;</li> <li>Возможность работать <strong>удаленно</strong>;</li> <li>ДМС, включая стоматологию, страхование жизни;</li> <li>2 day-off в год;</li> <li>Каждые полгода проводим ревью всех сотрудников, составляем планы развития и даём возможность расти по карьерной лестнице;</li> <li>Пицца/пироги/суши каждую пятницу;</li> <li>Оплату посещения профильных конференций и курсов;</li> <li>Реферальная программа для сотрудников;</li> <li>Материальная помощь при рождении ребёнка +3 дня оплачиваемого отпуска;</li> <li>Ремоут-френдли. Доставим всё, что нужно для комфортной работы, и организуем встречи с командой онлайн и офлайн.</li> <li>Поддержку в обмене знаниями и идеями: поощряем выступления на митапах и помогаем с подготовкой докладов.</li> <li>Корпоративные мероприятия и тимбилдинги;</li> <li>Помощь с переездом для кандидатов из других городов (оплата билетов и первого месяц квартиры).</li> </ul>',\n",
       " '78124365': '<p><strong>Flowwow </strong>— маркетплейс, где продают свои товары тысячи локальных магазинов в 950 городах по всему миру, наше удобное приложение позволяет заказать, оплатить и отследить доставку цветов, тортов, украшений, картин и других товаров для себя и близких — где бы вы ни находились! Наша команда создает качественный удобный сервис, который делает счастливыми тысячи людей по всему миру.</p> <p><br /><br /><strong>Что нужно делать:</strong></p> <ul> <li> <p>Разрабатывать и оптимизировать процессы выгрузки данных из различных источников</p> </li> <li> <p>Разрабатывать процессы обработки данных</p> </li> <li> <p>Оркестрация ETL/ELT на Airflow</p> </li> <li> <p>Развитие Data Quality фреймворка</p> </li> <li> <p>Развитие Airflow DAG генератора</p> </li> <li> <p>Глубокая работа с Data Catalog и DA</p> </li> <li> <p>Рефакторинг кода DE проекта</p> </li> <li> <p>Участие в формировании Data Flow</p> </li> </ul> <p><strong>Что мы ждем от вас:</strong></p> <ul> <li> <p>Опыт работы от 2 лет</p> </li> <li> <p>Опыт работы с любым cloud провайдером</p> </li> <li> <p>Python: ООП, asyncio, multiprocessing, pandas</p> </li> <li> <p>Опыт работы с api (интеграции)</p> </li> <li> <p>Понимание разницы между ETL/ELT OLAP/OLTP</p> </li> <li> <p>Хорошие навыки работы с консолью</p> </li> <li> <p>Уверенные знания SQL: агрегации, джойны, оконки, вложенные запросы, индексы, оптимизация запросов</p> </li> <li> <p>Airflow (операторы, хуки, сенсоры, restapi plugin)</p> </li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li> <p>Опыт в разработке сервисов (Fastapi)</p> </li> <li> <p>Опыт в web-парсинге (Selenium)</p> </li> <li> <p>Devops опыт (Docker, k8s, grafana, настройка окружения на серверах и тд.)</p> </li> </ul> <p><strong>Условия:</strong></p> <p><em><strong>С первого дня —</strong></em></p> <ul> <li> <p>работа в технологической компании над сервисом, приносящим пользу сотням тысяч людей;</p> </li> <li> <p>официальное трудоустройство;</p> </li> <li> <p>культура открытости и взаимопомощи: у нас работают люди, вовлеченные в процесс и не безразличные к тому, что они делают;</p> </li> <li> <p>возможность быстро реализовать свои идеи и чувствовать вклад в общий успех;</p> </li> <li> <p>удалённая работа или современный комфортный офис в центре Москвы на ваш выбор;</p> </li> </ul> <p><em><strong>После прохождения испытательного срока —</strong></em></p> <ul> <li> <p>полис ДМС со стоматологией;</p> </li> <li> <p>возможность проходить профильные курсы/обучение за счет компании;</p> </li> <li> <p>подписка на консультации с психологом (по необходимости);</p> </li> <li> <p>компенсация расходов на занятия спортом.</p> </li> </ul> <ul> </ul> <ul> </ul>',\n",
       " '79144555': '<p>В Центр поддержки и сопровождения инструментов мониторинга защиты объектов открылся набор java разработчиков.</p> <p>Мы ждем именно твой отклик!</p> <p><strong>Обязанности</strong></p> <ul> <li>Анализ исходных данных в различных системах и форматах для решения бизнес-задач (оценка структуры, качества, полноты и применимости данных)</li> <li>Загрузка, очистка и трансформация больших объемов данных из различных источников в базу данных</li> <li>Проектирование и разработка аналитических витрин данных</li> <li>Мониторинг и оптимизация процессов загрузки, преобразования данных и сборки витрин</li> <li>Контроль качества загружаемых данных, разработка автоматизированных инструментов для оценки качества данных</li> <li>Разработка, поддержка и оптимизация инфраструктуры и внутренних сервисов для обработки больших объемов данных</li> <li>Разработка инструментов для автоматизации рутинных задач, связанных с обработкой данных</li> <li>Разработка и поддержка сопроводительной документации и спецификаций данных, развитие и поддержка базы знаний по вопросам работы с данными</li> <li>Предоставление экспертной поддержки внутренним потребителям (data analysts, data scientists) по вопросам, связанным с использованием данных.</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Опыт работы в качестве Data Engineer / Data Analyst / ETL Developer</li> <li>Знание SQL на продвинутом уровне (аналитические функции, подзапросы, хранимые процедуры, оптимизация производительности)</li> <li>Опыт работы с большими объемами данных</li> <li>Знание основных понятий и концепций из области Data Warehouse</li> <li>Желателен опыт разработки витрин данных</li> <li>Опыт работы со стеком технологий Big Data является большим преимуществом</li> <li>Опыт работы по Agile (SCRUM, Kanban, и т.д.) приветствуется.</li> </ul> <p><strong>Условия</strong></p> <ul> <li>месторасположения офиса: ул. Академика Сахарова 2а</li> <li>График работы: 8:00-17:00</li> <li>работа в крупнейшем банке России</li> <li>трудоустройство согласно ТК РФ</li> <li>регулярное корпоративное обучение</li> <li>ДМС, страхование от несчастных случаев и тяжелых заболеваний</li> <li>материальная помощь и социальная поддержка, корпоративная пенсионная программа</li> <li>льготные условия кредитования</li> <li>яркая и насыщенная корпоративная жизнь.</li> </ul>',\n",
       " '79072460': '<p>Проект в банке топ-10 по разработке и развитию корпоративного хранилища данных.</p> <p><strong>ЧЕМ ТЫ БУДЕШЬ ЗАНИМАТЬСЯ:</strong></p> <ul> <li>Разработка и миграция витрин;</li> <li>Составление и чтение маппингов;</li> <li>Создание ETL потоков (обязательно построение инкрементальной загрузки);</li> <li>Работа с системой контроля версий Git (ветвления, коммиты, пул-реквесты);</li> <li>Ведение рабочей документации;</li> <li>Оптимизация запросов;</li> <li>Анализ корректности и консистентности данных;</li> <li>Вывод в релиз.</li> </ul> <p><strong>ТЫ НАШ ИДЕАЛЬНЫЙ КАНДИДАТ, ЕСЛИ У ТЕБЯ:</strong></p> <ul> <li>Высокий уровень знаний по SQL;</li> <li>Опыт разработки функций PL/pgSQL (Postgres, Greenplum, допустим Oracle);</li> <li>Опыт оптимизации запросов;</li> <li>Опыт работы с ETL инструментами (Informatica, IBM Data Stage, Talend, etc);</li> <li>Опыт работы с отчетностью (SAP BO, IBM Cognos BI или другими BI средствами);</li> <li>Опыт работы с системами контроля версий (Git, SVN, etc);</li> <li>Spark, scala, python (на уровне чтение кода);</li> <li>Hive, Java (базовое владение).</li> </ul> <p><strong>СОВСЕМ КРУТО, ЕСЛИ ТЫ ИМЕЕШЬ:</strong></p> <ul> <li>Опыт и/или понимание на базовом уровне работы в confluence, jira;</li> <li>Опыт работы с Hadoop.</li> </ul> <p><strong>У НАС ТЫ СМОЖЕШЬ НАЙТИ:</strong></p> <ul> <li><strong>Работу в комфортабельном офисе г. Москва (Волгоградский проспект);</strong></li> <li>Прозрачную систему карьерного развития в компании;</li> <li>Персонального наставника с первого дня работы;</li> <li>Возможность развития личной экспертизы и экспертизы компании;</li> <li>Собственную платформу внутренних и внешних образовательных программ;</li> <li>Возможность пройти сертификацию;</li> <li>Возможность участия в обучении, конференциях, митапах;</li> <li>Неповторимую корпоративную культуру компании.</li> </ul>',\n",
       " '79179362': '<p>Яндекс Практикум — это образовательный сервис, который помогает людям освоить профессию с нуля или приобрести навыки, чтобы преуспеть в текущей. Главная ценность для нас — успех наших студентов, который подтверждают наши исследования.</p> <p>Команда дата-инженеров разрабатывает, эксплуатирует и развивает аналитическую инфраструктуру, которая помогает принимать решения другим командам Практикума. У нас атмосфера стартапа, мы много экспериментируем, работаем с гипотезами и непрерывно улучшаем внутренние процессы. Стремимся применять лучшие практики в инженерии данных, минимизировать toil work и автоматизировать процессы эксплуатации.</p> <p><strong>Что нужно делать:</strong></p> <ul> <li>заниматься инфраструктурой обработки данных для аналитики и пользовательских сервисов;</li> <li>проектировать и создавать витрины данных из «сырых» продуктовых источников и внешних API;</li> <li>настраивать эффективное обновление данных;</li> <li>развивать логическую архитектуру DWH для удобной работы аналитиков;</li> <li>отвечать за стабильность и надежность DWH и улучшать их;</li> <li>формулировать требования для новых источников данных и встраивать их в существующие модели агрегаторов данных.</li> </ul> <p><strong>Мы ждем, что вы:</strong></p> <ul> <li>больше трёх лет работали инженером данных;</li> <li>работали со специализированными аналитическими базами данных Greenplum, Vertica, ClickHouse, Teradata;</li> <li>работали с AWS, PostgreSQL;</li> <li>пишете на Python или Go чистый оптимальный код, который легко поддерживать и тестировать, или хотите этому научиться;</li> <li>покрываете свой код тестами;</li> <li>разрабатывали ETL-процессы;</li> <li>хорошо знаете SQL и понимаете устройство реляционных баз данных;</li> <li>проектировали витрины для дашбордов;</li> <li>готовы принимать решения и отвечать за них;</li> <li>хотите строить нагруженные системы и понимаете, как это делать;</li> <li>понимаете принципы отказоустойчивости и масштабирования сервисов.</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>участие в развитии социально-значимого образовательного сервиса;</li> <li>интересные задачи, возможность влиять на процессы и видеть результаты своей работы;</li> <li>гибкий график;</li> <li>возможность работать удалённо или из офиса;</li> <li>премии каждые полгода для всех, кто успешно прошёл ревью;</li> <li>расширенная программа ДМС (со стоматологией) и оплата 80% стоимости ДМС для супругов и детей;</li> <li>компенсация оплаты питания и мобильной связи;</li> <li>программы жилищных займов для сотрудников под 3% или без процентов;</li> <li>оплата обучения и участия в профильных конференциях;</li> <li>тренажёрный зал, массаж и капсула для сна в офисе;</li> <li>скидки у партнёров компании.</li> </ul>',\n",
       " '78778781': '<p>Приглашаем в нашу команду<strong> </strong>профессионального и амбициозного <strong>Senior Data scientist / Аналитика данных. </strong>Вам предстоит решать сложные, но очень интересные задачи. </p> <p><strong>Мы - международная финтех компания AMarkets. </strong>С 2007 года предоставляем передовые решения для торговли на финансовых рынках для трейдеров и инвесторов во всем мире.</p> <p><strong>Задачи, которые предстоит решать:</strong></p> <ul> <li> <p>Сбор, валидация и анализ данных из корпоративных информационных систем;</p> </li> <li> <p>Анализ исторических данных;</p> </li> <li> <p>Построение алгоритмических математических моделей;</p> </li> <li> <p>Разработка и обучение нейронных сетей;</p> </li> <li> <p>Анализ и прогноз временных рядов;</p> </li> </ul> <ul> <li> <p>Кластеризация и сегментация клиентской базы;</p> </li> <li> <p>Разработка моделей скоринга клиентов;</p> </li> <li> <p>Разработка моделей прогнозирования и моделирования сальдо торговых счетов клиентов;</p> </li> <li> <p>Разработка моделей прогнозирования и моделирования LT клиентов;</p> </li> <li> <p>Разработка моделей прогнозирования и моделирования оттока клиентов;</p> </li> <li> <p>Разработка моделей прогнозирования и моделирования LTV;</p> </li> <li> <p>Прогнозирование кол-ва обращений в поддержку;</p> </li> <li> <p>Прогнозирование и моделирование показателей эффективности клиентской поддержки;</p> </li> <li> <p>Выдвижение и проверка гипотез для улучшения ключевых показателей эффективности бизнеса;</p> </li> <li> <p>Разработка новых показателей (метрики, KPI).</p> </li> </ul> <p> </p> <p><strong>Наши ожидания:</strong></p> <ul> <li> <p>Опыт работы в финтех индустрии, брокерских, букмекерских компаниях или игорной индустрии от 3 - х лет;</p> </li> <li> <p>Высшее образование (желательно наличие ученой степени) в таких предметных областях, как: математика, финансы или экономика, информатика, физика, инженерия или аналогичные;</p> </li> <li> <p>Исследовательский склад ума: быть глубоким мыслителем, творческим человеком, настойчивым, умным, инициативным, внимательным к деталям;</p> </li> <li> <p>Критическое мышление и способность придумывать нестандартные подходы;</p> </li> <li> <p>Сильная рабочая этика;</p> </li> <li> <p>Уверенное пользование инструментами BI аналитики;</p> </li> <li> <p>Tools: MySQL, Google BigQuery, Python, Hadoop, Git;</p> </li> <li> <p>Английский язык: upper -intermediate (B2).</p> </li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li> <p>Сильный послужной список академических достижений (докторская степень, научные публикации, презентации на конференциях, гранты или награды);</p> </li> <li> <p>Знание финансовых рынков.</p> </li> </ul> <p> </p> <p><strong>Мы предлагаем:</strong></p> <ul> <li> <p>Релокация в Черногорию , г. Подгорица;</p> </li> <li> <p>В случае релокации - работа в комфортабельном офисе, переезд и легализация за счет компании;</p> </li> <li> <p>Конкурентную заработную плату в зависимости от ваших компетенций и опыта;</p> </li> <li> <p>ДМС, программа лояльности для сотрудников;</p> </li> <li> <p>Сплоченная команда профессиональных единомышленников;</p> </li> <li> <p>Амбициозные и интересные задачи.</p> </li> </ul>',\n",
       " '76954495': '<p><strong>PETER PARTNER</strong> – это молодая продуктовая IT-компания, которая занимается созданием собственных крутых финтех продуктов.</p> <p>Мы – команда профессионалов, готовых помочь с трудной задачей в понедельник утром и классно провести время в пятницу вечером. Придерживаемся agile-подхода. Для нас agile – это не только гибкая методология создания продуктов, но и прозрачность взаимоотношений внутри команды и вовлеченность каждого в общее дело.</p> <p>В нашем активе есть коммерчески успешные продукты. Мы реализовали систему по автоматизации торговли, которая интегрирована с крупными торговыми брокерами. Все продукты компании Peter Partner локализованы на множество языков и ими пользуется свыше 1 млн. человек в странах Азии, Африки и Южной Америки.</p> <p>Также, используя свой опыт в алгоритмах трейдинга мы разработали платформу, которая позволяет управлять активами на ведущих криптовалютных биржах. Мы работаем в технологически-сложной сфере и наша задача – сделать простой и удобный продукт, понятный для каждого.</p> <p>Наша команда развивает и поддерживает инфраструктуру для продуктовой аналитики в Peter Partner. Мы отвечаем за контуры обработки и трансформации данных (ETL), организацию хранения данных (DWH), средства визуализации (BI) и A/B тестирования.</p> <p>Мы в поисках Data-engineer который, также как и мы разделяет data driven подход, знает как искать insights в данных и сможет вывести наши продукты на новый уровень!)</p> <p>Это про тебя? Давай с нами!)</p> <p><strong>ТЕБЕ ПРЕДСТОИТ:</strong></p> <ul> <li> <p>Создание единого пайплайна обновления отчетности на базе Airflow + Clickhouse + Postgres;</p> </li> <li> <p>Создание витрин с разными слоями данных (сырые данные, агрегированные данные, таблицы под отчеты и датасеты для BI), проектирование модели данных исходя из бизнес-требований;</p> </li> <li> <p>Подключение и сверка новых источников Performance маркетинга;</p> </li> <li> <p>Формирование стратегии по работе с данными с четкой, понятной и описанной структурой;</p> </li> <li> <p>Проектирование структуры хранения данных (в том числе бэкапы, сжатие неиспользуемых данных), создание ETL-процессов;</p> </li> <li> <p>Выстраивание и поддержка процессов data quality (health-метрики, алерты);</p> </li> <li> <p>Поддержка и развитие инфраструктуры A/B тестирования.</p> </li> </ul> <p><strong>НЕОБХОДИМЫЕ НАВЫКИ:</strong></p> <ul> <li> <p>Уверенное владение Python.</p> </li> <li> <p>Владение SQL, понимание принципов оптимизации запросов и организации хранения данных;</p> </li> <li> <p>Автономность и проактивность в работе - умение находить, подсвечивать и решать проблемные моменты самостоятельно, не дожидаясь задач со стороны;</p> </li> <li> <p>Опыт разработки ETL-процессов с применением Airflow;</p> </li> <li> <p>Опыт работы с Clickhouse;</p> </li> <li> <p>Опыт работы с логированием пользовательских событий (в мобильном приложении);</p> </li> <li> <p>Желание разбираться в бизнес-особенностях данных с которыми предстоит работать.</p> </li> </ul> <p><strong>БУДЕТ ПЛЮСОМ:</strong></p> <ul> <li> <p>Понимание принципов разработки bi - как преимущество (SuperSet / Tableau / Looker / Power Bi или аналоги);</p> </li> <li> <p>Опыт решения DevOps задач - Docker, Kubernetes (или Nomad).</p> </li> </ul> <p><strong>НАШ СТЕК:</strong></p> <ul> <li> <p>Базы данных: Postgres, Clickhouse, Mongo DB;</p> </li> <li> <p>Для ETL: Airflow, Airbyte, DBT, Jitsu, Pipeline платформа Hevo;</p> </li> <li> <p>Внешние источники данных:</p> <ul> <li> <p>Facebook и Google - рекламные кабинеты;</p> </li> <li> <p>Branch - трекер атрибуций;</p> </li> <li> <p>Apphud - выручка;</p> </li> <li> <p>Amplitude - продуктовая аналитика.</p> </li> </ul> </li> <li> <p>BI - Superset.</p> </li> </ul> <p><strong>МЫ ПРЕДЛАГАЕМ:</strong></p> <ul> <li> <p>Стать частью амбициозной IT-команды с крутыми процессами и насыщенной корпоративной культурой;</p> </li> <li> <p>Официальное оформление, белую заработную плату;</p> </li> <li> <p>График работы 5/2 с гибким началом рабочего дня с 8:00 до 11:00 и возможностью работать как в офисе, так и удаленно;</p> </li> <li> <p>Работу в уютном, двухэтажном офисе с зоной отдыха, 2 кухнями и мини-библиотекой, и все это в 8 минутах ходьбы от ст.м. Выборгская;</p> </li> <li> <p>Образовательные мероприятия внутри команды (мы их назвали “ppsync”), где можно не только обмениваться опытом, но и прокачать скиллы ораторского мастерства и выступлений на публике;</p> </li> <li> <p>Регулярные командные мероприятия.</p> </li> </ul>',\n",
       " '78571917': '<p><strong>Чем будет заниматься разработчик по миграции данных?</strong></p> <ul> <li>разработкой продуктов для миграции данных с использованием ETL инструментов;</li> <li>внедрением ETL системы;</li> <li>разработкой дополнительного инструментария в целях оптимизации процессов миграции на проектах;</li> <li>поддержкой внедрения продукта у Клиента.</li> </ul> <p><strong>Мы ожидаем, что вы:</strong></p> <ul> <li>имеете высшее техническое образование;</li> <li>владеете навыками использования инструментов по работе с данными из различных источников и СУБД (PentahoDI, Talend, Apache NiFi);</li> <li>владеете моделированием данных и ETL: умеете проектировать и создавать масштабируемые и эффективные модели данных, нужен опыт работы с процессами ETL (извлечение, преобразование, загрузка) для перемещения данных между системами;</li> <li>имеете опыт работы с Apache AirFlow (или другого ПО оркестрации потоков, например Luigi);</li> <li>владеете Python, опыт практического использования от 2-х лет;</li> <li>имеете опыт работы с фреймворком Django;</li> <li>владеете навыками разработки реляционных баз данных;</li> <li>владеете SQL и имеете опыт практического использования;</li> <li>умеете читать и отлаживать собственный и чужой программный код;</li> </ul> <p><strong>Дополнительный плюс, если вы:</strong></p> <ul> <li>имеете опыт автоматизации процессов миграции;</li> <li>владеете навыками оптимизации запросов и процедур Oracle;</li> <li>умеете выявлять и решать сложные проблемы, связанные с хранением, обработкой и анализом данных, уделяете внимание деталям;</li> <li>обладаете коммуникативными навыками;</li> <li>любознательны и готовы изучать новые технологии, инструменты и методы, чтобы быть в курсе последних тенденций в области обработки данных.</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li>стабильную работу над масштабным комплексным решением;</li> <li>конкурентную заработную плату и бонусы;</li> <li>офис или удалёнку, на ваш выбор;</li> <li>надбавки за руководящие роли, за кураторство и др.;</li> <li>материальную помощь сотрудникам (в трудной ситуации);</li> <li>бонусы за рекомендации на открытые вакансии;</li> <li>опыт работы в крупном коллективе, распределенной команде;</li> <li>возможность карьерного роста;</li> <li>оплачиваемые внешние курсы и программы обучения;</li> <li>участие во внешних конференциях;</li> <li>внутреннее обучение и доступ к корпоративной библиотеке;</li> <li>корпоративные мероприятия онлайн и офлайн;</li> <li>подарки детям сотрудников на НГ;</li> <li>бесплатный сервис профессиональных медицинских онлайн-консультаций «Онлайн Доктор».</li> </ul>',\n",
       " '77330018': '<p>Компания BAUM - с 2010 года специализируется на системах хранения и обработки больших данных.<br />Ведем разработку zero-code платформы прикладного искусственного интеллекта, которая позволяет создавать модели и приложения ИИ, без необходимости прямого кодирования<br />В команду разработки платформы прикладного искусственного интеллекта требуется data-engineer</p> <p><strong>ОБЯЗАННОСТИ</strong></p> <ol> <li>Участие в разработке распределенного вычислительного кластера;</li> <li>Настройка Spark, как центрального фреймворка на платформе;</li> <li>Настройка load balancer kubernetes для возможности динамически выделять ресурсы пользователю в соответствии с квотами;</li> <li>Настройка облака, адаптация стека технологий под наш кластер, настройка БД, сервисов и подов;</li> <li>Выделение мощностей на основе конфиг файла от пользователя (разработатьресурс менеджер совместно с питонистами)Мониторинг инфраструктуры, Обновление CI/CD пайплайна;</li> <li>Обновление Helm Chart&#39;ов под нужды новых сервисов;</li> <li>Написание и обновление Dockerfile&#39;ов;</li> <li>Ввод в эксплуатацию новых нод k8s кластера;</li> <li>Ввод в строй новых стэйджей и поддержание текущих;</li> <li>Работа с БД и их резервным копированием.</li> </ol> <p><strong>ТРЕБОВАНИЯ</strong></p> <ol> <li>\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bОпыт работы с Apache Spark;</li> <li>Опыт работы с Apache Spark Streaming;</li> <li>Опыт работы с Apache Kafka;</li> <li>Опыт работы с kubernetes;</li> <li>Опыт работы с MongoDB, PostgreSQL.</li> </ol> <p><strong>ПРИВЕТСТВУЕТСЯ</strong></p> <ol> <li>Опыт разработки с Hadoop или S3;</li> <li>Опыт работы с облачными провайдерами;</li> <li>DevOps и MLops концепции.</li> </ol> <p><strong>НАШ СТЕК</strong></p> <ol> <li>Unix системы;</li> <li>Shell Script, JS, Python;</li> <li>Apache spark, streaming;</li> <li>Apache kafka;</li> <li>БД - Postgresql, MySQL, Hbase, MongoDB;</li> <li>Виртуализация KVM, Vmware;</li> <li>Микросервисы на Docker и Kubernetes;</li> <li>Системы мониторинга EFK/ELK, Grafana, Prometheus.</li> </ol> <p><strong>УСЛОВИЯ</strong></p> <ol> <li>Работа в стабильной, динамично развивающейся компании;</li> <li>Официальное трудоустройство с первого дня;</li> <li>Достойная з/п по результатам собеседования;</li> <li>График работы - полная занятость, гибкий график;</li> <li>Возможность профессионального и карьерного роста;</li> <li>Комфортабельный офис в центре города на Бауманской.</li> </ol> <p>Мы лояльны и не обременены бюрократией, так что твоя инициатива всегда в тему.</p> <p>\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b</p>',\n",
       " '78052551': '<strong>Обязанности:</strong> <ul> <li>Разработка и поддержка инфраструктуры для хранения и обработки данных;</li> <li>Создание ETL-процессов, используя внутренние и внешние источники данных;</li> <li>Обеспечение полноты и доступности данных для решения задач в области статистического анализа и машинного обучения;</li> <li>Наш технологический стек: CDH, Hadoop, Spark, Kafka, Hive, Impala, Kudu, Hue, Zeppelin, Jupyter, StreamSets.</li> </ul> <strong>Требования:</strong> <ul> <li>Опыт работы с распределенными системами хранения и обработки данных: Cloudera, Hadoop, Spark, Hive, Impala, Kudu;</li> <li>Опыт работы с распределенными системами построения потоков данных: Kafka, Oozie, Airflow, StreamSets;</li> <li>Опыт проектирования схем хранения данных;</li> <li>Владение языком программирования Python;</li> <li>Уверенные знания SQL;</li> <li>Опыт администрирования OC Linux;</li> <li>Желателен опыт администрирования Tableau;</li> <li>Желателен опыт сбора информации о действиях пользователей с UI (web, mobile app).</li> </ul> <strong>Условия:</strong> <ul> <li>У нас нет бессмысленного формализма и дресс-кода, мы не занимаемся микро-менеджментом и осмотрительно относимся к KPI;</li> <li>Удаленный режим работы;</li> <li>Амбициозные и интересные задачи;</li> <li>Конкурентная заработная плата в полном соответствии с ТК РФ;</li> <li>Соцпакет с первого дня работы (ДМС, скидки на корпоративные продукты Сбера)</li> <li>Стабильная Компания</li> </ul>',\n",
       " '78564408': '<p>Мы интернациональная команда, занимающаяся адаптацией, \\u2063 \\u2063локализацией и созданием новых сервисов экосистемы для дочерних банков экосистемы Сбер.</p> <p><strong>Чем предстоит заниматься:</strong></p> <p> </p> <ul> <li>полным циклом разработки всех слоев хранилища данных и отображения отчетности</li> <li>участвовать в составлении плана проекта, оценка проектных сроков и рисков</li> <li>участвовать в проектировании системы, решать сложные исследовательские задачи по реализации никем ранее не реализованного функционала</li> <li>развивать корпоративное хранилище и витрины данных (АС Облако данных) для развития международного бизнеса Сбербанка. Участвовать в сложных интеграционных проектах по развитию корпоративного DWH (преимущественно Hadoop, возможно, интеграции с БД Oracle и Teradata) в роли разработчика.</li> <li>разрабатывать и проектировать потоки данных, алгоритмы загрузки и обработки данных в Hadoop с использованием Apache Spark</li> <li>разрабатывать и оптимизировать ETL, обеспечивать производительность и стабильность, при необходимости участвовать в анализе инцидентов</li> <li>организовывать оптимальный процесс разработки</li> <li>участвовать в приемке системы</li> </ul> <p><strong>Мы ожидаем:</strong></p> <p> </p> <ul> <li>опыт работы в области Data engineer от одного года</li> </ul> <p>Работа с данными:</p> <ul> <li>знание SQL - простые запросы, Join`ы, агрегаты, группировки, вложенные запросы</li> <li>знание python: стандартные структуры данных (dict, list, set, модуль collections), pandas, numpy, h5py</li> <li>опыт работы с Hadoop (Hive, Spark, HBase) является плюсом</li> </ul> <p>Моделирование:</p> <ul> <li>Feature Engineering: - методы оценки значимости и отбора признаков, методы уменьшения размерности, приемы работы с текстом</li> <li>Model - умение различать основные классы задач (регрессия, классификация, кластеризация) и формулировать бизнес-задачу в их терминах. Знать основные методы и знать api по их использованию</li> <li>Python - sklearn, numpy, scipy, xgboost (в порядке убывания приоритета)</li> </ul> <p>Evaluation:</p> <ul> <li>умение различать методы оценки качества модели под основные классы задач и понимать плюсы и минусы их применения. (f1, precision, recall, roc auc, mse, rmse, silhouette)</li> <li>опыт работы с инструментами для организации и автоматизации работы: GridSearch, pipelines, ide, git, Jira, Confluence</li> <li>понимание методологии Agile и DevOps</li> <li>владение английским языком на уровне чтения технической документации</li> </ul> <p><strong>Мы предлагаем:</strong></p> <p> </p> <ul> <li>работа в команде профессионалов, возможность разрабатывать уникальные и крупные проекты масштаба нашей страны</li> <li>можно работать в офисе или в смешанном графике</li> <li>конференции и обучение на корпоративных или вендорских курсах за наш счёт</li> <li>отличная ДМС, включая несчастные случаи и тяжелые заболевания</li> <li>льготные условия по ипотеке и кредитам</li> <li>материальная помощь и социальная поддержка</li> <li>корпоративная пенсионная программа</li> <li>офис в бизнес-центре Поклонка (м. Кутузовская)</li> </ul>',\n",
       " '73888455': '<p>У нас в DWH очень много данных: 6000 объектов, 200 тб в Greenplum и 2 пт в Hadoop. С данными активно работают более 3000+ аналитиков в бизнес командах. На стороне команды DWH созданы инструменты, которые позволяют аналитику не зависеть от etl разработчика и самостоятельно собирать данные для своих задач. Сейчас мы ищем экспертов sql, готовых помогать аналитикам оптимально работать с данными.</p> <p><strong>Наша инфраструктура</strong></p> <p>• Greenplum / Hadoop / Clickhouse в качестве ядра обработки данных</p> <p>• Собственные решения на базе Apache Zeppelin, Apache Airflow и Apache Flink в качестве инструментов трансформации данных для аналитиков (selfservice etl)</p> <p>Мы не требуем знания этих инструментов от кандидатов. Наша задача сделать инструменты работы с данными максимально удобными и доступными. Но нам важно, чтобы кандидат на эту позицию имел большой опыт работы с данными.</p> <p><strong>Обязанности</strong></p> <ul> <li>Быть экспертом по данным: помогать аналитикам в решении вопросов с источниками данных, моделью данных DWH</li> <li>Разрабатывать витрины в помощь аналитикам</li> <li>Выступать заказчиком для разработки витрин в смежных командах</li> <li>Оптимизировать существующие запросы</li> <li>Внедрять и развивать культуру написания оптимальных запросов</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Высшее техническое образование</li> <li>Опыт работы с базами данных в качестве разработчика от 1 года</li> <li>Свободное владение SQL</li> <li>Опыт проектирования объектов БД на основании бизнес требований</li> <li>Понимание теории СУБД и ETL-процессов</li> <li>Знакомство с ETL-инструментами будет плюсом</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе у метро «Водный стадион». График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li> <p>Бесплатный фитнес-зал Tinkoff Sport. Тренируйтесь, посещайте групповые программы, грейтесь в сауне и участвуйте в спортивных турнирах</p> </li> <li>Бесплатные обеды в Tinkoff Cafe. А если захотите перекусить, на каждом этаже есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '77846851': '<p>12 лет компания <strong>Apptimizm </strong>создает удобные и полезные веб и мобильные сервисы. За нашими плечами огромный опыт и множество успешных реализаций бизнес идей. Каждый новый проект для нас — не только работа, но и любимое занятие.</p> <p>Мы сотрудничаем с такими компаниями как Канал RuTV, радио DFM, радио Монте-Карло, Icult.ru, BoomMarket, МТТ, Renault, Lada, Русское радио и многие другие. Мы с оптимизмом превращаем идеи клиентов в полноценные инструменты для ведения бизнеса.</p> <p><strong>Предстоящие задачи:</strong></p> <ul> <li>Дизайн и разработка витрин в Azure;</li> <li>Дизайн, разработка и поддержка ETL процессов для загрузки данных в/из Data Lake;</li> <li>Контроль качества и полноты данных (ручные и автоматические DQ тесты);</li> <li>Интеграции новых источников с DataLake;</li> <li>Написание документаций;</li> <li>Работать в связке с аналитиками и экспертами для получения end-to-end продукта.</li> </ul> <p><strong>Мы ожидаем от тебя:</strong></p> <ul> <li>Знание принципов построения DWH и баз данных;</li> <li>Опыт работы с технологией Hadoop (HDInsight, Spark, Hive, Scala, etc.);</li> <li>Уверенное знание SQL, Python/Scala or Java;</li> <li>1+ лет опыта с Azure / Yandex Cloud Platform.</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>Опыт работы с Greenplum;</li> <li>Опыт работы с Nifi, Kafka, Airflow.</li> </ul>',\n",
       " '79314123': '<p>У нас в DWH очень много данных: 6000 объектов, 200 тб в Greenplum и 2 пт в Hadoop. С данными активно работают более 3000+ аналитиков в бизнес командах. На стороне команды DWH созданы инструменты, которые позволяют аналитику не зависеть от etl разработчика и самостоятельно собирать данные для своих задач. Сейчас мы ищем экспертов sql, готовых помогать аналитикам оптимально работать с данными.</p> <p>Наша инфраструктура</p> <p>• Greenplum / Hadoop / Clickhouse в качестве ядра обработки данных</p> <p>• Собственные решения на базе Apache Zeppelin, Apache Airflow и Apache Flink в качестве инструментов трансформации данных для аналитиков (selfservice etl)</p> <p>Мы не требуем знания этих инструментов от кандидатов. Наша задача сделать инструменты работы с данными максимально удобными и доступными. Но нам важно, чтобы кандидат на эту позицию имел большой опыт работы с данными.</p> <p><strong>Обязанности</strong></p> <ul> <li>Быть экспертом по данным: помогать аналитикам в решении вопросов с источниками данных, моделью данных DWH</li> <li>Разрабатывать витрины в помощь аналитикам</li> <li>Выступать заказчиком для разработки витрин в смежных командах</li> <li>Оптимизировать существующие запросы</li> <li>Внедрять и развивать культуру написания оптимальных запросов</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Высшее техническое образование</li> <li>Опыт работы с базами данных в качестве разработчика от 1 года</li> <li>Свободное владение SQL</li> <li>Опыт проектирования объектов БД на основании бизнес требований</li> <li>Понимание теории СУБД и ETL-процессов</li> <li>Знакомство с ETL-инструментами будет плюсом</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li>Профессиональное развитие. Вы получите доступ к библиотеке с технической литературой, тренингами и мастер-классами для сотрудников</li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Оплату спорткомплекса Иннополиса</li> <li>Компенсацию оплаты аренды квартиры в г. Иннополис</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '79281902': '<p><strong>Команда занимается развитием платформы данных Комплаенс. В задачи команды входят:</strong></p> <ul> <li>аналитика источников и подготовка технических требования на их использование;</li> <li>разработка витрин данных для построения отчетности, исследований и интеграционных потоков;</li> <li>снабжение данными процессов Комплаенс;</li> <li>внедрение прогнозных моделей;</li> <li>разработка отчетных форм;</li> <li>мониторинг процессов передачи данных;</li> </ul> <p><strong>В задачи сотрудника будет входить:</strong></p> <ul> <li>разработка механизмов и объектов для хранения данных, аналитика данных в источниках (исследование, проектирование и разработка модели данных и хранилищ и витрин данных, механизмов наполнения хранилищ из источников данных, разработка enterprise решений). Так же потребуется написание unit-тестов на разработанный функционал (где применимо).</li> </ul> <p><strong>Функционал:</strong></p> <ul> <li>загрузка источников Комплаенс в ядро Облака данных;</li> <li>подготовка данных для аналитиков;</li> <li>разработка витрин данных;</li> <li>автоматизация расчёта витрин и моделей данных;</li> <li>интеграция с потребителями рассчитанных значений;</li> <li>визуализация данных Комплаенс (аналитическая отчётность).</li> </ul> <p><strong>Ожидаемые знания и умения:</strong></p> <ul> <li>Hadoop, ANSI SQL, PL/SQL, PL/pgSQL, Java, Python, ETL;</li> <li>владение SQL;</li> <li>навыки проектирования баз данных;</li> <li>знания в области мат. статистики;</li> <li>навыки разработки на Java или Python;</li> <li>умение документировать разработку.</li> </ul> <p><strong>Дополнительно:</strong></p> <ul> <li>опыт построения интеграционных решений;</li> <li>знание Scala,брокеров сообщений, ETL-инструментов, в том числе IPC будет плюсом.</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>Конкурентная оплата труда + бонусы по результатам работы;</li> <li>Возможность посещения всероссийских и международных IT-конференций;</li> <li>Профессиональное развитие: тренинги, митапы, мастер-классы, изучение английского языка не выходя из офиса, доступ к различным образовательным платформам;</li> <li>ДМС и страховка от несчастных случаев, льготные условия для близких родственников;</li> <li>Льготные предложения от компаний-партнеров, корпоративные условия от ведущих фитнес-клубов и многое другое;</li> <li>Офис в бизнес центре “Даниловская мануфактура”. Шаговая доступность от МЦК Верхние котлы, станции метро Тульская или офис метро Кутузовская, 7 мин пешком.</li> <li>Корпоративный транспорт, парковка для сотрудников.</li> </ul>',\n",
       " '76109157': '<p>Создание и развитие цифровых инструментов, улучшающих клиентский опыт, занимает ключевое место в стратегии Билайн. Здесь ждет работа с самыми современными технологиями и продуктами на базе AI, Big Data, 5G и других; гибкость и самостоятельность в планировании задач, возможность предлагать идеи и развивать экспертизу внутри. В феврале 2020 года Билайн был признан одним из самых привлекательных работодателей для IT-специалистов (по версии Хабр Карьера). Наша сильная сторона – это также и люди – профессионалы, готовые поделиться опытом, советом и поддержать тебя в работе над проектом.</p> <p>Если ты готов решать по-настоящему сложные задачи, которые, возможно, до тебя не решал никто, обладаешь аналитическими способностями, тебя не пугают масштабные проекты и профессиональные вызовы – присоединяйся!</p> <p>И сейчас у тебя есть возможность присоединиться к лучшей команде в качестве <em><strong>DATA Инженера!</strong></em></p> <p> </p> <p><strong>Итак, твои обязанности:</strong></p> <ul> <li>Анализ требований к витринам данных (взаимодействие с владельцем продукта, BI-разработчиками, datascientist-ами);</li> <li>Поиск и исследование источников данных для последующей интеграции во внутренних витринах;</li> <li>Оценка пригодности, качества исходных данных;</li> <li>Разработка ETL процессов на Spark;</li> <li>Оркестрация ETL процессов в Airflow;</li> <li>Проектирование баз данных.</li> </ul> <p> </p> <p><strong>Мы будем рады рассмотреть твою кандидатуру, если у тебя есть:</strong></p> <ul> <li>любовь к работе в команде и ты умеешь это делать;</li> <li>опыт с SQL на высоком уровне (в т. ч. DDL, табличные выражения, оконные функции);</li> <li>опыт работы с Hive, PostgreSQL ;</li> <li>опыт разработки ETL процессов Spark на Scala (потоковая обработка как преимущество);</li> <li>опыт: можешь что-то написать на Python – в объеме, чтобы пользоваться AirFlow или еще круче;</li> <li>умение проектировать базы данных (знает Data Vault 2.0 например);</li> <li>понимание принципов работы реляционных СУБД и HDFS;</li> <li>представление о колоночных и NoSQL СУБД;</li> <li>понимание подходов к работе с качеством данных.</li> </ul>',\n",
       " '79143960': '<p><strong>Компания Axenix</strong><strong> (ранее Accenture</strong><strong>)</strong> продолжает работу на российском рынке и аккумулирует 30-ти летний консалтинговый опыт внедрения инновационных решений.<br />Наша экспертиза - стратегия и консалтинг, технологии и операции, направленные на цифровизацию бизнеса.</p> <p>В своей деятельности мы сочетаем обширные знания, опыт в различных отраслях и глубокое понимание специфики и возможностей российского бизнеса.</p> <p>Офисы компании находятся в Москве, Твери и Ростове-на-Дону, Алматы и Ереване.<br />Помимо этого, у нас есть команды в Санкт-Петербурге, Краснодаре и Воронеже, а также сотрудники, работающие удаленно из других городов.</p> <p>На данный момент ищем<strong> Data Engineer в команду Data&amp;AI</strong>.</p> <p><strong>Что мы ожидаем от кандидата:</strong></p> <ul> <li>Опыт работы с Informatica PC;</li> <li>Опыт работы с ETL процессами, построения витрин, миграция данных;</li> <li>Уверенный уровень владения SQL.</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Динамичную работу без рутины в ведущей IT компании;</li> <li>Возможность использовать передовые технологии и стратегии и менять бизнес наших клиентов к лучшему;</li> <li>Проекты в разных индустриальных направлениях с передовыми технологическими решениями и современной архитектурой приложений;</li> <li>Конкурентоспособный уровень дохода, годовые бонусы и регулярное повышение по результатам Performance Review;</li> <li>Культуру непрерывного обучения: сертификация, online и offline обучение в России, менторство в профессиональном развитии;</li> <li>ДМС с первого дня работы, включая стоматологию, для cотрудника и его семьи (жена/муж, дети до 18 лет);</li> <li>Страхование жизни в размере годового оклада сотрудника;</li> <li>Дополнительные дни оплачиваемого отпуска в год;</li> <li>Ежемесячную денежную компенсацию на питание;</li> <li>Участие в корпоративных спортивных и развлекательных мероприятиях за счет компании.</li> </ul>',\n",
       " '72054128': '<p>Мы ищем в нашу практику по управлению данными дата-инженера. Вакансия обусловлена большим портфелем проектов в сфере DATA, предусмотренным стратегией развития нашей компании и имеющим высокий приоритет для бизнеса.</p> <p>Развитие ДАТА-практики охватывает контур компании и ее ключевых партнеров по цепочке создания ценности и предусматривает внедрение корпоративной платформы данных, включая построение классического DWH и озера данных (Data Lake), разработку приложений BI-аналитики и отчетности на платформе, обеспечение процессов интеграции данных. Работа организована по продуктовым командам, в которые входят представители бизнеса и ИТ (продакты, дата-инженеры, специалисты Data Science, аналитики данных и специалисты ИТ, отвечающие за сервисы платформы данных).</p> <p><strong>Обязанности:</strong></p> <ul> <li>Сбор и обработка данных из различных источников для проектов построения ML-моделей прогнозирования и ad-hoc аналитики по запросам бизнес-функций маркетинга, HR и финансов;</li> <li>Анализ требований ко входящим потокам данных;</li> <li>Проектирование потоков данных и схем загрузки;</li> <li>Профилирование и очистка данных;</li> <li>Разработка скриптов трансформации данных;</li> <li>Разработка интеграционного взаимодействия (REST API);</li> <li>Подготовка витрин данных и поставка данных для контента в аналитические приложения;</li> <li>Проведение ревью кода и участие в программе менторинга джуниор дата-инженеров команды.</li> </ul> <p><strong>Наши пожелания к знаниям и опыту:</strong></p> <ul> <li>Высшее образование</li> <li>Опыт работы в роли дата-инженера/ аналитика данных в продуктовых командах либо опыт участия в проектах построения DWH и Data Lake в роли ETL-разработчика – от 2 лет</li> <li>Твердые знания Python</li> <li>Навыки оптимизации SQL-запросов и опыт разработки на PostgreSQL, ClickHouse</li> <li>Опыт построения потоков данных (предпочтительно Ni-Fi, AirFlow)</li> <li>Практические навыки работы с данными на кластере Hadoop или S3</li> <li>Опыт написания задач для Spark, Kafka</li> <li>Базовые навыки MLOps (в объеме самообслуживания)</li> <li>Предпочтителен опыт работы с Яндекс Облаком и/или Сбер Cloud</li> <li>Умение писать чистый поддерживаемый код и техническую документацию</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>Работа в масштабном бренде в сфере быстрого обслуживания с уникальной историей</li> <li>Достойный уровень оплаты труда</li> <li>Продвинутая система бенефитов (годовой бонус, ДМС, мобильная связь, частичная компенсация фитнеса, 3-х летний накопительный план<em> </em>от оклада и премий)</li> <li>Гибридный график работы</li> <li>Профессиональное развитие и карьерный рост</li> <li>Возможность принять участие в цифровой трансформации масштабного бренда</li> </ul>',\n",
       " '79127190': '<p><em>Контур</em> — экосистема для бизнеса. Наши сервисы помогают тратить меньше времени на рутину, делают общение с контрагентами и госорганами проще и прозрачнее. У нас продуктовая разработка: 121 команда развивает как внутренние сервисы, которые приносят пользу внутри Контура, так и продукты для наших клиентов.</p> <p>У нас есть команда, которая разрабатывает инфраструктурный сервис Контур.Метрика. Им пользуются разработчики, аналитики, менеджеры разработки, маркетологи и все, кто принимает решения на основе данных. Ребята любят исследовать и внедрять новые подходы, всегда открыты для идей и улучшений. Команда растет и ищет опытного Data Engineer&#39;a.</p> <p><strong>Задачи</strong></p> <p>1. Настраивать, поддерживать и развивать ETL-процессы (AirFlow и наша инфраструктура для настройки потоков).</p> <p>2. Лидить зону Airflow: внедрять новые подходы, стандарты, драйвить изменения, работать над бэклогом, понимать потребности пользователей, предлагать решения.</p> <p>3. Писать конвертеры и адаптеры в нашей инфраструктуре по настройке потоков.</p> <p>4. Проводить ревью кода аналитиков данных.</p> <p><strong>Стек</strong></p> <ul> <li>Python, Airflow, Kubernetes, Redash;</li> <li> <p>C#, Net Core, Docker, Gitlab CI, Ansible, Git;</p> </li> <li> <p>Octopus, Teamcity, Moira, Graphite, Grafana, YouTrack;</p> </li> <li> <p>Singular, Vault;</p> </li> <li> <p>MS SQL, ClickHouse.</p> </li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li> <p>много интересных задач, современные технологии и инструменты, есть пространство для проверки гипотез и экспериментов;</p> </li> <li> <p>свободу для развития в смежных областях (девопс, дата-аналитика);</p> </li> <li> <p>быструю обратную связь от пользователей и внутренних заказчиков: сразу видно, как можно влиять на жизнь Контура;</p> </li> <li> <p>комьюнити опытных python-разработчиков, внутренние митапы и конференции, возможность прокачивать хард-скиллы в разработке.</p> </li> </ul> <p><strong>Требования</strong></p> <ul> <li> <p>опыт разработки на python: работы с данными, оптимизации запросов, проектирования и решения архитектурно сложных задач;</p> </li> <li> <p>опыт настройки ETL-процессов;</p> </li> <li> <p>опыт работы с SQL;</p> </li> <li> <p>понимание CI/CD концепции;</p> </li> <li> <p>знание баз данных, особенно аналитических;</p> </li> <li> <p>опционально — опыт работы с Airflow.</p> </li> </ul>',\n",
       " '79303157': '<p>Привет! Если вам интересна тема инвестиций, то мы в <strong>Альфа-Капитал </strong>как раз &quot;делаем это&quot; - мы занимаемся доверительным управлением. Если вы когда-нибудь слышали о <strong>паевых инвестиционных фондах</strong>, то мир наших продуктов будет вам уже отчасти понятен. Можно еще ИИС открыть или вообще индивидуальную стратегию сформировать. В общем, мы - это умные инвестиции с профессионалами :)</p> <p>Входим в ТОП-5, в цифрах это: <strong>11,8 млрд $ под управлением, 630К клиентов, 500 человек в компании.</strong> За 5 лет наши активы выросли в 5 раз, а количество клиентов - в 15!</p> <p><strong>Около 150 человек в нашей компании </strong>– это IT-команда, которая технически и технологически обеспечивает такой рост и результат.</p> <p>У нас нет адовой бюрократии и есть немного хаоса, который есть всегда, когда компания переживает рост, а значит перемены. У нас гибкие процессы, адекватные руководители, реально есть свобода и с другой стороны – ответственность, возможность влиять и менять все, если мы понимаем, что продукт станет лучше.</p> <p>Мы в поисках <strong>Инженера данных. </strong>Одна из больших предстоящих задач - переход на потоковую передачу данных</p> <p><strong>Что предстоит делать:</strong></p> <ul> <li>Разрабатывать и сопровождать решения для пакетной выгрузки данных из корпоративных систем в аналитическое хранилище с постепенным переходом на потоковые конвейеры данных;</li> <li>Прорабатывать логическую и физическую модели, подготавливать структуры данных хранилища (DDL, DML);</li> <li>Реализовывать средства мониторинга и автоматического тестирования разработанных компонентов;</li> <li>Проводить рефакторинг витрин данных для бизнес-пользователей аналитики;</li> <li>Формировать DQ проверки данных, разрабатывать средства мониторинга и информирования о качестве данных</li> </ul> <p><strong>Здорово, если у вас есть:</strong></p> <ul> <li>Опыт разработки потоковых конвейеров данных на Java/C#/Python или другом языке программирования</li> <li>Опыт работы с БД Oracle/PostgreSQL, владение SQL и навыками оптимизации запросов;</li> <li>Опыт проектирования аналитических хранилищ данных, знание методологий моделирования;</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <ul> <li><strong>Зарплата</strong> в рынке, есть ежегодный пересмотр, все прозрачно и договариваемся в моменте - это не тот вопрос, который мы не можем решить. Плюс есть годовая премия.</li> <li><strong>Оформление</strong> по ТК РФ в ИТ-компанию, все &quot;белое&quot;</li> <li><strong>Условия: </strong>ДМС со стоматологией, страховкой + скидка на подключение близких, мощное железо (windows/macOS, доп. монитор), скидка в World Class&amp;UFC Gym; библиотека МИФ, английский от Skyeng со скидкой, фрукты, снеки, напитки и автомат Вкусвилл в офисе, софинансирование программ по обучению, etc. – все, для того чтобы работать и жить было в радость</li> <li><strong>Формат:</strong> на ваш выбор - гибрид (офис на &quot;Маяковской&quot;) или удаленно</li> </ul>',\n",
       " '78984043': '<p>Команда Axenix<strong> (ex-Accenture)</strong> продолжает работу на российском рынке и аккумулирует 30-ти летний консалтинговый опыт внедрения инновационных решений. Наша экспертиза - стратегия и консалтинг, технологии и операции, направленные на цифровизацию бизнеса.</p> <p><strong>Чем тебе придется заниматься:</strong></p> <p>В рамках пилотного проекта создание пробных версий загрузки и сравнение результатов разных технологий:</p> <ul> <li>Обработка инкремента данных из кафки и применение к таблицам CLickHouse</li> <li>Загрузка данных в GP через External Tables</li> <li>Сравнение результатов Presto/GP/Clickhouse</li> <li>Создание артефактов для интеграции c Oracle(Batch-PULL)</li> <li>Создание артефактов для интеграции с Teradata (JDBC Batch-PULL)</li> <li>Создание артефактов для интеграции c MSSQL(Batch-PULL)</li> <li>Создание артефактов для интеграции c Postgres(Batch-PULL)</li> </ul> <p><strong>Мы ждем что у тебя есть:</strong></p> <ul> <li><strong>В идеале</strong> <em>Стэк</em> - pySpark, Python, Trino/Presto/Athena, Microsoft Azure Databricks или AWS EMR с поддержкой DeltaLake / Apache Iceberg, Yandex.Cloud, Airflow, git, JIRA/Confluence</li> <li><strong>Желательный минимум</strong> - PySpark (или любой Spark) + Hadoop</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li> <p>Динамичную работу без рутины в ведущей IT компании;</p> </li> <li> <p>Возможность использовать передовые технологии и стратегии и менять бизнес наших клиентов к лучшему;</p> </li> <li> <p>Проекты в разных индустриальных направлениях с передовыми технологическими решениями и современной архитектурой приложений;</p> </li> <li> <p>Конкурентоспособный уровень дохода, годовые бонусы и регулярное повышение по результатам Performance Review;</p> </li> <li> <p>Культуру непрерывного обучения: сертификация, online и offline обучение в России, менторство в профессиональном развитии;</p> </li> <li> <p>ДМС с первого дня работы, включая стоматологию, в лучших клиниках Москвы и МО для cотрудника и его семьи (жена/муж, дети до 18 лет);</p> </li> <li> <p>Страхование жизни в размере годового оклада сотрудника;</p> </li> <li> <p>Программу поддержки сотрудника и его родственников по психологическим, юридическим, финансовым вопросам;</p> </li> <li> <p>Дополнительные дни оплачиваемого отпуска в год;</p> </li> <li> <p>Ежемесячную денежную компенсацию на питание.</p> </li> </ul>',\n",
       " '78195446': '<p>В команде <strong>ETL X5</strong> <strong>Технологии</strong><strong>,</strong> в связи расширением и появлением новых задач по загрузке новых данных, открыта позиция <strong>Data engineer (интеграция с источниками)</strong><br />На данный момент, у нас построен кластер Hadoop общей емкостью 1 петабайт. Команда ETL отвечает за интеграцию кластера с источниками данных.<br />Мы загружаем данные из различных источников данных (как внутри компании, так и извне) в кластер Hadoop.<br />Результат работы разработанных нашей командой механизмов - стабильно обновляемые базы данных в Hive. На основе этих баз данных ДБД разрабатывает различные продукты для широкого круга заказчиков внутри X5 Group и снаружи.</p> <p><strong>Наш стек</strong><strong>:</strong> Apache Nifi, Kafka, Hadoop, Hive, Sqoop, Postgres, Java</p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Разработка приложений загрузки данных;</li> <li>Валидация данных;</li> <li>Отладка потоков данных;</li> <li>Исправление ошибок загрузки данных.</li> </ul> <p><strong>Наш кандидат:</strong></p> <ul> <li>Знание SQL (индексы, функции, умение читать планы запросов). Опыт работы с любой реляционной БД (Oracle, Postgres, MySQL, MsSQL, DB2 и т.п.).</li> <li>Умение писать на любом языке (Python, Groovy, Java и т.п.).</li> <li>Умение работать с Git в консоли.</li> <li>Опыт работы с любым графическим ETL инструментом (Apache Nifi, Airflow, Talend, Informatica, SAS и т.п.).</li> <li>Опыт работы с Apache Kafka и системами хранения и визуализации логов (примеры - EFK, Graylog). Опыт работы с Grafana.</li> <li>Опыт работы с Hadoop. Понимание устройства HDFS, форматов данных. Опыт работы с Hive или любым другим хранилищем на основе Hadoop.</li> <li>Желание самостоятельного усовершенствования своих навыков и повышения квалификации.</li> </ul> <p><strong>Будет плюсом</strong></p> <ul> <li>Опыт работы с NoSQL (Mongo, Redis, Cassandra, Clickhouse и т.п.)</li> <li>Опыт работы в смежных областях (Саппорт, системный анализ и т.п.)</li> <li>Опыт работы в больших компаниях</li> </ul> <p><strong>Мы предлагаем:</strong></p> <ul> <li>возможность работать удалённо или ездить в офис на м. Волгоградский проспект / м. Добрынинская/ м. Парк Культуры</li> <li>возможность роста: план развития, регулярная обратная связь, 2 раза в год оценка персонала</li> <li>яркую корпоративную жизнь с большим количеством мероприятий, конкурсов и возможностей для творческой реализации: X5 Tech Bar, регулярные внутренние митапы, демо-дни, открытые микрофоны, обмен опытом через внутренние комьюнити, день IT-специалиста, программы корпоративного волонтерства, корпоративное предпринимательство X5 Idea Challenge</li> <li>широкий пакет ДМС (включая выезд за рубеж и стоматологию), страхование жизни и здоровья; забота о благополучии сотрудников: регулярная онлайн-йога, ежедневные онлайн-зарядки по утрам, ЗОЖ-марафоны.</li> <li>скидки в экосистеме бизнесов Х5 («Пятёрочка», «Перекрёсток», «Карусель», «ОКОЛО», «Много лосося», «5post», «Перекресток Впрок»)</li> <li>программа привилегий Prime-zone (скидки на товары и услуги и специальные предложения от компаний-партнёров);</li> <li>материальную помощь сотрудникам, попавшим в сложную жизненную ситуацию</li> </ul> <p> </p> <p><strong>Узнать о нас больше вы можете в соц.сетях #X5TECH</strong></p> <p> </p>',\n",
       " '79050033': '<p>Мы - амбициозная команда, которая ищет такого же Data Engineer!</p> <p><strong>Обязанности</strong></p> <ul> <li>Разработка функционала (фронт\\\\бэк) в АС</li> <li>Написание алгоритмов расчета метрик</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Знание SQL: опыт написания сложных запросов, оконных функций, понимание принципов работы с БД</li> <li>Знание принципов обработки информации и навыки работы с большими данными, консолидация данных из различных источников</li> <li>Опыт написания ETL с использованием Python или SQL</li> <li>Знание языка программирования Python, Pandas или Apache Spark. Работа c DataFrame, преобразование типов данных, вычисление статистических функций</li> <li>Продвинутые навыки MS Excel</li> </ul> <p><strong>Условия</strong></p> <ul> <li>Занятость: полный день</li> <li>Формат работы: офис</li> <li>Локация: г. Пермь, ул. Куйбышева 66/1</li> <li>График работы: с 09:00 до 18:00</li> <li>Работа в крупнейшем банке России</li> <li>Трудоустройство согласно ТК РФ</li> <li>Регулярное корпоративное обучение</li> <li>ДМС, страхование от несчастных случаев и тяжелых заболеваний</li> <li>Материальная помощь и социальная поддержка, корпоративная пенсионная программа</li> <li>Льготные условия кредитования</li> <li>Яркая и насыщенная корпоративная жизнь.</li> </ul>',\n",
       " '79224519': '<p>Мы занимаемся развитием и поддержкой платформы Big Data на vk.com. Платформа построена как на проверенных решениях с открытым исходным кодом (Hadoop, Kafka, Spark, Zeppelin), так и на собственных разработках, заточенных под работу 24/7 в условиях высоких нагрузок.</p> <p>ВКонтакте — самая большая социальная сеть в России, поэтому у нас самая большая Big Data:</p> <ul> <li>Kafka ~ 1 Пбайт;</li> <li>HDFS ~ 50 Пбайт;</li> <li>Clickhouse ~ 3 Пбайт (NVMe).</li> </ul> <p>Ищем специалиста, который отлично владеет любым из этих инструментов.</p> <p><strong>Вам предстоит:</strong></p> <ul> <li>развивать платформу хранения и обработки Big Data;</li> <li>внедрять новые инструменты для анализа данных и машинного обучения;</li> <li>решать задачи производительности и отказоустойчивости инфраструктуры Big Data;</li> <li>строить новые и оптимизировать существующие ETL-процессы.</li> </ul> <p><strong>У нас интересно, потому что:</strong></p> <ul> <li>действительно много данных (десятки петабайт), настоящие и сложные задачи;</li> <li>мы не зацикливаемся на работе с одним инструментом или хранилищем, а всегда ищем наиболее подходящее решение;</li> <li>мы используем весь стек технологий — от железа и настроек инструментов до реализации собственных разработок.</li> </ul> <p><strong>Мы рассчитываем, что вы:</strong></p> <ul> <li>хорошо знаете Java/Scala, Python;</li> <li>разбираетесь в принципах работы баз данных, распределённых систем хранения и обработки данных;</li> <li>работали и понимаете внутреннее устройство Hadoop, HDFS, Kafka, Spark, Zeppelin, Airflow, ZooKeeper, ClickHouse.</li> </ul> <p><em><strong>Приглашаем специалиста, который сможет посещать офис в Москве или Санкт-Петербурге или работать в комбинированном режиме. Ждём ваших откликов. Удачи!</strong></em></p>',\n",
       " '79156500': '<p>Приглашаем <strong>Data Engineer-a </strong>присоединиться к команде Департамента централизованного управления данными.</p> <p>В Росбанке возможен полностью дистанционный формат работы: откуда работать - решать только тебе! Ты можешь работать из любой точки РФ! Приходи к нам, чтобы работать над амбициозными и интересными проектами и наслаждаться балансом между работой и личной жизнью!</p> <p>Росбанк – частный универсальный банк, работающий на российском рынке с 1993 года. Росбанк обслуживает порядка 1,5 млн активных розничных клиентов, около 83 тысяч активных клиентов малого бизнеса и около 10 тысяч активных клиентов корпоративного бизнеса более чем в 60 регионах России. Росбанк входит в ТОП 5 самых надежных российских банков по версии журнала Forbes и включен Банком России в число 13 системно-значимых кредитных организаций России. Наш банк имеет наивысшие кредитные рейтинги российских и международных рейтинговых агентств.</p> <p><strong>Обязанности:</strong></p> <p> </p> <ul> <li>Реализация ETL-загрузки данных из внешних и внутренних источников (Python, Spark, Hadoop);</li> <li>Подготовка витрин для анализа (Hive + Spark+ SQL);</li> <li>Раскладка данных в хранилище на сделочном слое (GreenPlum), разработка функций по раскладке;</li> <li>Разработка витрин на слое Enterprise Marts (em) хранилища (GreenPlum);</li> <li>Построение, развитие и поддержка дата-инфраструктуры МСБ (на уровне DWH в GreenPlum).</li> </ul> <strong>Требования:</strong> <ul> <li> <p>Опыт работы с реляционными базами данных, продвинутый уровень;</p> </li> <li> <p>Желательно GreenPlum/PostgreSQL от 2 лет, или T-SQL или PL/SQL от 2 лет;</p> </li> <li> <p>Уверенное владение Python;</p> </li> <li> <p>Опыт использования эко-системы Hadoop: HDFS, Apache AirFlow, Hive, Kafka.</p> </li> </ul> <strong>Условия:</strong> <ul> <li>Полностью дистанционный формат работы – это возможность работать из любой точки РФ;</li> <li>Стабильный и прозрачный доход: зарплата и премии по результатам работы;</li> <li>График 5/2;</li> <li>31 календарный день оплачиваемого отпуска;</li> <li>Забота о здоровье сотрудников: программа ДМС, включая стоматологию и консультации психолога, страхование при выезде за рубеж, скидки на абонементы в фитнес-клубы, страхование жизни;</li> <li>Дополнительная оплата за больничный лист: доплата сверх пособия по болезни до оклада (до 5 рабочих дней в год);</li> <li>Кадровый электронный документооборот, который позволяет подписывать кадровые документы (дополнительные соглашения к трудовому договору, приказы и т.п.) в электронном виде;</li> <li>Личностное развитие и рост: корпоративная электронная библиотека Альпина с книгами, курсами, тестами по бизнесу и саморазвитию, возможность прохождения бесплатного обучения и тренингов, участия в регулярных встречах корпоративных клубов и скидки на курсы он-лайн школ;</li> <li>Скидки и привилегии по кредитам, ипотека на льготных условиях, льготные условия обслуживания и т.п.;</li> <li>Программа корпоративных скидок и привилегий Primezone: развлечение, отдых, товары для взрослых и детей, услуги (авиакомпании, рестораны, магазины электроники и т.п.), обучение;</li> <li>Программа волонтерства и благотворительности;</li> <li>Сообщества по интересам сотрудников.</li> </ul>',\n",
       " '79271442': '<p><em>Мы команда Digital Apps, и мы занимаемся развитием маркетинговых технологий для Сбербанка. Мы создаем инструменты сквозной аналитики, позволяющие отследить весь пользовательский путь от просмотра баннера до первой транзакции, автоматизировать запуск рекламы в Digital и эффективнее управлять ей. Наша основная цель – быть #1 в привлечение клиентов в цифровом пространстве.</em></p> <p> </p> <p><strong>Задачи:</strong></p> <ul> <li>Загрузка, очистка и трансформация больших объемов данных из различных источников (RDBMS, Hadoop, плоские файлы) в рабочую область (платформы Hadoop)</li> <li>Проектирование и разработка аналитических витрин данных</li> <li>Проектирование и разработка коннекторов для ETL процессов</li> <li>Мониторинг и оптимизация процессов загрузки, преобразования данных и сборки витрин</li> <li>Разработка, поддержка и оптимизация инфраструктуры и внутренних сервисов для обработки больших объемов данных;</li> <li>Разработка инструментов для автоматизации рутинных задач, связанных с обработкой данных</li> </ul> <p><strong>Hard Skills: </strong></p> <ul> <li>SQL на продвинутом уровне (аналитические функции, подзапросы, хранимые процедуры, оптимизация производительности).</li> <li>Cтек технологий Big Data (Hadoop, Spark, Hive/Impala) и любой СУБД.</li> <li>Знание понятий и концепций DWH</li> <li>Python (PySpark, Pandas, NumPy, REST API);</li> <li>Airflow/Dagster/Oozie</li> <li>BitBucket\\\\Git</li> <li>Bash</li> <li>HTML, JavaScript</li> <li>Знание YAML</li> </ul> <p><strong>Soft Skills:</strong></p> <ul> <li>Внимательность к деталям</li> <li>Аналитические навыки</li> <li>Коммуникационные навыки</li> <li>Ответственность</li> </ul> <p><strong>Будут плюсом </strong><strong>Hard+</strong><strong>Soft</strong>:</p> <ul> <li>Знание банковского бизнеса;</li> <li>Опыт работы по Agile (SCRUM, Kanban, и т.д.);</li> </ul> <ul> <li>Опыт работы в качестве Data Engineer / Data Analyst / ETL Developer</li> <li>Знание Scala/ Java, Maven</li> </ul>',\n",
       " '78936775': '<strong>Вместе с нами ты будешь:</strong> <ul> <li>Заниматься проектированием и разработкой витрин данных для анализа и моделирования</li> <li>Заниматься мониторингом и оптимизацией процессов сборки витрин</li> <li>Заниматься загрузкой и обработкой данных из различных источников</li> <li>Заниматься поддержкой и развитием базы знаний</li> <li>Предоставлять экспертную поддержку внутренним потребителям(data analysts,data scientists)</li> </ul> <p><strong>\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bКакие знания и навыки для нас важны:</strong></p> <ul> <li>Знание SQL</li> <li>Хорошее знание устройства Hadoop,Spark,Hive/Impala</li> <li>Опыт разработки на Python/Scala/Java</li> <li>Понимание основных концепций DWH</li> <li>Понимание базовых команд Git и основных приципов работы</li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>Опыт работы с банковскими данными</li> <li>Опыт работы с различными СУБД в роли разработчика/аналитика витрин данных</li> <li>Опыт работы с Airflow</li> </ul> <p>\\u200b\\u200b\\u200b\\u200b</p>',\n",
       " '78632954': '<p><strong>Кто мы</strong></p> <p>Мы – команда МоегоСклада. Уже больше 15 лет развиваем и продаем веб-сервис, упрощающий жизнь малому и среднему бизнесу.</p> <p>У нас налажен и постоянно развивается процесс разработки. Сделанные в первый день работы тикеты могут уже завтра быть на проде. Продукт большой и сложный, пользователи активно делятся пожеланиями. Мы придумываем как реализовать полезный для людей продукт – и делаем.</p> <p>Мы делаем совершенно разные вещи - создаем новые фичи, пилим интеграции с самыми популярными маркетплейсами, поддерживаем наши работающие решения, делаем инструменты аналитики более ценными, улучшаем онбординг и дизайн, исследуем пользователей, развиваем API и многое-многое другое.</p> <p><strong>Мы ищем того, кто</strong></p> <ul> <li> <p>Понимает принципы организации хранилищ данных, подходов к проектированию логической и физической моделей;</p> </li> <li> <p>Хорошо знает SQL (в т.ч. DDL, агрегации, джойны, вложенные запросы, аналитические запросы, опыт оптимизации запросов);</p> </li> <li> <p>Имеет опыт оркестрации ETL с помощью Apache Airflow или других систем;</p> </li> <li> <p>Знает и имеет опыт работы с PostgreSQL, Clickhouse;</p> </li> <li> <p>Знает Python на уровне написания собственных операторов и хуков для Apache Airflow;</p> </li> </ul> <p><strong>Будет плюсом:</strong></p> <ul> <li>Опыт в построении CI/CD pipeline с использованием Gitlab CI;</li> <li>Опыт работы с брокером сообщений Apache Kafka;</li> <li>Опыт работы с Apache Nifi;</li> <li>Построение отчётов и визуализации данных на основе Metabase или иных средств визуализации;</li> </ul> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li> <p>Разработка и поддержка инфраструктуры для хранения и обработки данных;</p> </li> <li> <p>Участие в разработке и развитии текущих архитектурных решений по обработке данных;</p> </li> <li> <p>Разработка новых ETL pipelines и поддержка существующих, используя внутренние и внешние источники данных;</p> </li> <li> <p>Разработка дашбордов с использованием BI-системы;</p> </li> <li> <p>Анализ требований к витринам данных (взаимодействие с владельцем продукта, разработчиками из других команд);</p> </li> <li> <p>Ведение документации;</p> </li> </ul> <p><strong>Что мы предлагаем</strong></p> <ul> <li> <p>Официальное оформление и белую зарплату</p> </li> <li> <p>Гибкое начало рабочего дня</p> </li> <li> <p>Можно работать удаленно или в нашем комфортном офисе — в Москве в двух минутах от метро Автозаводская/удаленно</p> </li> <li> <p>Отсутствие бюрократии: все онлайн в удобном интранете</p> </li> <li> <p>В офисе — компенсируем оплату обедов, закупаем фрукты и перекусы</p> </li> <li> <p>Профессиональное развитие (оплата обучения, корпоративная библиотека, выступления на конференциях)</p> </li> <li> <p>5 оплачиваемых “дней без больничного” в год</p> </li> <li> <p>Компенсация больничного и отпуска — 100 % оклада</p> </li> <li> <p>Компенсируем 50% затрат на спорт</p> </li> <li> <p>Предоставляем скидку 50% на сервис онлайн-психотерапии</p> </li> <li>Возможность оформить отсрочку от призыва и мобилизации, льготную ИТ–ипотеку</li> <li> <p>А еще мы дарим классный фирменный мерч</p> </li> <li> <p>Подключаем к ДМС со стоматологией после окончания испытательного срока.</p> </li> </ul>',\n",
       " '67854817': '<p>Мы создаем собственную платформу данных, которая включает в себя инструменты по получению, обработке, загрузке и визуализации данных. С помощью этих инструментов мы каждый день загружаем терабайты информации в единое хранилище данных (DWH), приводим ее к удобному виду, предоставляем нашим пользователям и даем возможность на основании их строить важные отчеты, графики, проводить анализ. Часть из этих инструментов мы разрабатываем с нуля, часть - разрабатываем на основе opensource решений, таких как Apache Airflow, NiFi, Zepellin, Flink и других. В нашу команду нужны талантливые дата-инженеры, которые будут развивать продукты Data Platform.</p> <p><strong>Чем предстоит заниматься</strong></p> <ul> <li>Участие в разработке ядра продуктов Data Platform</li> <li>Разработка и оптимизация процессов выгрузки данных</li> </ul> <p><strong>Что вам необходимо</strong></p> <ul> <li>Знание одного из языков программирования: Python, Java, Go</li> <li>Знание основ ООП, теории алгоритмов, структур данных</li> <li>Знание SQL</li> <li>Опыт написания процессов загрузки данных (ETL)</li> <li>Будет плюсом знакомство с современными системами хранения данных: MPP, NoSQL</li> <li>Будет плюсом знакомство с: Pandas, Numpy, Groovy</li> <li>Исследовательский склад ума, любознательность, широкий кругозор, желание изучать новые технологии</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '77774985': '<p>Приглашаем <strong>Data Engineer-а</strong> присоединиться к нашей команде.<br /><br /><em>Мы занимаемся разработкой Centralized Data Platform (CDP) для домена Риски, создаем и развиваем скоринговые модели по клиентам.</em></p> <p>В Росбанке возможен полностью дистанционный формат работы: откуда работать - решать только тебе! Приходи к нам, чтобы работать над амбициозными и интересными проектами и наслаждаться балансом между работой и личной жизнью!</p> <p>Росбанк – частный универсальный банк, работающий на российском рынке с 1993 года. Росбанк обслуживает порядка 1,5 млн активных розничных клиентов, около 83 тысяч активных клиентов малого бизнеса и около 10 тысяч активных клиентов корпоративного бизнеса более чем в 60 регионах России. Росбанк входит в ТОП 5 самых надежных российских банков по версии журнала Forbes и включен Банком России в число 13 системно-значимых кредитных организаций России. Наш банк имеет наивысшие кредитные рейтинги российских и международных рейтинговых агентств.</p> <p> </p> <p><strong>Обязанности:</strong></p> <ul> <li>Создание витрин данных;</li> <li>Осуществление обработки и загрузки данных;</li> <li>Организация хранения данных в рамках скоринговых моделей;</li> <li>Мониторинг скоринговых моделей (настройка репликации данных на регулярной основе);</li> <li>Разработка отчетности по скоринговым продуктам лизинга.</li> </ul> <strong>Требования:</strong> <ul> <li>Опыт работы в роли Инженера данных от 2 лет;</li> <li>Уверенное владение SQL;</li> <li>Опыт работы с Python;</li> <li>Опыт работы с Hadoop, Spark;</li> <li>Знание основных алгоритмов обработки больших данных;</li> <li>Знание математической статистики и теории вероятности;</li> <li>Опыт работы в банковской сфере будет преимуществом.</li> </ul> <strong>Условия:</strong> <ul> <li>Полностью дистанционный формат работы – это возможность работать из любой точки;</li> <li>Стабильный и прозрачный доход: зарплата и премии по результатам работы;</li> <li>График 5/2;</li> <li>31 календарный день оплачиваемого отпуска;</li> <li>Забота о здоровье сотрудников: программа ДМС, включая стоматологию и консультации психолога, страхование при выезде за рубеж, скидки на абонементы в фитнес-клубы, страхование жизни;</li> <li>Дополнительная оплата за больничный лист: доплата сверх пособия по болезни до оклада (до 5 рабочих дней в год);</li> <li>Личностное развитие и рост: корпоративная электронная библиотека Альпина с книгами, курсами, тестами по бизнесу и саморазвитию, возможность прохождения бесплатного обучения и тренингов, участия в регулярных встречах корпоративных клубов и скидки на курсы он-лайн школ;</li> <li>Скидки и привилегии по кредитам, ипотека на льготных условиях, льготные условия обслуживания и т.п.;</li> <li>Программа корпоративных скидок и привилегий Primezone: развлечение, отдых, товары для взрослых и детей, услуги (авиакомпании, рестораны, магазины электроники и т.п.), обучение;</li> <li>Программа волонтерства и благотворительности;</li> <li>Сообщества по интересам сотрудников.</li> </ul>',\n",
       " '77291257': '<p><strong>Duties and Responsibilities:</strong></p> <ul> <li> <p>Gather actual data and update weekly reports on key QA/QC KPIs for weekly project status meetings (CODIR): Russian sites, Chinese yards.</p> </li> <li> <p>Gather actual data and update internal and external monthly presentations in the area of QAQC;</p> </li> <li> <p>On weekly bases ensure update Quality dashboards and registers such as but not limited to Non-Conformance Report register, Surveillance Reports, Punch Lists, retests, Audit findings, Lessons Learned, etc;</p> </li> <li> <p>Update, when necessary, the audit plan and on weekly bases prepare the execution status.</p> </li> <li> <p>Conduct the selective analysis of substantial deviations in QAQC KPIs reporting.</p> </li> <li> <p>Prepare ad-hoc QAQC presentations/ reports when requested by management.</p> </li> <li> <p>Develop and implement initiatives on automation of KPIs data extractions for various QAQC reports.</p> </li> <li> <p>Coordinate from QAQC side the development and implementation of IT systems (such as but not limited to NCRs and audit management, and Inspections management).</p> <p> </p> </li> </ul> <p><strong>Requirements:</strong></p> <ul> <li>University Degree;</li> <li>Advanced level in MS Word, Access, Excel, (including macros development) and Power Point skills.</li> <li>Excellent communication skills;</li> <li>Computer literacy;</li> <li>Accuracy with document flow;</li> <li><strong>Fluent English.</strong></li> </ul> <p><strong>Conditions:</strong></p> <ul> <li>Moscow office near Shabolovskaya metro station.</li> <li>Arctic LNG 2 Project; 100% official employment and salary.</li> <li>Competitive Salary.</li> <li>Medical insurance.</li> <li>Highly professional international team.</li> </ul>',\n",
       " '79091569': '<p><strong>Big Data МТС </strong>– место, где телеком данные превращаются в реально работающие IT-продукты. Мы создали и протестировали несколько десятков сервисов. Самые успешные из них уже стали частью экосистемы МТС. Например, МТС Маркетолог, рекомендации в KION (МТС ТВ), услуга “Кто звонит?” или Спам blacklist.</p> <p><strong>Кого мы ищем?</strong></p> <p>Data Engineer в направление Core Data Lake</p> <p><strong>Описание продукта:</strong></p> <p>Core Data Lake обеспечивает централизованное подключение коммунальных источников в Data Lake Big Data и создание коммунальных витрин для всех продуктов Big Data и аналитиков всех вертикалей МТС.</p> <p><strong>Обязательно:</strong></p> <ul> <li>коммерческий опыт работы Data Engineer (разработчик ETL) от 1 года</li> <li>знание Python (работа с большими объемами данных)</li> <li>знание SQL (аналитические функции, оптимизация)</li> <li>знание Spark (работа с большими объемами данных)</li> <li>знание любой реляционной СУБД (Orcle, Mysql, postgres)</li> </ul> <p><strong>Что предстоит делать?</strong></p> <p> </p> <ul> <li>загружать данные с реляционных источников в Data Lake, размещать их в соответствии с подходами команды, обвешивать мониторингами и DQ проверками</li> <li>дорабатывать программный код</li> <li>оповещать пользователей о планируемых работах и их результатах</li> <li>собирать сущности и витрины в хранилище Hadoop</li> </ul> <p><strong>Стек технологий: </strong></p> <p><strong>Условия: </strong>каждый месяц - аванс и зарплата, дважды в год - премия. ДМС + стоматология, корпоративная связь, специальные предложения от партнеров и друзей МТС, отпуск 31 день в год. Выдаем 16 MacBook Pro или Dell на выбор.</p> <p><strong>Есть ли обучение?</strong></p> <ul> <li>Локальные и международные конференции, митапы.</li> <li>Корпоративный университет МТС и масштабная виртуальная библиотека.</li> <li>А ещё мы регулярно обмениваемся опытом на совместных синках с лидами экспертизы</li> </ul> <p><strong>Какой график?</strong></p> <p>Гибкое начало рабочего дня в промежутке с 8 до 11. Есть возможность работать несколько дней вне офиса по договоренности с командой.</p> <p><strong>Сколько этапов при отборе?</strong></p> <p>Не более трех:</p> <ol> <li>HR + первое тех. интервью с лидом направления</li> <li>Тестовое задание/второе интервью - по необходимости</li> <li>Собеседование с PO и командой, выбор кандидатом проекта</li> </ol>',\n",
       " '79155981': '<p><strong>Кто мы:</strong></p> <p>BestDoctor — экосистема медицинских и страховых сервисов, созданная экспертами для удобного управления здоровьем. Мы меняем рынок медицинского страхования и отношение людей к своему здоровью с помощью качественного сервиса и принципиально нового клиентского опыта.</p> <p>Благодаря глубокой экспертизе в создании медицинских решений и любви к своему делу, нам удалось создать ту самую экосистему полезных, эффективных и, главное, простых медицинских сервисов для управления здоровьем компаний и людей.</p> <p><strong>Чего мы достигли:</strong></p> <p>⚡️ За семь лет мы вышли на второе место по числу клиентов среди insurtech компаний в Европе. Сегодня с нами уже 100 000+ застрахованных пользователей (40 000 из них мы подключили в 2021 году), а в списке клиентов — Мегафон, Aviasales, МТС Банк, Нетология, InDriver, Эвотор, Ivi.ru, Ostrovok.ru, VC.ru и еще 140 компаний.</p> <p>⚡️ Создали экосистему медицинских сервисов, которая включает: сервис онлайн-поддержки 24/7, сервис онлайн записи в оффлайн клинику, своя виртуальная клиника, сервис второго мнения специалиста, чекапы, сервис поддержки психологов и др.</p> <p><strong>Наша следующая цель</strong> — сделать все продукты, входящие в экосистему BestDoctor, мультисервисными, разработать подбор индивидуальных программ, создать возможность управлять бюджетом, улучшить HR-кабинет и умную маршрутизацию каждого сотрудника для B2B сегмента. И для этого ищем лучших экспертов, чтобы вместе захватить рынок.</p> <p><strong>Подробнее о сервисе BestDoctor:</strong> https://bestdoctor.ru/ и https://hh.ru/article/29681</p> <p><strong>О проекте:</strong></p> <p>За время существования компании мы накопили много данных и разных инструментов аналитики. На этих данных мы строим предложения для новых клиентов и продлеваем старых, проводим переговоры с клиниками и непосредственно помогаем нашим пациентам. Главная задача - весь этот информационный поток перенести в чётко организованную систему сбора, обработки и анализа данных любого объёма.</p> <p>Сейчас мы планируем вести сборку data lake house на базе GreenPlum, куда будут сливаться данные всех источников, таких как, PostgreSQL, Yandex,Google drive, сторонние API и др.). Мы мигрируем туда с Postgres+Astroniomer. Также у нас будет большой проект с фичастором и MLFLOW. В твоих задачах будет много архитектуры и хорошего продакшн кода, перенос, рефакторинг старого и написание очень динамического и автоматизированого нового, а также опыт с очень крутым датасайнсом, аналитикой и продуктом.</p> <p><strong>В целом, тебе предстоит:</strong></p> <ul> <li>Мигрировать даги с Airflow c Астрономера на Кубер;</li> <li>Развивать и оптимизировать GreenPlum(PXF) даги;</li> <li>Пилить Data Managment нового поколения;</li> <li>Интегрировать сторонние API.</li> </ul> <p><strong>Что для нас важно:</strong></p> <ul> <li>Опыт программирования на Python 3;</li> <li>Опыт работы с Airflow;</li> <li>Отличные знания и опыт работы с SQL.</li> <li>Опыт работы с GreenPlum и PostgreSQL;</li> </ul> <p><strong>Дополнительным плюсом будет:</strong></p> <ul> <li>Опыт работы с Apache Kafka;</li> <li>Навыки в DevOps / опыт работы с Docker;</li> <li>Опыт работы с Apache Spark;</li> <li>Опыт работы с хранилищами (DataVault Anchor LakeHouse FeatureStore)</li> </ul> <p><strong>Как мы нанимаем:</strong></p> <p>Мы готовы оперативно выходить с оффером, если понимаем, что подходим друг другу.</p> <ul> <li>1 этап - телефонное интервью с HR (15-20 минут);</li> <li>2 этап - техническое интервью с лидом аналитики;</li> <li>3 этап - финальное интервью с CDO и HR.</li> </ul> <p><strong>Почему с нами круто:</strong></p> <ul> <li>Мы меняем рынок медицинского страхования, и у нас это отлично получается;</li> <li>В нас поверили и проинвестировали топовые венчурные фонды в России: российский Winter Capital, шведский VNV Global и австрийская страховая компания Uniqa;</li> <li>Удаленный формат работы (будем рады тебя видеть у нас в офисе (м.Савёловская);</li> <li>У нас гибкий график работы, который подойдет как жаворонку, так и сове;</li> <li>Тебя будет окружать команда талантливых и мотивированных людей;</li> <li>Мы предоставим тебе ноутбук и всю необходимую технику, которая позволит эффективно и комфортно работать;</li> <li>Медицинское обслуживание через систему BestDoctor.</li> </ul> <p>❤️ Ты будешь частью <strong>большого социально значимого дела.</strong> Мы реализуем амбициозную задачу — меняем рынок здравоохранения, действительно помогаем людям, и у нас это отлично получается.</p> <p>И у тебя получится!</p>',\n",
       " '79117910': '<p>В связи с формированием проекта MLOps в компании Ингосстрах приглашаем в команду <strong>Дата инженера</strong></p> <p><strong>Проект: </strong>внедрение единой для компании платформы MLOps</p> <p><strong>Текущая команда проекта ML OPS: </strong>15 DS, 4 MLOps, Data engineer, выделенные специалисты от отделов Информационной безопасности, Devops, Инфраструктуры (K8s). Сейчас в проекте 25+ человек. Планируется дальнейшее расширение проекта</p> <p><strong>Предстоящие задачи:</strong></p> <ul> <li>Разработка ETL потоков</li> <li>Разработка витрин данных</li> <li>Миграция с Oracle (legacy) в PostgreSQL, Redis, Ceph(S3)</li> </ul> <p><strong>Для нас важно:</strong></p> <ul> <li>Опыт работы cо Spark/Hadoop, Kafka, HiveQL</li> <li>Опыт работы с PostgreSQL, Oracle</li> <li>опыт разработки на Scala/Python</li> <li>NoSQL (MongoDB, HBase)</li> <li>Ceph(S3)</li> <li>Желательно (необязательно)<br />o Flink, опыт работы со стриминговыми пайплайнами<br />o опыт работы c ArangoDB, Spark GraphX<br />o опыт работы c Redis</li> </ul> <p><strong>Предлагаем:</strong></p> <ul> <li>Оформление в штат по бессрочному трудовому договору, полностью белую заработную плату: оклад + стабильная ежеквартальная премия + годовой бонус;</li> <li>После 3 месяцев работы базовый пакет ДМС (поликлиника); спустя 9 месяцев работы – расширенный ДМС (стоматология, иммунотерапия, диспансеризация, лечение сложных заболеваний, телемедицина, плановая и экстренная помощь) + ДМС для родственников;</li> <li>Удаленный формат работы (однако если хотите работать в офисе, то в Москве и Санкт-Петербурге это можно организовать);</li> <li>Гибкое начало рабочего дня на ваше усмотрение: с 8:00 до 17:00, с 9:00 до 18:00, с 10:00 до 19:00;</li> <li>Бесплатный корпоративный доступ к электронной библиотеке «Альпина»;</li> <li>Льготный отдых в ГК «Имеретинский» в г. Сочи;</li> <li>Скидки на обучение в языковых школах Speak English и Skyeng – от 15 – 25%;</li> <li>Корпоративные предложения от сетей фитнес-клубов: WORLD CLASS, Зебра, X-fit;</li> <li>Скидки от партнеров на приобретение недвижимости, авто и др.;</li> <li>Подарки на Новый Год детям;</li> <li>Льготные страховые продукты (Страхование-Тур, Страхование-Авто, Страхование-Имущество);</li> <li>У вас будет наставник на период испытательного срока (3 мес), который поможет адаптироваться в компании + будет команда, которая ответит на ваши вопросы в процессе работы;</li> <li>Возможность внести свой вклад в развитие нового проекта и самой команды;</li> <li>Мы не ограничиваем и даем возможность дальше прокачивать hard skills на других проектах.</li> </ul>',\n",
       " '76153658': '<p><strong>Хочешь всё и сразу: хорошую зарплату уже на старте, стабильную работу в надежном банке, успешную карьеру и яркую корпоративную жизнь?</strong></p> <p><strong>С Совкомбанком все реально!</strong></p> <p><strong>Обязанности:</strong></p> <ul> <li>Построение ETL процессов, решение задач по организации сбора данных из различных источников в DWH</li> <li>Разработка и автоматизация процессов преобразования данных внутри DWH</li> <li>Организация хранения данных, оптимизация хранимых данных</li> <li>Сборка витрин данных в соответствии с бизнес-требованиями заказчика</li> <li>Проектирование и разработка OLAP-кубов</li> <li>Построение визуализационных моделей данных, отчетов и дашбордов (MS Reporting Service, Metabase)</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Хорошие знания SQL, опыт работы с одной из реляционной БД - MS SQL/PostgreSQL/Oracle в качестве разработчика</li> <li>Навыки проектирования реляционных баз данных и/или DWH</li> <li>Опыт работы с BI-системами (системами визуализации отчетов) .</li> <li>Желание расти и развиваться Будет плюсом:</li> <li>Опыт работы с инструментами Apache Airflow, MS Integration Service (или другой ETL-системой)</li> <li>Опыт работы с базой данных ClickHouse</li> <li>Опыт работы с системами визуализации данных (MS Reporting Service, Metabase и др.)</li> <li>Опыт работы с OLAP-кубами</li> <li>Навыки администрирования MS SQL, оптимизации производительности \\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>Официальное оформление с перового рабочего дня;</li> <li>Обучение в нашем Учебном центре и прокачку своих навыков;</li> <li>График работы 5/2 с 9.00 -18.00;</li> <li>Бесплатный полис ДМС;</li> <li>Отдых летом на море и зимой на горнолыжных курортах при финансовой поддержке от Банка;</li> <li>Компенсацию расходов на абонемент в спортзал, детский отдых и изучение английского языка;</li> <li>Соц.программы по самым выгодным условиям от Банка: кредит, автокредит, страхование и другие банковские продукты;</li> <li>Финансовую поддержку от Банка в различных жизненных ситуациях;</li> <li>Участие в корпоративных забегах, турнирах по шахматам, футбольных и волейбольных матчах;</li> <li>Свободное общение с Руководством Банка: предлагай любые идеи и реализовывай интересные проекты;</li> <li>Путешествия по России, знакомства с новыми людьми, конкурсы и корпоративы – живи яркой корпоративной жизнью!\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b</li> </ul> <p> </p> <p><strong>Откликайся на вакансию – присоединяйся к нашей команде!</strong></p> <p> </p> <p><em>Отклик на данную вакансию соискателем посредством сайта http://hh.ru является действием самого соискателя по предоставлению своего согласия в соответствии с Федеральным законом от 27.07.2006 г. № 152-ФЗ «О персональных данных» на осуществление со всеми указанными в том резюме соискателя на сайте http://hh.ru персональными данными (а также с самим резюме), которое включается в содержание отклика на данную вакансию, следующих действий: систематизацию, хранение, уточнение (обновление или изменение), использование, обезличивание, блокирование и уничтожение, для целей возможного трудоустройства соискателя у оператора персональных данных: ПАО «СОВКОМБАНК», ИНН 4401116480, 156000, г. Кострома, Текстильщиков пр-кт, дом 46, которому настоящее согласие предоставляется без ограничения срока обработки этих персональных данных, но до момента отзыва соискателем согласия на обработку этих персональных данных. Настоящее согласие на обработку персональных данных может быть отозвано соискателем путем направления соответствующего заявления по адресу оператора: 156000, г. Кострома, Текстильщиков пр-кт, дом 46.</em></p>',\n",
       " '78272299': '<p>Компания <strong>ARK </strong>- экосистема решений, позволяющих закрывать полный цикл по оптимизации бизнес процессов. Мы работали над сайтами, мобильными приложениями и IT платформами для крупнейших медийных, фармацевтических, киберспорт и FMCG компаний.</p> <p><strong>Собственное SFA-решение</strong> – автоматизация работы полевых команд (мерч, торговые представители, сотрудники торгового зала), Retail &amp; FMCG.</p> <p> </p> <p><strong>Чем предстоит заниматься:</strong></p> <ul> <li>Разработка витрин данных Teradata, MS SQL;</li> <li>Поддержка пользователей при работе с инструментами отчетности (вопросы управления доступом, корректности данных, скорости работы отчетов и выгрузок);</li> <li>Мониторинг и оптимизация процедур загрузки данных;</li> <li>Взаимодействие с другими техническими командами, в том числе, на иностранном языке (переписки, встречи);</li> <li>Оптимизация сложных объемных запросов к хранилищу данных;</li> <li>Ведение документации.</li> </ul> <p> </p> <p><strong>Требования:</strong></p> <ul> <li>Опыт разработки витрин данных и SQL-запросов (MS SQL/Teradata);</li> <li>Опыт работы с большими объемами данных и оптимизация запросов;</li> <li>Опыт работы с ETL-инструментами и опыт построения ETL процессов;</li> <li>Опыт разработки на языке Python;</li> <li>Опыт работы с Git и облачными решениями;</li> <li>Понимание принципов построения баз данных и принципов построения запросов из различных приложений к ним;</li> <li>Знание принципов архитектурного построения BI решений, умение оптимизировать и предлагать лучшие процессы в архитектуре;</li> <li>Понимание бизнес-процессов в одной или нескольких предметных областях: Заказы, Продажи, Поставки;</li> <li>Знание английского на разговорном уровне и возможность ведения рабочей переписки.</li> </ul> <p> </p> <p><strong>Будет плюсом:</strong></p> <ul> <li>Опыт разработки юниверсов SAP BusinessObjects (особенно контексты);</li> <li>Опыт построения отчетов и дашбордов SAP Business Objects;</li> <li>Опыт работы с ETL-инструментами и опыт построения ETL процессов;</li> <li>Опыт разработки на языке Python;</li> <li>Опыт работы с Git и облачными решениями.</li> </ul> <p> </p> <p><strong>Мы предлагаем:</strong></p> <ul> <li> <p>аккредитованная it-компания</p> </li> <li> <p>удаленная работа</p> </li> <li> <p>процесс обмена знаниями и рост в команде</p> </li> <li> <p>график работы 5/2</p> </li> <li> <p>интересные и нетривиальные задачи</p> </li> <li> <p>дружный коллектив</p> </li> </ul> <p> </p>',\n",
       " '78267157': '<p><strong>Обязанности:</strong></p> <ul> <li>Проектирование и разработка программно-технологической инфраструктуры витрин и хранилищ аналитических данных, ETL потоков данных, BI.</li> <li>Разработка и интеграционных модулей для получения/обмена аналитическими данными.</li> <li>Всесторонняя поддержка бизнеса со стороны данных.</li> <li>Ведение ETL процессов в средних и верхних слоях данных.</li> <li>Построение роадмапа и непосредственное участие в процессе по обмену данными.</li> <li>Программирование витрин DataMart, бэкенд разработка.</li> </ul> <p><strong>Требования:</strong></p> <ul> <li>Опыт проектной деятельности по Agile (SCRUM, Kanban).</li> <li>Стек технологий: PostgreSQL, Python, nosql, приветствуется R или C++, Apache Spark, Hadoop, MapReduce, Airflow, Data Analysis, Data Mining, Dashboards.</li> <li>Приветствуется ML и Data Science, Java платформы, микросервисная архитектура, REST и SOAP, BPMN, UML, иные нотации и инструментальное ПО описания процессов, моделей и потоков данных Data Governance и Data Lineage.</li> <li>Крайне приветствуется опыт систем мастер-данных UNIDATA и т.п., BI и OLAP инструментарий.</li> <li>Приветствуется опыт BigData и AI.</li> <li>Приветствуется опыт в информационной безопасности (ФСТЭК, ФСБ).</li> <li>Приветствуется опыт межведомственного и межсистемного взаимодействия (СМЭВ, ПГУ, Электронное правительство).</li> </ul> <p><strong>Условия:</strong></p> <ul> <li>Официальное трудоустройство в соответствии с ТК РФ.</li> <li>Конкурентная заработная плата и ежегодные бонусы.</li> <li>ДМС + стоматология.</li> <li>Гибкое начало рабочего дня и удаленный формат работы.</li> </ul>',\n",
       " '78391528': '<p>Яндекс хочет сделать повседневную жизнь людей более безопасной, удобной и комфортной. Мы ставим перед собой масштабную цель: оказаться в числе первых в мире компаний, создавших технологии беспилотного управления автомобилем и роботом-доставщиком. Чтобы воплотить это в жизнь, мы создаём команду увлечённых, умных и целеустремленных профессионалов.</p> <p> </p> <p>Развитию технологий беспилотного вождения помогает анализ данных. Так, за управление флотом и прозрачность ежедневных процессов отвечает операционная аналитика, за векторы будущего развития технологии — продуктовая и стратегическая. Развитию аналитики помогает выстраивание процессов управления данными, а у нас одни только сенсоры поставляют петабайты данных. Ищем специалистов, которые помогут решать задачи по анализу данных.</p> <p><strong>Что нужно делать:</strong></p> <ul> <li>проектировать и создавать витрины данных по процессам и технологиям YSDC во внутренней инфраструктуре Яндекса;</li> <li>реализовывать ETL-процессы по формированию аналитического слоя данных;</li> <li>создавать процедуры мониторинга контроля качества данных;</li> <li>создавать модель управления данными и выстраивать операционные процессы по управлению данными.</li> </ul> <p><strong>Мы ждем, что вы:</strong></p> <ul> <li>окончили технический вуз, отлично знаете мат. статистику, теорию вероятностей, методы оптимизации;</li> <li>системно мыслите, умеете получать, структурировать и систематизировать информацию, чётко излагаете мысли;</li> <li>уверенно знаете SQL, понимаете принципы построения моделей хранения и обработки данных;</li> <li>знаете Python.</li> </ul> <p><strong>Будет плюсом, если вы:</strong></p> <ul> <li>работали с хранилищами данных и проектировали схемы данных или ETL-процессы в реальных задачах;</li> <li>знаете инструменты оркестрации ETL-пайплайнов (Airflow или аналоги);</li> <li>работали с YT/YQL и другими инструментами инфраструктуры Яндекса;</li> <li>владеете современными инструментами визуализации данных на основе открытых фреймворков Superset, Seaborn, Shiny или промышленных BI-сред PowerBI, SAP BI, Tableau, Grafana, DataLens, Pentaho или их аналогов;</li> <li>знаете методы машинного обучения, участвовали в соревнованиях по программированию или обработке данных (Kaggle, TopCoder, ACM или подобных) или решали подобные задачи.</li> </ul>',\n",
       " '78044523': '<strong>Обязанности:</strong> <ul> <li>Обеспечивать и поддерживать бесперебойную работу микросервиса и поток данных,</li> <li>Сбор, обработка и очистка данных, связанных с микросервисом,</li> <li>Автоматизация мониторинга данных, работы scheduler и настройка оповещений,</li> <li>Организация и оптимизация хранения данных,</li> <li>Участие в формировании архитектуры баз данных, нацеленной на масштабируемость.</li> </ul> <strong>Требования:</strong> <ul> <li>Основной стек технологий: Python (numpy, pandas, sklearn), MongoDB, grpc, git,</li> <li>Опыт разработки или поддержки приложений на Python,</li> <li>Уверенное знание ETL-процессов и опыт их организации,</li> <li>Понимание общих принципов работы ML-моделей и алгоритмов AI,</li> <li>Умение работать в команде.<br /><br /><em>Будет плюсом:</em></li> <li>Знание SQL, формат данных JSON</li> <li>Опыт работы с docker, kubernetes, MS Azure.</li> </ul> <strong>Условия:</strong> <ul> <li>Участие в проекте международного уровня;</li> <li>Высокая белая заработная плата;</li> <li>Четкие цели, открытые коммуникации, минимальная бюрократия;</li> <li>Формат работы: офис/удаленно;</li> <li>Возможность проявлять экспертизу и делиться ею, влиять на выбор инструментов и архитектуры решений;</li> <li>Комфортный офис в центре города с оборудованной кухней, зоной отдыха;</li> <li>Бесплатная парковка для сотрудников.</li> </ul>',\n",
       " '79085498': '<p>Мы -<strong> АО «Гринатом</strong>», и это:</p> <ul> <li>ИТ-интегратор Госкорпорации «Росатом»;</li> <li>создание современных решений для цифровизации атомной отрасли;</li> <li>более 6000 сотрудников в 22 филиалах в 17 регионах России;</li> <li>10 лет экспертизы, высокое качество сервиса, международный уровень проектов.</li> </ul> <p>В нашу команду ищем <strong>Data Engineer.</strong></p> <p><strong>Задачи:</strong></p> <ul> <li>Участие в разработке и проектировании интеграционных решений;</li> <li>Разработка интеграционных сценариев в Атом.Мост (Apache NiFi);</li> <li>Участие в масштабных проектах импортозамещения ГК Росатом;</li> <li>Участие в presale активностях;</li> <li>Разработка проектной и технической документации;</li> <li>Обучение подрастающего поколения.</li> </ul> <p><strong>Мы ожидаем:</strong></p> <ul> <li>Опыт работы с Apache NiFi от 1 года;</li> <li>Понимание основных интеграционных механизмов – синхронный обмен, асинхронный, очереди;</li> <li>Знание основных форматов данных и протоколов обмена – HTTP, REST, SOAP, JSON, XML, XSD, WSDL;</li> <li>Опыт оптимизации производительности Apache NiFi;</li> <li>Опыт разработки на скриптовых языках – groovy, python;</li> <li>Навыки работы в linux.</li> </ul> <p>Будет плюсом:</p> <ul> <li>Опыт эксплуатации высоконагруженных систем на базе Apache NiFi;</li> <li>Опыт работы с SAP PI/PO и другими интеграционными платформами;</li> <li>Опыт работы с GrayLog, ELK;</li> <li>Опыт разработки на Java.</li> </ul> <p><strong>Что мы предлагаем:</strong></p> <p><em>Комфортные условия работы</em></p> <ul> <li>современные рабочие места;</li> <li>цифровые сервисы для сотрудников.</li> </ul> <p><em>Обучение и развитие</em></p> <ul> <li>собственная онлайн-платформа с программами профессионального и личностного роста – от инженерных курсов до изучения иностранных языков;</li> <li>участие в конференциях, тренингах и конкурсах профессионального мастерства.</li> </ul> <p><em>Карьерные возможности</em></p> <ul> <li>карьерные консультации для построения экспертной или управленческой траектории роста;</li> <li>поддержка карьерного развития сотрудников.</li> </ul> <p><em>Социальные программы</em></p> <ul> <li>ДМС со стоматологией и госпитализацией;</li> <li>страхование несчастных случаев на производстве;</li> <li>линия психологической поддержки;</li> <li>финансовая помощь в особых жизненных ситуациях.</li> </ul> <p><em>Корпоративная жизнь</em></p> <ul> <li>тимбилдинги;</li> <li>спортивные активности и отраслевые соревнования;</li> <li>волонтерские движения;</li> <li>мероприятия для сотрудников и их семей.</li> </ul>',\n",
       " '77279957': '<p><strong>Общий Центр Обслуживания Ростелеком – </strong>это большая команда профессионалов в сферах бухгалтерского, налогового, кадрового учета, закупок, юриспруденции, анализа данных и информационных технологий.<br /><br /><strong>Служба внутреннего контроля </strong>оказывает содействие Бизнесу в выстраивании прозрачных и управляемых процессов, сбалансированных с точки зрения их надежности и затрат на обеспечение необходимого уровня контроля.</p> <p><strong>Обязанности:</strong></p> <ul> <li> <p>Разработка архитектуры доставки, хранения и обработки данных в рамках реализации BI отчетности или сервисов автоматизации;</p> </li> <li> <p>Разработка ETL процессов, их оркестрация и поддержка;</p> </li> <li> <p>Разработка витрин данных;</p> </li> <li> <p>Повышение качества данных и документации;</p> </li> <li> <p>Активное участие в развитии инфраструктуры команды.</p> </li> </ul> <strong>Требования:</strong> <ul> <li> <p>Уверенное знание теории реляционных баз данных;</p> </li> <li> <p>Уверенное владение SQL + навыки оптимизации запросов (PostgreSQL);</p> </li> <li> <p>Опыт разработки DAGов в Airflow/Dagster;</p> </li> <li> <p>Уверенное владение python в направлении обработки данных;</p> </li> <li> <p>Опыт работы с Clickhouse.</p> <p> </p> </li> </ul> <strong>Условия:</strong> <ul> <li>Официальное оформление в штат компании;</li> <li>Стабильный и прозрачный доход: оклад и премия по результатам работы;</li> <li>График работы: 5/2, возможность работать удалённо;</li> <li>Личностное развитие и карьерный рост: корпоративная электронная библиотека, возможность прохождения бесплатного обучения и тренингов;</li> <li>Индивидуальную материальную помощь (компенсация процентов по ипотечному кредиту, дополнительные социальные льготы);</li> <li>Программу корпоративных скидок и привилегий: интернет, ТВ, сотовая связь, скидки от компаний-партнёров (отдых, покупки, обучение, спорт);</li> <li>Заботу о здоровье сотрудников: программа ДМС.</li> </ul>',\n",
       " '67984755': '<p>У нас в DWH очень много данных: 6000 объектов, 200 тб в Greenplum и 2 пт в Hadoop. С данными активно работают более 3000+ аналитиков в бизнес командах. На стороне команды DWH созданы инструменты, которые позволяют аналитику не зависеть от etl разработчика и самостоятельно собирать данные для своих задач. Сейчас мы ищем экспертов sql, готовых помогать аналитикам оптимально работать с данными.</p> <p>Наша инфраструктура</p> <p>• Greenplum / Hadoop / Clickhouse в качестве ядра обработки данных</p> <p>• Собственные решения на базе Apache Zeppelin, Apache Airflow и Apache Flink в качестве инструментов трансформации данных для аналитиков (selfservice etl)</p> <p>Мы не требуем знания этих инструментов от кандидатов. Наша задача сделать инструменты работы с данными максимально удобными и доступными. Но нам важно, чтобы кандидат на эту позицию имел большой опыт работы с данными.</p> <p><strong>Обязанности</strong></p> <ul> <li>Быть экспертом по данным: помогать аналитикам в решении вопросов с источниками данных, моделью данных DWH</li> <li>Разрабатывать витрины в помощь аналитикам</li> <li>Выступать заказчиком для разработки витрин в смежных командах</li> <li>Оптимизировать существующие запросы</li> <li>Внедрять и развивать культуру написания оптимальных запросов</li> </ul> <p><strong>Требования</strong></p> <ul> <li>Высшее техническое образование</li> <li>Опыт работы с базами данных в качестве разработчика от 1 года</li> <li>Свободное владение SQL</li> <li>Опыт проектирования объектов БД на основании бизнес требований</li> <li>Понимание теории СУБД и ETL-процессов</li> <li>Знакомство с ETL-инструментами будет плюсом</li> </ul> <p><strong>Мы предлагаем</strong></p> <ul> <li>Возможность работы в аккредитованной ИТ-компании</li> <li>Работу в офисе. График работы — гибридный</li> <li> <p>Платформу обучения и развития Тинькофф Апгрейд. Курсы, тренинги, вебинары и базы знаний. Поддержка менторов и наставников, помощь в поиске точек роста и карьерном развитии</p> </li> <li>Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким</li> <li>Компенсацию такси и 50% затрат на спорт от стоимости абонемента</li> <li>Компенсацию обедов. А если захотите перекусить, есть кухня с чаем, кофе и фруктами</li> <li>Достойную зарплату — обсудим ее на собеседовании</li> </ul>',\n",
       " '79111005': '<p>Наш целевой стек технологий это MS SQL, OLAP Cubes, ClickHouse, dbt, Airflow. Основные языки разработки Python, SQL и bash. Все это подкрепляется устойчивыми и прогрессивными DevOps практиками CI/CD.</p> <p><strong>Что предстоит делать:</strong></p> <ul> <li>Совместно с командой усовершенствовать текущие решения по загрузке и обработке данных;</li> <li>Интеграция с новыми источниками данных (DB, API, файлы);</li> <li>Разработка и поддержка существующих библиотек кода и проведение code review;</li> <li>Усиление экспертизы Python внутри команды;</li> </ul> <strong>Что мы ожидаем:</strong> <ul> <li>Сильная экспертиза в Python и его экосистемы (9+, black, pylint, pytest, pandas, polars, pip);</li> <li>Опыт написания unit и acceptance тестов;</li> <li>Обширный опыт работы с инструментами Airflow, Jupyter.</li> <li>Опыт работы с такими СУБД как MS SQL, ClickHouse;</li> <li>Опыт написания сложных SQL запросов - ANSI SQL 2003+;</li> <li>Опыт использования практик и инструментов CI/CD, docker.</li> <li>Уверенные знания принципов построения системной архитектуры, архитектуры данных, работы с базами данных;</li> <li>Большим плюсом будет опыт работы с dbt;</li> <li>Сильные коммуникативные навыки, командная работа и желание расти, изучать новые технологии и развиваться вместе с командой.</li> </ul> <strong>Условия:</strong> <ul> <li>карьерный рост и интересные кейсы;</li> <li>возможность реализовывать свой потенциал;</li> <li>«белую» заработную плату;</li> <li>гибридный график работы, гибкое время начала и окончания рабочего дня;</li> <li>работу в комфортном офисе в шаговой доступности от м. Ботанический сад;</li> <li>оформление согласно ТК РФ с первого рабочего дня;</li> <li>возможность обучения и развития.</li> </ul>',\n",
       " '79114543': '',\n",
       " '78579913': '',\n",
       " '79122883': '',\n",
       " '79174743': '',\n",
       " '79314037': '',\n",
       " '79174830': '',\n",
       " '79144349': '',\n",
       " '77904581': '',\n",
       " '77909262': '',\n",
       " '77833054': '',\n",
       " '71331696': '',\n",
       " '78984042': '',\n",
       " '78984041': '',\n",
       " '76291546': '',\n",
       " '72679130': '',\n",
       " '79170149': '',\n",
       " '78879767': '',\n",
       " '67471218': '',\n",
       " '75656476': '',\n",
       " '78329582': '',\n",
       " '78994542': '',\n",
       " '79144363': '',\n",
       " '79103187': '',\n",
       " '75833648': '',\n",
       " '78586005': '',\n",
       " '79127185': '',\n",
       " '69146997': '',\n",
       " '78849243': '',\n",
       " '77082533': '',\n",
       " '72312345': '',\n",
       " '78291485': '',\n",
       " '79127174': '',\n",
       " '77245139': '',\n",
       " '67984722': '',\n",
       " '75663573': '',\n",
       " '77846831': '',\n",
       " '79228875': '',\n",
       " '78854296': '',\n",
       " '75718905': '',\n",
       " '78464968': '',\n",
       " '73135054': '',\n",
       " '79138407': '',\n",
       " '79164875': '',\n",
       " '75918380': '',\n",
       " '79064726': '',\n",
       " '78787693': '',\n",
       " '79251949': '',\n",
       " '76278243': '',\n",
       " '78810293': '',\n",
       " '75833712': '',\n",
       " '77082491': '',\n",
       " '78982411': '',\n",
       " '79260554': '',\n",
       " '79114989': '',\n",
       " '77583539': '',\n",
       " '78641539': '',\n",
       " '77080567': '',\n",
       " '79177561': '',\n",
       " '76363319': '',\n",
       " '71322369': '',\n",
       " '77899629': '',\n",
       " '76221565': '',\n",
       " '78552507': '',\n",
       " '67996180': '',\n",
       " '78148984': '',\n",
       " '79124094': '',\n",
       " '78027768': '',\n",
       " '77902544': '',\n",
       " '72161518': '',\n",
       " '77902560': '',\n",
       " '78834188': '',\n",
       " '77904641': '',\n",
       " '78027767': '',\n",
       " '77080599': '',\n",
       " '79309470': '',\n",
       " '78996792': '',\n",
       " '72563146': '',\n",
       " '78815196': '',\n",
       " '73328509': '',\n",
       " '79010531': '',\n",
       " '73328814': '',\n",
       " '78937079': '',\n",
       " '78333637': '',\n",
       " '78507209': '',\n",
       " '73328547': '',\n",
       " '79012004': '',\n",
       " '76363320': '',\n",
       " '78566705': '',\n",
       " '79092544': '',\n",
       " '78662110': '',\n",
       " '67924416': '',\n",
       " '67996104': '',\n",
       " '78621767': '',\n",
       " '78667319': '',\n",
       " '78960860': '',\n",
       " '78987580': '',\n",
       " '78960197': '',\n",
       " '78889781': '',\n",
       " '78936721': '',\n",
       " '78619379': '',\n",
       " '78987579': '',\n",
       " '78959480': '',\n",
       " '76468718': '',\n",
       " '76111917': '',\n",
       " '77767767': '',\n",
       " '76775596': '',\n",
       " '79014315': '',\n",
       " '78667622': '',\n",
       " '78908230': '',\n",
       " '77478896': '',\n",
       " '78976825': '',\n",
       " '77833232': '',\n",
       " '78961917': '',\n",
       " '78467622': '',\n",
       " '72673144': '',\n",
       " '78898370': '',\n",
       " '78887882': '',\n",
       " '77772446': '',\n",
       " '77432992': '',\n",
       " '78936789': '',\n",
       " '77697283': '',\n",
       " '78508667': '',\n",
       " '77772473': '',\n",
       " '78905162': '',\n",
       " '77550414': '',\n",
       " '77697328': '',\n",
       " '76693962': '',\n",
       " '78667404': '',\n",
       " '78829956': '',\n",
       " '75566599': '',\n",
       " '72818601': '',\n",
       " '78911120': '',\n",
       " '77496490': '',\n",
       " '75566561': '',\n",
       " '77407716': '',\n",
       " '78714980': '',\n",
       " '78886559': '',\n",
       " '78669166': '',\n",
       " '77019444': '',\n",
       " '76703124': '',\n",
       " '78830715': '',\n",
       " '78998578': '',\n",
       " '78843112': '',\n",
       " '78506716': '',\n",
       " '67996407': '',\n",
       " '78908191': '',\n",
       " '73186377': '',\n",
       " '77645744': '',\n",
       " '78823057': '',\n",
       " '78856764': '',\n",
       " '78896261': '',\n",
       " '78672191': '',\n",
       " '78902782': '',\n",
       " '77478897': '',\n",
       " '75525600': '',\n",
       " '78851864': '',\n",
       " '78508119': '',\n",
       " '78726846': '',\n",
       " '78736318': '',\n",
       " '78401887': '',\n",
       " '78788548': '',\n",
       " '78077588': '',\n",
       " '78641714': '',\n",
       " '77556769': '',\n",
       " '76907060': '',\n",
       " '76442309': '',\n",
       " '78179393': '',\n",
       " '67996131': '',\n",
       " '78499239': '',\n",
       " '74267349': '',\n",
       " '78179909': '',\n",
       " '78639128': '',\n",
       " '76542547': '',\n",
       " '78726524': '',\n",
       " '78340258': '',\n",
       " '78431549': '',\n",
       " '76578094': '',\n",
       " '70848827': '',\n",
       " '78642162': '',\n",
       " '78884329': '',\n",
       " '77029933': '',\n",
       " '72220912': '',\n",
       " '78721561': '',\n",
       " '77591792': '',\n",
       " '78580878': '',\n",
       " '78736655': '',\n",
       " '78821349': '',\n",
       " '78650967': '',\n",
       " '78199844': '',\n",
       " '78848653': '',\n",
       " '78634572': '',\n",
       " '78263837': '',\n",
       " '78271664': '',\n",
       " '78354144': '',\n",
       " '78680130': '',\n",
       " '78587217': '',\n",
       " '78376764': '',\n",
       " '78561927': '',\n",
       " '78453971': '',\n",
       " '78730030': '',\n",
       " '78668027': '',\n",
       " '78668022': '',\n",
       " '76327290': '',\n",
       " '76477548': '',\n",
       " '78524374': '',\n",
       " '77210869': '',\n",
       " '66519211': '',\n",
       " '78427105': '',\n",
       " '77536408': '',\n",
       " '78409830': '',\n",
       " '78663746': '',\n",
       " '78640898': '',\n",
       " '78520383': '',\n",
       " '77019069': '',\n",
       " '73061294': '',\n",
       " '78299959': '',\n",
       " '72856913': '',\n",
       " '78418843': '',\n",
       " '78170915': '',\n",
       " '78171858': '',\n",
       " '77019068': '',\n",
       " '78773275': '',\n",
       " '78685697': '',\n",
       " '76225754': '',\n",
       " '69199016': '',\n",
       " '77856030': '',\n",
       " '78440865': '',\n",
       " '78667679': '',\n",
       " '74291242': '',\n",
       " '78447454': '',\n",
       " '78405101': '',\n",
       " '78688678': '',\n",
       " '74291265': '',\n",
       " '78712739': '',\n",
       " '78652981': '',\n",
       " '78396587': '',\n",
       " '78149780': '',\n",
       " '69614008': '',\n",
       " '78177414': '',\n",
       " '78375052': '',\n",
       " '78478137': '',\n",
       " '76291842': '',\n",
       " '78141390': '',\n",
       " '76291841': '',\n",
       " '77341231': '',\n",
       " '78685018': '',\n",
       " '72211424': '',\n",
       " '78271476': '',\n",
       " '78358402': '',\n",
       " '71902417': '',\n",
       " '78347781': '',\n",
       " '70430355': '',\n",
       " '73118539': '',\n",
       " '77215006': '',\n",
       " '78147621': '',\n",
       " '78378246': '',\n",
       " '76225045': '',\n",
       " '77235198': '',\n",
       " '75782775': '',\n",
       " '78347782': '',\n",
       " '71902418': '',\n",
       " '75914642': '',\n",
       " '45498560': '',\n",
       " '78266957': '',\n",
       " '73828260': '',\n",
       " '75782776': '',\n",
       " '78153954': '',\n",
       " '73328680': '',\n",
       " '78193962': '',\n",
       " '79251530': '',\n",
       " '79273001': '',\n",
       " '79302154': '',\n",
       " '79245437': '',\n",
       " '79197934': '',\n",
       " '71354549': '',\n",
       " '78754049': '',\n",
       " '78432046': '',\n",
       " '48788339': '',\n",
       " '68148229': '',\n",
       " '74007023': '',\n",
       " '78038884': '',\n",
       " '78794747': '',\n",
       " '79093164': '',\n",
       " '79089818': '',\n",
       " '77474066': '',\n",
       " '71752191': '',\n",
       " '75829622': '',\n",
       " '73591237': '',\n",
       " '78896775': '',\n",
       " '75525561': '',\n",
       " '78905646': '',\n",
       " '76491071': '',\n",
       " '78832982': '',\n",
       " '76420732': '',\n",
       " '70243733': '',\n",
       " '78769342': '',\n",
       " '78707376': '',\n",
       " '77593221': '',\n",
       " '77605879': '',\n",
       " '78786319': '',\n",
       " '75628514': '',\n",
       " '78736784': '',\n",
       " '78632812': '',\n",
       " '78632808': '',\n",
       " '78732746': '',\n",
       " '78631857': '',\n",
       " '78631856': '',\n",
       " '73740885': '',\n",
       " '78038162': '',\n",
       " '77604691': '',\n",
       " '76269193': '',\n",
       " '78418980': '',\n",
       " '78441128': '',\n",
       " '78407364': '',\n",
       " '78440207': '',\n",
       " '78280966': '',\n",
       " '76168231': '',\n",
       " '78329029': '',\n",
       " '78413803': '',\n",
       " '78348342': '',\n",
       " '78265355': '',\n",
       " '66550537': '',\n",
       " '75917632': '',\n",
       " '78253085': '',\n",
       " '79004050': '',\n",
       " '76930898': '',\n",
       " '79100536': '',\n",
       " '79090705': '',\n",
       " '77957886': '',\n",
       " '76734803': '',\n",
       " '78840014': '',\n",
       " '78857147': '',\n",
       " '78387709': '',\n",
       " '78610966': '',\n",
       " '78155339': '',\n",
       " '78879508': '',\n",
       " '69379894': '',\n",
       " '79203137': '',\n",
       " '77865056': '',\n",
       " '69027915': '',\n",
       " '77652440': '',\n",
       " '79252589': '',\n",
       " '79268026': '',\n",
       " '73404665': '',\n",
       " '69027863': '',\n",
       " '78056962': '',\n",
       " '79255894': '',\n",
       " '76671592': '',\n",
       " '79255397': '',\n",
       " '79255262': '',\n",
       " '76402191': '',\n",
       " '79149802': '',\n",
       " '77652420': '',\n",
       " '79216086': '',\n",
       " '79255519': '',\n",
       " '79255279': '',\n",
       " '78027819': '',\n",
       " '77000754': '',\n",
       " '77998800': '',\n",
       " '78909534': '',\n",
       " '79145049': '',\n",
       " '78136491': '',\n",
       " '79146495': '',\n",
       " '78874436': '',\n",
       " '78154515': '',\n",
       " '79324011': '',\n",
       " '78833072': '',\n",
       " '76859360': '',\n",
       " '78447491': '',\n",
       " '73037198': '',\n",
       " '77999764': '',\n",
       " '79056298': '',\n",
       " '79173289': '',\n",
       " '77945874': '',\n",
       " '79106245': '',\n",
       " '77837670': '',\n",
       " '79053099': '',\n",
       " '79170579': '',\n",
       " '78662144': '',\n",
       " '78084559': '',\n",
       " '79150787': '',\n",
       " '79119638': '',\n",
       " '78785653': '',\n",
       " '76402167': '',\n",
       " '79115463': '',\n",
       " '78629495': '',\n",
       " '79114459': '',\n",
       " '79101165': '',\n",
       " '79121821': '',\n",
       " '79121820': '',\n",
       " '79121819': '',\n",
       " '78427482': '',\n",
       " '77949666': '',\n",
       " '78381745': '',\n",
       " '77949850': '',\n",
       " '78520995': '',\n",
       " '75624204': '',\n",
       " '76700906': '',\n",
       " '78581500': '',\n",
       " '79032735': '',\n",
       " '79053098': '',\n",
       " '79050561': '',\n",
       " '79026328': '',\n",
       " '78294254': '',\n",
       " '78975175': '',\n",
       " '78960127': '',\n",
       " '78939763': '',\n",
       " '78901125': '',\n",
       " '78959836': '',\n",
       " '76691650': '',\n",
       " '78941888': '',\n",
       " '78938988': '',\n",
       " '77818891': '',\n",
       " '69819329': '',\n",
       " '76209727': '',\n",
       " '78856762': '',\n",
       " '78786481': '',\n",
       " '76558739': '',\n",
       " '78786488': '',\n",
       " '78771160': '',\n",
       " '77600278': '',\n",
       " '77599910': '',\n",
       " '78759114': '',\n",
       " '77606187': '',\n",
       " '78754704': '',\n",
       " '77616872': '',\n",
       " '78732240': '',\n",
       " '77963503': '',\n",
       " '78732436': '',\n",
       " '78691431': '',\n",
       " '78693608': '',\n",
       " '76008410': '',\n",
       " '78675094': '',\n",
       " '78657775': '',\n",
       " '78662643': '',\n",
       " '73740883': '',\n",
       " '73740887': '',\n",
       " '78629493': '',\n",
       " '78042103': '',\n",
       " '78571983': '',\n",
       " '78135303': '',\n",
       " '78135302': '',\n",
       " '77338025': '',\n",
       " '78498635': '',\n",
       " '78474888': '',\n",
       " '78463562': '',\n",
       " '78409080': '',\n",
       " '78426451': '',\n",
       " '78410566': '',\n",
       " '78381741': '',\n",
       " '72312769': '',\n",
       " '78381736': '',\n",
       " '78394183': '',\n",
       " '78373670': '',\n",
       " '78374239': '',\n",
       " '78300280': '',\n",
       " '78281712': '',\n",
       " '77236315': '',\n",
       " '78309269': '',\n",
       " '78300370': '',\n",
       " '78269906': '',\n",
       " '78272831': '',\n",
       " '78169151': '',\n",
       " '78194146': '',\n",
       " '78156204': '',\n",
       " '78255940': '',\n",
       " '79127171': '',\n",
       " '79127182': '',\n",
       " '79127178': '',\n",
       " '78879281': '',\n",
       " '78937116': '',\n",
       " '67996119': '',\n",
       " '77687838': '',\n",
       " '79121822': '',\n",
       " '79225796': '',\n",
       " '79146431': '',\n",
       " '78694897': '',\n",
       " '79012615': '',\n",
       " '79309883': '',\n",
       " '77411849': '',\n",
       " '79143443': '',\n",
       " '79209147': '',\n",
       " '79127460': '',\n",
       " '79225928': '',\n",
       " '78710089': '',\n",
       " '78006685': '',\n",
       " '79199364': '',\n",
       " '79262670': '',\n",
       " '79236331': '',\n",
       " '79301204': '',\n",
       " '79157478': '',\n",
       " '79209616': '',\n",
       " '78579078': '',\n",
       " '79255312': '',\n",
       " '54012112': '',\n",
       " '79319467': '',\n",
       " '77502305': '',\n",
       " '79173134': '',\n",
       " '79248513': '',\n",
       " '76784959': '',\n",
       " '79170279': '',\n",
       " '79150358': '',\n",
       " '77586678': '',\n",
       " '79220809': '',\n",
       " '79161633': '',\n",
       " '79275335': '',\n",
       " '78711842': '',\n",
       " '79171944': '',\n",
       " '79105527': '',\n",
       " '79181984': '',\n",
       " '78784729': '',\n",
       " '78675069': '',\n",
       " '79293278': '',\n",
       " '79205247': '',\n",
       " '77541047': '',\n",
       " '79108064': '',\n",
       " '79122341': '',\n",
       " '79112858': '',\n",
       " '78192324': '',\n",
       " '79057856': '',\n",
       " '78512324': '',\n",
       " '77196140': '',\n",
       " '79308741': '',\n",
       " '78845017': '',\n",
       " '50293361': '',\n",
       " '78725974': '',\n",
       " '79229664': '',\n",
       " '77468242': '',\n",
       " '78005784': '',\n",
       " '79304164': '',\n",
       " '79303970': '',\n",
       " '79275855': '',\n",
       " '78768855': '',\n",
       " '79026713': '',\n",
       " '76391927': '',\n",
       " '79047478': '',\n",
       " '79169643': '',\n",
       " '79197831': '',\n",
       " '68822135': '',\n",
       " '79318840': '',\n",
       " '79050201': '',\n",
       " '78904517': '',\n",
       " '70985788': '',\n",
       " '78985817': '',\n",
       " '79296916': '',\n",
       " '77339764': '',\n",
       " '71324858': '',\n",
       " '78102643': '',\n",
       " '78142687': '',\n",
       " '79114600': '',\n",
       " '79002008': '',\n",
       " '78186921': '',\n",
       " '79098677': '',\n",
       " '78512323': '',\n",
       " '79106819': '',\n",
       " '78937901': '',\n",
       " '79117415': '',\n",
       " '78266083': '',\n",
       " '78656467': '',\n",
       " '78183585': '',\n",
       " '79205236': '',\n",
       " '76936797': '',\n",
       " '79279709': '',\n",
       " '79111025': '',\n",
       " '79046699': '',\n",
       " '79085508': '',\n",
       " '78759476': '',\n",
       " '79191513': '',\n",
       " '77480809': '',\n",
       " '79117421': '',\n",
       " '78309106': '',\n",
       " '78307983': '',\n",
       " '79311788': '',\n",
       " '76969784': '',\n",
       " '78745518': '',\n",
       " '79179045': '',\n",
       " '79177117': '',\n",
       " '78860693': '',\n",
       " '78107782': '',\n",
       " '79141398': '',\n",
       " '79092853': '',\n",
       " '78634621': '',\n",
       " '79047659': '',\n",
       " '78961029': '',\n",
       " '78173971': '',\n",
       " '78971010': '',\n",
       " '78585391': '',\n",
       " '79314170': '',\n",
       " '79035705': '',\n",
       " '78769816': '',\n",
       " '78855362': '',\n",
       " '79191515': '',\n",
       " '79005656': '',\n",
       " '79011111': '',\n",
       " '75631064': '',\n",
       " '78426024': '',\n",
       " '79134476': '',\n",
       " '77774476': '',\n",
       " '76222353': '',\n",
       " '76934468': '',\n",
       " '76863778': '',\n",
       " '79115520': '',\n",
       " '78915868': '',\n",
       " '74377863': '',\n",
       " '76504777': '',\n",
       " '79275792': '',\n",
       " '76536709': '',\n",
       " '78398423': '',\n",
       " '78122011': '',\n",
       " '77056705': '',\n",
       " '79318265': '',\n",
       " '78608518': '',\n",
       " '77181469': '',\n",
       " '79266085': '',\n",
       " '78652933': '',\n",
       " '72888885': '',\n",
       " '79118242': '',\n",
       " '78731177': '',\n",
       " '77186103': '',\n",
       " '78821598': '',\n",
       " '75832792': '',\n",
       " '77415912': '',\n",
       " '78580416': '',\n",
       " '77659634': '',\n",
       " '79280697': '',\n",
       " '70163749': '',\n",
       " '78149683': '',\n",
       " '78715073': '',\n",
       " '77493395': '',\n",
       " '78841306': '',\n",
       " '77413250': '',\n",
       " '78996156': '',\n",
       " '76826976': '',\n",
       " '78578025': '',\n",
       " '77643478': '',\n",
       " '78895462': '',\n",
       " '77504201': '',\n",
       " '78632870': '',\n",
       " '78052511': '',\n",
       " '78572973': '',\n",
       " '77731388': '',\n",
       " '78426679': '',\n",
       " '77213565': '',\n",
       " '77567000': '',\n",
       " '78077083': '',\n",
       " '78769543': '',\n",
       " '78077082': '',\n",
       " '78609928': '',\n",
       " '78742192': '',\n",
       " '78405363': '',\n",
       " '76826977': '',\n",
       " '78266507': '',\n",
       " '78928573': '',\n",
       " '78914227': '',\n",
       " '78150043': '',\n",
       " '78779600': '',\n",
       " '78375082': '',\n",
       " '76615658': '',\n",
       " '77213566': '',\n",
       " '78855322': '',\n",
       " '78769739': '',\n",
       " '78423651': '',\n",
       " '78301838': '',\n",
       " '78254193': '',\n",
       " '78685969': '',\n",
       " '78686565': '',\n",
       " '77847286': '',\n",
       " '78768854': '',\n",
       " '76957610': '',\n",
       " '78668714': '',\n",
       " '78260367': '',\n",
       " '78257276': '',\n",
       " '78683099': '',\n",
       " '78572972': '',\n",
       " '78725471': '',\n",
       " '78365368': '',\n",
       " '78815500': '',\n",
       " '78178051': '',\n",
       " '77758898': '',\n",
       " '77316163': '',\n",
       " '77213747': '',\n",
       " '78365379': '',\n",
       " '76777464': '',\n",
       " '78438562': '',\n",
       " '78260313': '',\n",
       " '78585198': '',\n",
       " '78366554': '',\n",
       " '78825971': '',\n",
       " '78451060': '',\n",
       " '77992314': '',\n",
       " '76240797': '',\n",
       " '76939405': '',\n",
       " '78260411': '',\n",
       " '78507896': '',\n",
       " '76217954': '',\n",
       " '78740551': '',\n",
       " '78736467': '',\n",
       " '78262270': '',\n",
       " '78598869': '',\n",
       " '78880796': '',\n",
       " '78464362': '',\n",
       " '77214740': '',\n",
       " '78758907': '',\n",
       " '78411745': '',\n",
       " '78576888': '',\n",
       " '75601449': '',\n",
       " '78366553': '',\n",
       " '73372498': '',\n",
       " '77656659': '',\n",
       " '78618503': '',\n",
       " '78500924': '',\n",
       " '76829697': '',\n",
       " '78421979': '',\n",
       " '78156215': '',\n",
       " '78277760': '',\n",
       " '72750262': '',\n",
       " '73072735': '',\n",
       " '78376755': '',\n",
       " '76560618': '',\n",
       " '78673012': '',\n",
       " '75889655': '',\n",
       " '78184057': '',\n",
       " '77229862': '',\n",
       " '78298546': '',\n",
       " '78374058': '',\n",
       " '78256139': '',\n",
       " '78292882': '',\n",
       " '77218099': '',\n",
       " '73728017': '',\n",
       " '78373823': '',\n",
       " '78146526': '',\n",
       " '78330086': '',\n",
       " '76165484': '',\n",
       " '78633730': '',\n",
       " '78233634': '',\n",
       " '78415490': '',\n",
       " '78400616': '',\n",
       " '78127250': '',\n",
       " '79281474': '',\n",
       " '77435865': '',\n",
       " '78412489': '',\n",
       " '77819247': '',\n",
       " '78357091': '',\n",
       " '79281445': '',\n",
       " '78771662': '',\n",
       " '79221081': '',\n",
       " '79226257': '',\n",
       " '78787579': '',\n",
       " '78726443': '',\n",
       " '78618131': '',\n",
       " '78201381': '',\n",
       " '79253134': '',\n",
       " '79323724': '',\n",
       " '78657980': '',\n",
       " '79279424': '',\n",
       " '78201409': '',\n",
       " '79292840': '',\n",
       " '78964240': '',\n",
       " '77484157': '',\n",
       " '78778416': '',\n",
       " '78139717': '',\n",
       " '73313775': '',\n",
       " '79173107': '',\n",
       " '79123098': '',\n",
       " '78856842': '',\n",
       " '79227315': '',\n",
       " '78689178': '',\n",
       " '67224916': '',\n",
       " '71214153': '',\n",
       " '78422892': '',\n",
       " '77412327': '',\n",
       " '78268352': '',\n",
       " '79168673': '',\n",
       " '79179057': '',\n",
       " '79301293': '',\n",
       " '79277563': '',\n",
       " '78199358': '',\n",
       " '79171712': '',\n",
       " '78759238': '',\n",
       " '79257772': '',\n",
       " '78378348': '',\n",
       " '78771813': '',\n",
       " '79007407': '',\n",
       " '78641187': '',\n",
       " '77432365': '',\n",
       " '79177091': '',\n",
       " '78102401': '',\n",
       " '79189245': '',\n",
       " '79128850': '',\n",
       " '79239782': '',\n",
       " '78851205': '',\n",
       " '74339917': '',\n",
       " '79174375': '',\n",
       " '79103682': '',\n",
       " '79118707': '',\n",
       " '79101896': '',\n",
       " '79095952': '',\n",
       " '79117338': '',\n",
       " '71072780': '',\n",
       " '79124147': '',\n",
       " '78084364': '',\n",
       " '77557350': '',\n",
       " '79090323': '',\n",
       " '77814064': '',\n",
       " '78855121': '',\n",
       " '78914325': '',\n",
       " '79098637': '',\n",
       " '77824574': '',\n",
       " '77710805': '',\n",
       " '78391592': '',\n",
       " '79033465': '',\n",
       " '77019760': '',\n",
       " '79048440': '',\n",
       " '79052557': '',\n",
       " '77723392': '',\n",
       " '74257288': '',\n",
       " '78937939': '',\n",
       " '78917422': '',\n",
       " '78873878': '',\n",
       " '78981530': '',\n",
       " '78956427': '',\n",
       " '78843442': '',\n",
       " '78848021': '',\n",
       " '77737078': '',\n",
       " '77850252': '',\n",
       " '78816359': '',\n",
       " '78286979': '',\n",
       " '78812907': '',\n",
       " '78888209': '',\n",
       " '76519651': '',\n",
       " '73298719': '',\n",
       " '78825961': '',\n",
       " '78120333': '',\n",
       " '78692078': '',\n",
       " '78670878': '',\n",
       " '78663781': '',\n",
       " '78663613': '',\n",
       " '78051546': '',\n",
       " '78673079': '',\n",
       " '66509309': '',\n",
       " '78754613': '',\n",
       " '78576605': '',\n",
       " '78657197': '',\n",
       " '78563803': '',\n",
       " '78645453': '',\n",
       " '78585476': '',\n",
       " '78568157': '',\n",
       " '77472643': '',\n",
       " '75705029': '',\n",
       " '76560882': '',\n",
       " '78567035': '',\n",
       " '78587129': '',\n",
       " '78043382': '',\n",
       " '78474156': '',\n",
       " '77527309': '',\n",
       " '78333804': '',\n",
       " '76049575': '',\n",
       " '78528574': '',\n",
       " '78290216': '',\n",
       " '78271635': '',\n",
       " '78372663': '',\n",
       " '78365180': '',\n",
       " '78352255': '',\n",
       " '78453886': '',\n",
       " '78199871': '',\n",
       " '78278511': '',\n",
       " '78311656': '',\n",
       " '78143965': '',\n",
       " '78308747': '',\n",
       " '75863011': '',\n",
       " '78135656': '',\n",
       " '77229824': '',\n",
       " '78241386': '',\n",
       " '79154945': '',\n",
       " '68581636': '',\n",
       " '77667044': '',\n",
       " '79139093': '',\n",
       " '75697706': '',\n",
       " '76964932': '',\n",
       " '77696974': '',\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacancies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление ключа \"description\" для каждой вакансии\n",
    "for vacancy in vacancies:\n",
    "    vacancy[\"description\"] = \"\"\n",
    "\n",
    "# Получение списка id вакансий из ответа на запрос\n",
    "vacancy_ids = [vacancy[\"id\"] for vacancy in vacancies]\n",
    "\n",
    "# Цикл по каждому id вакансии\n",
    "for vacancy_id in vacancy_ids:\n",
    "    \n",
    "\n",
    "    # Запрос описания вакансии\n",
    "    response = requests.get('https://api.hh.ru/vacancies/' + vacancy_id)\n",
    "    \n",
    "    # Получение описания вакансии из ответа на запрос\n",
    "    description = json.loads(response.text).get(\"description\", \"\")\n",
    "\n",
    "    \n",
    "    # Запись описания вакансии в ключ \"description\"\n",
    "    for vacancy in vacancies:\n",
    "        if vacancy[\"id\"] == vacancy_id:\n",
    "            vacancy[\"description\"] = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158a56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем описания для каждой вакансии датафрейма, записываем результат\n",
    "\n",
    "def get_description(vacancy_id):\n",
    "    url = f'https://api.hh.ru/vacancies/{vacancy_id}'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    description = \"\"\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.ok:\n",
    "                data = response.json()\n",
    "                description = data['description']\n",
    "            break\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Ошибка получения описания вакансии {vacancy_id}. Повтор запроса через 5 секунд.\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "    return description\n",
    "\n",
    "# Применяем функцию для каждой вакансии в датафрейме и записываем результат в новый столбец\n",
    "df['description'] = df['id'].apply(get_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c88e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105458a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fe633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab761e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52957e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e800c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700041f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fcaeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://api.hh.ru/vacancies'\n",
    "\n",
    "params = {\n",
    "    'text': \"Data Scientist\",\n",
    "    'area': 1,\n",
    "    'page': 0,\n",
    "    'per_page': 10\n",
    "}\n",
    "\n",
    "req = requests.get(URL, params)\n",
    "data = json.loads(req.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564e51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078526d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c413b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645ca6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e89044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67690c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e985a03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cf8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e1a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим описание первой вакансии\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сколько найдено вакансий\n",
    "data['found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620566dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Страниц в результатах поиска\n",
    "data['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем так, чтобы выводились все столбцы датафрейма\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a069542",
   "metadata": {},
   "source": [
    "С помощью метода pandas.json_normalize разберем структурированные данные из JSON в табличный формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(data['items'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eebdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670697b",
   "metadata": {},
   "source": [
    "Видим, что в столбце 'professional_roles' данные не нормализовались. Что бы разобрать вложенный список из professional_roles, применим к столбцу лямбда-функцию, разделим его на два новых столбца:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5150c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['professional_roles_id', 'professional_roles_name']] = (\n",
    "    df['professional_roles']\n",
    "    .apply(lambda x: pd.Series([x[0]['id'], x[0]['name']]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем названия столбцов\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предвинем два получившихся столбца на место изначального professional_roles, \n",
    "# а его не будем включать в обновленный датафрейм \n",
    "\n",
    "df=df[['id', 'premium', 'name', 'has_test', 'response_letter_required',\n",
    "       'address', 'response_url', 'sort_point_distance', 'published_at',\n",
    "       'created_at', 'archived', 'apply_alternate_url', 'insider_interview',\n",
    "       'url', 'adv_response_url', 'alternate_url', 'relations', 'contacts',\n",
    "       'schedule', 'working_days', 'working_time_intervals',\n",
    "       'working_time_modes', 'accept_temporary', 'professional_roles_id',\n",
    "       'professional_roles_name', 'accept_incomplete_resumes',\n",
    "       'department.id', 'department.name', 'area.id', 'area.name', 'area.url',\n",
    "       'salary.from', 'salary.to', 'salary.currency', 'salary.gross',\n",
    "       'type.id', 'type.name', 'employer.id', 'employer.name', 'employer.url',\n",
    "       'employer.alternate_url', 'employer.logo_urls.240',\n",
    "       'employer.logo_urls.90', 'employer.logo_urls.original',\n",
    "       'employer.vacancies_url', 'employer.trusted', 'snippet.requirement',\n",
    "       'snippet.responsibility', 'department', 'employer.logo_urls',\n",
    "       'address.city', 'address.street', 'address.building', 'address.lat',\n",
    "       'address.lng', 'address.description', 'address.raw', 'address.metro',\n",
    "       'address.metro_stations', 'address.id', 'salary',\n",
    "       'address.metro.station_name', 'address.metro.line_name',\n",
    "       'address.metro.station_id', 'address.metro.line_id',\n",
    "       'address.metro.lat', 'address.metro.lng']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae96e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['snippet.requirement'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969758ab",
   "metadata": {},
   "source": [
    "Также видим, что в столбцах snippet.requirement\tи snippet.responsibility есть теги. Если в тексте снипета встретилась поисковая фраза (параметр text ), она будет подсвечена тегом highlighttext (из документации по API). Но нам эти теги ни к чему, избавимся от них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e11c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'<.*?>', '', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df[['snippet.requirement', 'snippet.responsibility']] = df[['snippet.requirement', 'snippet.responsibility']].applymap(remove_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['snippet.requirement'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08838c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['snippet.responsibility'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eeb06b",
   "metadata": {},
   "source": [
    "Для получения полного описания вакансии потребуется задать отдельный запрос, используя ее id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39f513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac88ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca33cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy = df['id'].iloc[0]\n",
    "vacancy_url = f'https://api.hh.ru/vacancies/{vacancy}'\n",
    "\n",
    "req = requests.get(vacancy_url)\n",
    "vacancy_info = json.loads(req.content.decode())\n",
    "vacancy_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e2dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bec85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314fc26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591308fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ДАВАЙТЕ БЕЗ ДАВАЙТЕ ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a10787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27463aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premium</th>\n",
       "      <th>name</th>\n",
       "      <th>has_test</th>\n",
       "      <th>response_letter_required</th>\n",
       "      <th>address</th>\n",
       "      <th>response_url</th>\n",
       "      <th>sort_point_distance</th>\n",
       "      <th>published_at</th>\n",
       "      <th>created_at</th>\n",
       "      <th>...</th>\n",
       "      <th>address.metro_stations</th>\n",
       "      <th>address.id</th>\n",
       "      <th>salary</th>\n",
       "      <th>address.metro.station_name</th>\n",
       "      <th>address.metro.line_name</th>\n",
       "      <th>address.metro.station_id</th>\n",
       "      <th>address.metro.line_id</th>\n",
       "      <th>address.metro.lat</th>\n",
       "      <th>address.metro.lng</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79110745</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-10T14:42:13+0300</td>\n",
       "      <td>2023-04-10T14:42:13+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Ищу &lt;strong&gt;Data Engineer&lt;/strong&gt; в SportT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79120615</td>\n",
       "      <td>False</td>\n",
       "      <td>Data engineer</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-10T17:13:22+0300</td>\n",
       "      <td>2023-04-10T17:13:22+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;В крупную исследовательскую компанию, специ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78954091</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer / Дата-инженер (Middle)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-11T15:27:03+0300</td>\n",
       "      <td>2023-04-11T15:27:03+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'station_name': 'Белорусская', 'line_name': ...</td>\n",
       "      <td>12649470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Белорусская</td>\n",
       "      <td>Кольцевая</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.0</td>\n",
       "      <td>55.775179</td>\n",
       "      <td>37.582303</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;&lt;em&gt;Приветствуем тебя, будущий учас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78934984</td>\n",
       "      <td>False</td>\n",
       "      <td>Data engineer (Стажер)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-11T11:13:27+0300</td>\n",
       "      <td>2023-04-11T11:13:27+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'station_name': 'Бауманская', 'line_name': '...</td>\n",
       "      <td>886614.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Бауманская</td>\n",
       "      <td>Арбатско-Покровская</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>55.772405</td>\n",
       "      <td>37.679040</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;НЕ НУЖНО ОТКЛИКАТЬСЯ НА ВАКАНСИЮ! Ч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79084256</td>\n",
       "      <td>False</td>\n",
       "      <td>Data Engineer (lead)</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-13T09:31:35+0300</td>\n",
       "      <td>2023-04-13T09:31:35+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;strong&gt;Что нужно будет делать:&lt;/strong&gt; &lt;ul&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>78298565</td>\n",
       "      <td>False</td>\n",
       "      <td>Аналитик DWH</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-20T16:46:50+0300</td>\n",
       "      <td>2023-03-20T16:46:50+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>78312389</td>\n",
       "      <td>False</td>\n",
       "      <td>Системный аналитик DWH</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-20T21:23:22+0300</td>\n",
       "      <td>2023-03-20T21:23:22+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>78312390</td>\n",
       "      <td>False</td>\n",
       "      <td>Системный аналитик DWH</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-20T21:23:22+0300</td>\n",
       "      <td>2023-03-20T21:23:22+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5277</th>\n",
       "      <td>78527633</td>\n",
       "      <td>False</td>\n",
       "      <td>Аналитик систем целевого маркетинга</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-24T18:40:56+0300</td>\n",
       "      <td>2023-03-24T18:40:56+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>75833734</td>\n",
       "      <td>False</td>\n",
       "      <td>Главный инженер данных</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-17T12:36:39+0300</td>\n",
       "      <td>2023-03-17T12:36:39+0300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5279 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  premium                                   name  has_test  \\\n",
       "0     79110745    False                          Data Engineer     False   \n",
       "1     79120615    False                          Data engineer     False   \n",
       "2     78954091    False  Data Engineer / Дата-инженер (Middle)     False   \n",
       "3     78934984    False                 Data engineer (Стажер)     False   \n",
       "4     79084256    False                   Data Engineer (lead)     False   \n",
       "...        ...      ...                                    ...       ...   \n",
       "5274  78298565    False                           Аналитик DWH     False   \n",
       "5275  78312389    False                 Системный аналитик DWH     False   \n",
       "5276  78312390    False                 Системный аналитик DWH     False   \n",
       "5277  78527633    False    Аналитик систем целевого маркетинга     False   \n",
       "5278  75833734    False                 Главный инженер данных     False   \n",
       "\n",
       "      response_letter_required  address response_url  sort_point_distance  \\\n",
       "0                        False      NaN          NaN                  NaN   \n",
       "1                         True      NaN          NaN                  NaN   \n",
       "2                        False      NaN          NaN                  NaN   \n",
       "3                        False      NaN          NaN                  NaN   \n",
       "4                        False      NaN          NaN                  NaN   \n",
       "...                        ...      ...          ...                  ...   \n",
       "5274                     False      NaN          NaN                  NaN   \n",
       "5275                     False      NaN          NaN                  NaN   \n",
       "5276                     False      NaN          NaN                  NaN   \n",
       "5277                     False      NaN          NaN                  NaN   \n",
       "5278                     False      NaN          NaN                  NaN   \n",
       "\n",
       "                  published_at                created_at  ...  \\\n",
       "0     2023-04-10T14:42:13+0300  2023-04-10T14:42:13+0300  ...   \n",
       "1     2023-04-10T17:13:22+0300  2023-04-10T17:13:22+0300  ...   \n",
       "2     2023-04-11T15:27:03+0300  2023-04-11T15:27:03+0300  ...   \n",
       "3     2023-04-11T11:13:27+0300  2023-04-11T11:13:27+0300  ...   \n",
       "4     2023-04-13T09:31:35+0300  2023-04-13T09:31:35+0300  ...   \n",
       "...                        ...                       ...  ...   \n",
       "5274  2023-03-20T16:46:50+0300  2023-03-20T16:46:50+0300  ...   \n",
       "5275  2023-03-20T21:23:22+0300  2023-03-20T21:23:22+0300  ...   \n",
       "5276  2023-03-20T21:23:22+0300  2023-03-20T21:23:22+0300  ...   \n",
       "5277  2023-03-24T18:40:56+0300  2023-03-24T18:40:56+0300  ...   \n",
       "5278  2023-03-17T12:36:39+0300  2023-03-17T12:36:39+0300  ...   \n",
       "\n",
       "                                 address.metro_stations  address.id  salary  \\\n",
       "0                                                   NaN         NaN     NaN   \n",
       "1                                                   NaN         NaN     NaN   \n",
       "2     [{'station_name': 'Белорусская', 'line_name': ...  12649470.0     NaN   \n",
       "3     [{'station_name': 'Бауманская', 'line_name': '...    886614.0     NaN   \n",
       "4                                                   NaN         NaN     NaN   \n",
       "...                                                 ...         ...     ...   \n",
       "5274                                                NaN         NaN     NaN   \n",
       "5275                                                NaN         NaN     NaN   \n",
       "5276                                                NaN         NaN     NaN   \n",
       "5277                                                NaN         NaN     NaN   \n",
       "5278                                                NaN         NaN     NaN   \n",
       "\n",
       "     address.metro.station_name  address.metro.line_name  \\\n",
       "0                           NaN                      NaN   \n",
       "1                           NaN                      NaN   \n",
       "2                   Белорусская                Кольцевая   \n",
       "3                    Бауманская      Арбатско-Покровская   \n",
       "4                           NaN                      NaN   \n",
       "...                         ...                      ...   \n",
       "5274                        NaN                      NaN   \n",
       "5275                        NaN                      NaN   \n",
       "5276                        NaN                      NaN   \n",
       "5277                        NaN                      NaN   \n",
       "5278                        NaN                      NaN   \n",
       "\n",
       "     address.metro.station_id address.metro.line_id  address.metro.lat  \\\n",
       "0                         NaN                   NaN                NaN   \n",
       "1                         NaN                   NaN                NaN   \n",
       "2                        5.20                   5.0          55.775179   \n",
       "3                        3.17                   3.0          55.772405   \n",
       "4                         NaN                   NaN                NaN   \n",
       "...                       ...                   ...                ...   \n",
       "5274                      NaN                   NaN                NaN   \n",
       "5275                      NaN                   NaN                NaN   \n",
       "5276                      NaN                   NaN                NaN   \n",
       "5277                      NaN                   NaN                NaN   \n",
       "5278                      NaN                   NaN                NaN   \n",
       "\n",
       "      address.metro.lng                                        description  \n",
       "0                   NaN  <p>Ищу <strong>Data Engineer</strong> в SportT...  \n",
       "1                   NaN  <p>В крупную исследовательскую компанию, специ...  \n",
       "2             37.582303  <p><strong><em>Приветствуем тебя, будущий учас...  \n",
       "3             37.679040  <p><strong>НЕ НУЖНО ОТКЛИКАТЬСЯ НА ВАКАНСИЮ! Ч...  \n",
       "4                   NaN  <strong>Что нужно будет делать:</strong> <ul> ...  \n",
       "...                 ...                                                ...  \n",
       "5274                NaN                                                NaN  \n",
       "5275                NaN                                                NaN  \n",
       "5276                NaN                                                NaN  \n",
       "5277                NaN                                                NaN  \n",
       "5278                NaN                                                NaN  \n",
       "\n",
       "[5279 rows x 68 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd838c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Столбцы без данных: ['address', 'sort_point_distance', 'insider_interview', 'adv_response_url', 'contacts', 'schedule', 'department', 'employer.logo_urls', 'address.description', 'address.metro', 'salary']\n"
     ]
    }
   ],
   "source": [
    "# Проверим, какие столбцы не содержат данные\n",
    "\n",
    "missing_cols = df.columns[df.isna().all()].tolist()\n",
    "print(f'Столбцы без данных: {missing_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae22a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Избавимся от них\n",
    "\n",
    "df = df.drop(missing_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53abb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5279 entries, 0 to 5278\n",
      "Data columns (total 57 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           5279 non-null   int64  \n",
      " 1   premium                      5279 non-null   bool   \n",
      " 2   name                         5279 non-null   object \n",
      " 3   has_test                     5279 non-null   bool   \n",
      " 4   response_letter_required     5279 non-null   bool   \n",
      " 5   response_url                 1 non-null      object \n",
      " 6   published_at                 5279 non-null   object \n",
      " 7   created_at                   5279 non-null   object \n",
      " 8   archived                     5279 non-null   bool   \n",
      " 9   apply_alternate_url          5279 non-null   object \n",
      " 10  url                          5279 non-null   object \n",
      " 11  alternate_url                5279 non-null   object \n",
      " 12  relations                    5279 non-null   object \n",
      " 13  working_days                 5279 non-null   object \n",
      " 14  working_time_intervals       5279 non-null   object \n",
      " 15  working_time_modes           5279 non-null   object \n",
      " 16  accept_temporary             5279 non-null   bool   \n",
      " 17  professional_roles_id        5279 non-null   int64  \n",
      " 18  professional_roles_name      5279 non-null   object \n",
      " 19  accept_incomplete_resumes    5279 non-null   bool   \n",
      " 20  department.id                789 non-null    object \n",
      " 21  department.name              789 non-null    object \n",
      " 22  area.id                      5279 non-null   int64  \n",
      " 23  area.name                    5279 non-null   object \n",
      " 24  area.url                     5279 non-null   object \n",
      " 25  salary.from                  1445 non-null   float64\n",
      " 26  salary.to                    996 non-null    float64\n",
      " 27  salary.currency              1710 non-null   object \n",
      " 28  salary.gross                 1709 non-null   object \n",
      " 29  type.id                      5279 non-null   object \n",
      " 30  type.name                    5279 non-null   object \n",
      " 31  employer.id                  5266 non-null   float64\n",
      " 32  employer.name                5279 non-null   object \n",
      " 33  employer.url                 5266 non-null   object \n",
      " 34  employer.alternate_url       5266 non-null   object \n",
      " 35  employer.logo_urls.240       4948 non-null   object \n",
      " 36  employer.logo_urls.90        4948 non-null   object \n",
      " 37  employer.logo_urls.original  4948 non-null   object \n",
      " 38  employer.vacancies_url       5266 non-null   object \n",
      " 39  employer.trusted             5279 non-null   bool   \n",
      " 40  snippet.requirement          5250 non-null   object \n",
      " 41  snippet.responsibility       5251 non-null   object \n",
      " 42  address.city                 2660 non-null   object \n",
      " 43  address.street               2618 non-null   object \n",
      " 44  address.building             2539 non-null   object \n",
      " 45  address.lat                  2686 non-null   float64\n",
      " 46  address.lng                  2686 non-null   float64\n",
      " 47  address.raw                  2766 non-null   object \n",
      " 48  address.metro_stations       2878 non-null   object \n",
      " 49  address.id                   2878 non-null   float64\n",
      " 50  address.metro.station_name   1569 non-null   object \n",
      " 51  address.metro.line_name      1569 non-null   object \n",
      " 52  address.metro.station_id     1569 non-null   float64\n",
      " 53  address.metro.line_id        1569 non-null   float64\n",
      " 54  address.metro.lat            1569 non-null   float64\n",
      " 55  address.metro.lng            1569 non-null   float64\n",
      " 56  description                  121 non-null    object \n",
      "dtypes: bool(7), float64(10), int64(3), object(37)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9387581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5158"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.description.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809a7788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  name  count\n",
      "0                                             Аналитик    430\n",
      "1                                      Бизнес-аналитик    260\n",
      "2                                   Системный аналитик    184\n",
      "3                                      Аналитик данных    138\n",
      "4                                  Маркетолог-аналитик    106\n",
      "...                                                ...    ...\n",
      "1140                                Chief Data Officer      1\n",
      "1141                           Team Lead Data Platform      1\n",
      "1142          Ведущий администратор по Big data и NIFI      1\n",
      "1143  Старший эксперт по анализу данных (data science)      1\n",
      "2675               Аналитик систем целевого маркетинга      1\n",
      "\n",
      "[2676 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Подсчитаем количество каждой ваакансии\n",
    "\n",
    "value_counts = df['name'].value_counts()\n",
    "result = pd.DataFrame({'name': value_counts.index, 'count':value_counts.values})\n",
    "result.sort_values(by='count', ascending=False, inplace=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96012d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: joblib in /Users/dariavyatkina/opt/anaconda3/envs/hh_project/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp39-cp39-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.3.23 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c236d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-50 слов в объявлениях:\n",
      "\n",
      "nan: 5158\n",
      "li: 2357\n",
      "и: 1468\n",
      "p: 1411\n",
      "в: 955\n",
      "с: 696\n",
      "strong: 671\n",
      "данных: 584\n",
      "на: 455\n",
      "ul: 431\n",
      "опыт: 413\n",
      "работы: 408\n",
      "мы: 289\n",
      "для: 288\n",
      "data: 200\n",
      "по: 190\n",
      "br: 169\n",
      "от: 153\n",
      "знание: 133\n",
      "sql: 123\n",
      "из: 118\n",
      "разработка: 113\n",
      "python: 112\n",
      "или: 111\n",
      "к: 102\n",
      "возможность: 100\n",
      "and: 100\n",
      "будет: 97\n",
      "airflow: 97\n",
      "что: 88\n",
      "dwh: 87\n",
      "у: 86\n",
      "etl: 80\n",
      "hadoop: 79\n",
      "нас: 78\n",
      "spark: 76\n",
      "дмс: 76\n",
      "разработки: 74\n",
      "компании: 72\n",
      "работа: 70\n",
      "работать: 68\n",
      "em: 67\n",
      "обработки: 67\n",
      "а: 65\n",
      "плюсом: 64\n",
      "витрин: 64\n",
      "как: 64\n",
      "процессов: 64\n",
      "не: 63\n",
      "apache: 63\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Считываем данные из файла и объединяем все значения из столбца 'description'\n",
    "text = ' '.join(df['description'].astype(str))\n",
    "\n",
    "# Разбиваем текст на слова и удаляем стоп-слова\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "words = [word.lower() for word in word_tokenize(text, language='russian') if word.isalpha()]\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Выводим топ-50 слов\n",
    "print('Топ-50 слов в объявлениях:\\n')\n",
    "for word, count in word_count.most_common(50):\n",
    "    print(f'{word}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fcc6244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dariavyatkina/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "данных: 614\n",
      "работы: 412\n",
      "опыт: 412\n",
      "data: 201\n",
      "знание: 133\n",
      "sql: 129\n",
      "python: 118\n",
      "разработка: 115\n",
      "возможность: 100\n",
      "and: 100\n",
      "airflow: 98\n",
      "dwh: 85\n",
      "etl: 81\n",
      "hadoop: 80\n",
      "компании: 79\n",
      "дмс: 77\n",
      "spark: 76\n",
      "разработки: 74\n",
      "работа: 70\n",
      "работать: 69\n",
      "данными: 68\n",
      "обработки: 67\n",
      "инструментов: 65\n",
      "процессов: 65\n",
      "плюсом: 64\n",
      "витрин: 64\n",
      "apache: 63\n",
      "понимание: 62\n",
      "предлагаем: 60\n",
      "поддержка: 57\n",
      "график: 57\n",
      "требования: 53\n",
      "участие: 53\n",
      "условия: 52\n",
      "clickhouse: 52\n",
      "команда: 51\n",
      "обучения: 50\n",
      "работу: 48\n",
      "greenplum: 48\n",
      "навыки: 48\n",
      "задачи: 47\n",
      "офис: 46\n",
      "оптимизация: 43\n",
      "to: 43\n",
      "офисе: 42\n",
      "задач: 42\n",
      "россии: 42\n",
      "знания: 42\n",
      "ищем: 42\n",
      "развитие: 41\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Считываем данные из файла и объединяем все значения из столбца 'description'\n",
    "text = ' '.join([BeautifulSoup(desc, \"html.parser\").get_text() for desc in df['description'] if isinstance(desc, str)])\n",
    "\n",
    "\n",
    "# Определяем стоп-слова для русского языка\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "# Разбиваем текст на слова и удаляем стоп-слова\n",
    "words = [word.lower() for word in word_tokenize(text, language='russian') if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "# Считаем количество уникальных слов\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Выводим топ-50 слов\n",
    "for word, count in word_count.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
